[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "marginaleffects 0.12.0",
    "section": "",
    "text": "The marginaleffects package for R allows users to compute and plot predictions, slopes, marginal means, and comparisons (contrasts, risk ratios, odds, etc.) for over 80 classes of statistical models. It can conduct linear and non-linear hypothesis tests, or equivalence tests, and calculate uncertainty estimates using the delta method, bootstrapping, or simulation-based inference.\n\n1 Why?\nParameter estimates are often hard to interpret substantively, especially when they are generated by complex models with non-linear components or transformations. Many applied researchers would rather focus on simple quantities of interest, which have straightforward scientific interpretations. Unfortunately, these estimands (and their standard errors) are tedious to compute. Moreover, the different modeling packages in R often produce inconsistent objects that require special treatment.\nmarginaleffects offers a single point of entry to easily interpret the results of over 80 classes of models, using a simple and consistent user interface.\nBenefits of marginaleffects include:\n\n\nPowerful: It can compute predictions, comparisons (contrasts, risk ratios, etc.), slopes, and conduct hypothesis tests for 80 different classes of models in R.\n\nSimple: All functions share a simple and unified interface.\n\nDocumented: Each function is thoroughly documented with abundant examples. The website includes 20,000+ words of vignettes and case studies.\n\nEfficient: Some operations are orders of magnitude faster than with the margins package, and the memory footprint is much smaller.\n\nThin: Few dependencies.\n\nStandards-compliant: marginaleffects follows “tidy” principles and returns objects that work with standard functions like summary(), head(), tidy(), and glance(). These objects are easy to program with and feed to other packages like modelsummary.\n\n\nValid: When possible, numerical results are checked against alternative software like Stata or other R packages. Unfortunately, it is not possible to test every model type, so users are still strongly encouraged to cross-check their results.\n\nExtensible: Adding support for new models is very easy, often requiring less than 10 lines of new code. Please submit feature requests on Github.\n\n\nActive development: Bugs are fixed promptly.\n\n2 What?\nThe marginaleffects package allows R users to compute and plot three principal quantities of interest: (1) predictions, (2) comparisons, and (3) slopes. In addition, the package includes a convenience function to compute a fourth estimand, “marginal means”, which is a special case of averaged predictions. marginaleffects can also average (or “marginalize”) unit-level (or “conditional”) estimates of all those quantities, and conduct hypothesis tests on them.\nPredictions:\n\nThe outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels. a.k.a. Fitted values, adjusted predictions. predictions(), avg_predictions(), plot_predictions().\n\nComparisons:\n\nCompare the predictions made by a model for different regressor values (e.g., college graduates vs. others): contrasts, differences, risk ratios, odds, etc. comparisons(), avg_comparisons(), plot_comparisons().\n\nSlopes:\n\nPartial derivative of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends. slopes(), avg_slopes(), plot_slopes().\n\nMarginal Means:\n\nPredictions of a model, averaged across a “reference grid” of categorical predictors. marginalmeans().\n\nHypothesis and Equivalence Tests:\n\nHypothesis and equivalence tests can be conducted on linear or non-linear functions of model coefficients, or on any of the quantities computed by the marginaleffects packages (predictions, slopes, comparisons, marginal means, etc.). Uncertainy estimates can be obtained via the delta method (with or without robust standard errors), bootstrap, or simulation.\n\n\n\n\n\nGoal\nFunction\n\n\n\nPredictions\npredictions()\n\n\n\navg_predictions()\n\n\n\nplot_predictions()\n\n\nComparisons\ncomparisons()\n\n\n\navg_comparisons()\n\n\n\nplot_comparisons()\n\n\nSlopes\nslopes()\n\n\n\navg_slopes()\n\n\n\nplot_slopes()\n\n\nMarginal Means\nmarginal_means()\n\n\nGrids\ndatagrid()\n\n\n\ndatagridcf()\n\n\nHypothesis & Equivalence\nhypotheses()\n\n\nBayes, Bootstrap, Simulation\nposterior_draws()\n\n\n\ninferences()\n\n\n\n\n\n\n3 License and Citation\nThe marginaleffects package is licensed under the GNU General Public License v3.0. The content of this website/book is licensed under the Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0).\n\nWarning in citation(\"marginaleffects\"): no date field in DESCRIPTION file of package 'marginaleffects'\n\nTo cite package 'marginaleffects' in publications use:\n\n  Arel-Bundock V (2023). _marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests_. R package version 0.12.0.9008, <https://vincentarelbundock.github.io/marginaleffects/>.\n\nA BibTeX entry for LaTeX users is\n\n  @Manual{,\n    title = {marginaleffects: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis\nTests},\n    author = {Vincent Arel-Bundock},\n    year = {2023},\n    note = {R package version 0.12.0.9008},\n    url = {https://vincentarelbundock.github.io/marginaleffects/},\n  }"
  },
  {
    "objectID": "articles/marginaleffects.html#installation",
    "href": "articles/marginaleffects.html#installation",
    "title": "\n1  Get Started\n",
    "section": "\n1.1 Installation",
    "text": "1.1 Installation\nInstall the latest CRAN release:\n\ninstall.packages(\"marginaleffects\")\n\nInstall the development version:\n\ninstall.packages(\n    c(\"marginaleffects\", \"insight\"),\n    repos = c(\"https://vincentarelbundock.r-universe.dev\", \"https://easystats.r-universe.dev\"))\n\nRestart R completely before moving on."
  },
  {
    "objectID": "articles/marginaleffects.html#estimands-predictions-comparisons-and-slopes",
    "href": "articles/marginaleffects.html#estimands-predictions-comparisons-and-slopes",
    "title": "\n1  Get Started\n",
    "section": "\n1.2 Estimands: Predictions, Comparisons, and Slopes",
    "text": "1.2 Estimands: Predictions, Comparisons, and Slopes\nThe marginaleffects package allows R users to compute and plot three principal quantities of interest: (1) predictions, (2) comparisons, and (3) slopes. In addition, the package includes a convenience function to compute a fourth estimand, “marginal means”, which is a special case of averaged predictions. marginaleffects can also average (or “marginalize”) unit-level (or “conditional”) estimates of all those quantities, and conduct hypothesis tests on them.\nPredictions:\n\nThe outcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels. a.k.a. Fitted values, adjusted predictions. predictions(), avg_predictions(), plot_predictions().\n\nComparisons:\n\nCompare the predictions made by a model for different regressor values (e.g., college graduates vs. others): contrasts, differences, risk ratios, odds, etc. comparisons(), avg_comparisons(), plot_comparisons().\n\nSlopes:\n\nPartial derivative of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends. slopes(), avg_slopes(), plot_slopes().\n\nMarginal Means:\n\nPredictions of a model, averaged across a “reference grid” of categorical predictors. marginalmeans().\n\nPredictions, comparisons, and slopes are fundamentally unit-level (or “conditional”) quantities. Except in the simplest linear case, estimates will typically vary based on the values of all the regressors in a model. Each of the observations in a dataset is thus associated with its own prediction, comparison, and slope estimates. Below, we will see that it can be useful to marginalize (or “average over”) unit-level estimates to report an “average prediction”, “average comparison”, or “average slope”.\nOne ambiguous aspect of the definitions above is that the word “marginal” comes up in two different and opposite ways:\n\nIn “marginal effects,” we refer to the effect of a tiny (marginal) change in the regressor on the outcome. This is a slope, or derivative.\nIn “marginal means,” we refer to the process of marginalizing across rows of a prediction grid. This is an average, or integral.\n\nOn this website and in this package, we reserve the expression “marginal effect” to mean a “slope” or “partial derivative”.\nThe marginaleffects package includes functions to estimate, average, plot, and summarize all of the estimands described above. The objects produced by marginaleffects are “tidy”: they produce simple data frames in “long” format. They are also “standards-compliant” and work seamlessly with standard functions like summary(), head(), tidy(), and glance(), as well with external packages like modelsummary or ggplot2.\nWe now apply marginaleffects functions to compute each of the estimands described above. First, we fit a linear regression model with multiplicative interactions:\n\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ hp * wt * am, data = mtcars)\n\nThen, we call the predictions() function. As noted above, predictions are unit-level estimates, so there is one specific prediction per observation. By default, the predictions() function makes one prediction per observation in the dataset that was used to fit the original model. Since mtcars has 32 rows, the predictions() outcome also has 32 rows:\n\npre <- predictions(mod)\n\nnrow(mtcars)\n\n[1] 32\n\nnrow(pre)\n\n[1] 32\n\npre\n\n\n Estimate Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n     22.5      0.884 25.44   <0.001 471.7  20.8   24.2\n     20.8      1.194 17.42   <0.001 223.3  18.5   23.1\n     25.3      0.709 35.66   <0.001 922.7  23.9   26.7\n     20.3      0.704 28.75   <0.001 601.5  18.9   21.6\n     17.0      0.712 23.88   <0.001 416.2  15.6   18.4\n--- 22 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n     29.6      1.874 15.80   <0.001 184.3  25.9   33.3\n     15.9      1.311 12.13   <0.001 110.0  13.3   18.5\n     19.4      1.145 16.95   <0.001 211.6  17.2   21.7\n     14.8      2.017  7.33   <0.001  42.0  10.8   18.7\n     21.5      1.072 20.02   <0.001 293.8  19.4   23.6\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \n\n\nNow, we use the comparisons() function to compute the difference in predicted outcome when each of the predictors is incremented by 1 unit (one predictor at a time, holding all others constant). Once again, comparisons are unit-level quantities. And since there are 3 predictors in the model and our data has 32 rows, we obtain 96 comparisons:\n\ncmp <- comparisons(mod)\n\nnrow(cmp)\n\n[1] 96\n\ncmp\n\n\n Term Contrast Estimate Std. Error      z Pr(>|z|)   S   2.5 %    97.5 %\n   hp    +1     -0.0369     0.0185 -1.995  0.04607 4.4 -0.0732 -0.000643\n   hp    +1     -0.0287     0.0156 -1.836  0.06640 3.9 -0.0593  0.001942\n   hp    +1     -0.0466     0.0226 -2.062  0.03922 4.7 -0.0908 -0.002302\n   hp    +1     -0.0423     0.0133 -3.182  0.00146 9.4 -0.0683 -0.016238\n   hp    +1     -0.0390     0.0134 -2.909  0.00362 8.1 -0.0653 -0.012734\n--- 86 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   am    1 - 0   4.0807     3.9351  1.037  0.29973 1.7 -3.6319 11.793387\n   am    1 - 0   2.1064     2.2892  0.920  0.35751 1.5 -2.3804  6.593103\n   am    1 - 0   0.8951     1.6442  0.544  0.58618 0.8 -2.3275  4.117620\n   am    1 - 0   4.0272     3.2402  1.243  0.21391 2.2 -2.3235 10.377969\n   am    1 - 0  -0.2369     1.5864 -0.149  0.88129 0.2 -3.3462  2.872416\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, wt, am \n\n\nThe comparisons() function allows customized queries. For example, what happens to the predicted outcome when the hp variable increases from 100 to 120?\n\ncomparisons(mod, variables = list(hp = c(120, 100)))\n\n\n Term  Contrast Estimate Std. Error      z Pr(>|z|)   S  2.5 %  97.5 %\n   hp 120 - 100   -0.738      0.370 -1.995  0.04607 4.4 -1.463 -0.0129\n   hp 120 - 100   -0.574      0.313 -1.836  0.06640 3.9 -1.186  0.0388\n   hp 120 - 100   -0.931      0.452 -2.062  0.03922 4.7 -1.817 -0.0460\n   hp 120 - 100   -0.845      0.266 -3.182  0.00146 9.4 -1.366 -0.3248\n   hp 120 - 100   -0.780      0.268 -2.909  0.00362 8.1 -1.306 -0.2547\n--- 22 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   hp 120 - 100   -1.451      0.705 -2.058  0.03958 4.7 -2.834 -0.0692\n   hp 120 - 100   -0.384      0.270 -1.422  0.15498 2.7 -0.912  0.1451\n   hp 120 - 100   -0.641      0.334 -1.918  0.05513 4.2 -1.297  0.0141\n   hp 120 - 100   -0.126      0.272 -0.463  0.64360 0.6 -0.659  0.4075\n   hp 120 - 100   -0.635      0.332 -1.911  0.05598 4.2 -1.286  0.0162\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, wt, am \n\n\nWhat happens to the predicted outcome when the wt variable increases by 1 standard deviation about its mean?\n\ncomparisons(mod, variables = list(hp = \"sd\"))\n\n\n Term                Contrast Estimate Std. Error      z Pr(>|z|)   S 2.5 %  97.5 %\n   hp (x + sd/2) - (x - sd/2)   -2.530      1.269 -1.995  0.04607 4.4 -5.02 -0.0441\n   hp (x + sd/2) - (x - sd/2)   -1.967      1.072 -1.836  0.06640 3.9 -4.07  0.1332\n   hp (x + sd/2) - (x - sd/2)   -3.193      1.549 -2.062  0.03922 4.7 -6.23 -0.1578\n   hp (x + sd/2) - (x - sd/2)   -2.898      0.911 -3.182  0.00146 9.4 -4.68 -1.1133\n   hp (x + sd/2) - (x - sd/2)   -2.675      0.919 -2.909  0.00362 8.1 -4.48 -0.8731\n--- 22 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n   hp (x + sd/2) - (x - sd/2)   -4.976      2.418 -2.058  0.03958 4.7 -9.71 -0.2373\n   hp (x + sd/2) - (x - sd/2)   -1.315      0.925 -1.422  0.15498 2.7 -3.13  0.4974\n   hp (x + sd/2) - (x - sd/2)   -2.199      1.147 -1.918  0.05513 4.2 -4.45  0.0483\n   hp (x + sd/2) - (x - sd/2)   -0.432      0.933 -0.463  0.64360 0.6 -2.26  1.3970\n   hp (x + sd/2) - (x - sd/2)   -2.177      1.139 -1.911  0.05598 4.2 -4.41  0.0556\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, wt, am \n\n\nThe comparisons() function also allows users to specify arbitrary functions of predictions, with the comparison argument. For example, what is the average ratio between predicted Miles per Gallon after an increase of 50 units in Horsepower?\n\ncomparisons(\n  mod,\n  variables = list(hp = 50),\n  comparison = \"ratioavg\")\n\n\n Term  Contrast Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n   hp mean(+50)     0.91     0.0291 31.3   <0.001 712.0 0.853  0.966\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n\nSee the Comparisons vignette for detailed explanations and more options.\nThe slopes() function allows us to compute the partial derivative of the outcome equation with respect to each of the predictors. Once again, we obtain a data frame with 96 rows:\n\nmfx <- slopes(mod)\n\nnrow(mfx)\n\n[1] 96\n\nmfx\n\n\n Term Contrast Estimate Std. Error      z Pr(>|z|)   S   2.5 %    97.5 %\n   hp    dY/dX  -0.0369     0.0185 -1.995  0.04607 4.4 -0.0732 -0.000643\n   hp    dY/dX  -0.0287     0.0156 -1.836  0.06640 3.9 -0.0593  0.001942\n   hp    dY/dX  -0.0466     0.0226 -2.062  0.03922 4.7 -0.0908 -0.002302\n   hp    dY/dX  -0.0423     0.0133 -3.182  0.00146 9.4 -0.0683 -0.016238\n   hp    dY/dX  -0.0390     0.0134 -2.909  0.00362 8.1 -0.0653 -0.012734\n--- 86 rows omitted. See ?avg_slopes and ?print.marginaleffects --- \n   am    1 - 0   4.0807     3.9351  1.037  0.29973 1.7 -3.6319 11.793387\n   am    1 - 0   2.1064     2.2892  0.920  0.35751 1.5 -2.3804  6.593103\n   am    1 - 0   0.8951     1.6442  0.544  0.58618 0.8 -2.3275  4.117620\n   am    1 - 0   4.0272     3.2402  1.243  0.21391 2.2 -2.3235 10.377969\n   am    1 - 0  -0.2369     1.5864 -0.149  0.88129 0.2 -3.3462  2.872416\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, wt, am"
  },
  {
    "objectID": "articles/marginaleffects.html#grid",
    "href": "articles/marginaleffects.html#grid",
    "title": "\n1  Get Started\n",
    "section": "\n1.3 Grid",
    "text": "1.3 Grid\nPredictions, comparisons, and slopes are typically “conditional” quantities which depend on the values of all the predictors in the model. By default, marginaleffects functions estimate quantities of interest for the empirical distribution of the data (i.e., for each row of the original dataset). However, users can specify the exact values of the predictors they want to investigate by using the newdata argument.\nnewdata accepts data frames, shortcut strings, or a call to the datagrid() function. For example, to compute the predicted outcome for a hypothetical car with all predictors equal to the sample mean or median, we can do:\n\npredictions(mod, newdata = \"mean\")\n\n\n Estimate Std. Error  z Pr(>|z|)     S 2.5 % 97.5 %  hp   wt    am\n     18.4       0.68 27   <0.001 531.7    17   19.7 147 3.22 0.406\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \n\npredictions(mod, newdata = \"median\")\n\n\n Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp   wt    am\n     18.7      0.819 22.8   <0.001 379.8  17.1   20.3 123 3.33 0.406\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, wt, am \n\n\nThe datagrid function gives us a powerful way to define a grid of predictors. All the variables not mentioned explicitly in datagrid() are fixed to their mean or mode:\n\npredictions(\n  mod,\n  newdata = datagrid(\n    am = c(0, 1),\n    wt = range))\n\n\n Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %  hp am   wt\n     23.3       2.71 8.60   <0.001 56.7 17.96   28.6 147  0 1.51\n     12.8       2.98 4.30   <0.001 15.8  6.96   18.6 147  0 5.42\n     27.1       2.85 9.52   <0.001 69.0 21.56   32.7 147  1 1.51\n      5.9       5.81 1.01     0.31  1.7 -5.50   17.3 147  1 5.42\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am, wt \n\n\nThe same mechanism is available in comparisons() and slopes(). To estimate the partial derivative of mpg with respect to wt, when am is equal to 0 and 1, while other predictors are held at their means:\n\nslopes(\n  mod,\n  variables = \"wt\",\n  newdata = datagrid(am = 0:1))\n\n\n Term Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %  hp   wt am\n   wt    -2.68       1.42 -1.89   0.0594 4.1 -5.46  0.106 147 3.22  0\n   wt    -5.43       2.15 -2.52   0.0116 6.4 -9.65 -1.214 147 3.22  1\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, wt, am \n\n\nWe can also plot how predictions, comparisons, or slopes change across different values of the predictors using three powerful plotting functions:\n\n\nplot_predictions: Conditional Adjusted Predictions\n\nplot_comparisons: Conditional Comparisons\n\nplot_slopes: Conditional Marginal Effects\n\nFor example, this plot shows the outcomes predicted by our model for different values of the wt and am variables:\n\nplot_predictions(mod, condition = list(\"hp\", \"wt\" = \"threenum\", \"am\"))\n\n\n\n\nThis plot shows how the derivative of mpg with respect to am varies as a function of wt and hp:\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"wt\" = \"minmax\"))\n\n\n\n\nSee this vignette for more information: Plots, interactions, predictions, contrasts, and slopes"
  },
  {
    "objectID": "articles/marginaleffects.html#averaging",
    "href": "articles/marginaleffects.html#averaging",
    "title": "\n1  Get Started\n",
    "section": "\n1.4 Averaging",
    "text": "1.4 Averaging\nSince predictions, comparisons, and slopes are conditional quantities, they can be a bit unwieldy. Often, it can be useful to report a one-number summary instead of one estimate per observation. Instead of presenting “conditional” estimates, some methodologists recommend reporting “marginal” estimates, that is, an average of unit-level estimates.\n(This use of the word “marginal” as “averaging” should not be confused with the term “marginal effect” which, in the econometrics tradition, corresponds to a partial derivative, or the effect of a “small/marginal” change.)\nTo marginalize (average over) our unit-level estimates, we can use the by argument or the one of the convenience functions: avg_predictions(), avg_comparisons(), or avg_slopes(). For example, both of these commands give us the same result: the average predicted outcome in the mtcars dataset:\n\navg_predictions(mod)\n\n\n Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %\n     20.1       0.39 51.5   <0.001 Inf  19.3   20.9\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nThis is equivalent to manual computation by:\n\nmean(predict(mod))\n\n[1] 20.09062\n\n\nThe main marginaleffects functions all include a by argument, which allows us to marginalize within sub-groups of the data. For example,\n\navg_comparisons(mod, by = \"am\")\n\n\n Term          Contrast am Estimate Std. Error      z Pr(>|z|)   S   2.5 %   97.5 %\n   hp mean(+1)           1  -0.0436     0.0213 -2.050  0.04039 4.6 -0.0854 -0.00191\n   hp mean(+1)           0  -0.0343     0.0159 -2.160  0.03079 5.0 -0.0654 -0.00317\n   wt mean(+1)           1  -6.0718     1.9762 -3.072  0.00212 8.9 -9.9451 -2.19846\n   wt mean(+1)           0  -2.4799     1.2316 -2.014  0.04406 4.5 -4.8939 -0.06595\n   am mean(1) - mean(0)  1   1.9029     2.3086  0.824  0.40980 1.3 -2.6219  6.42773\n   am mean(1) - mean(0)  0  -1.3830     2.5250 -0.548  0.58388 0.8 -6.3319  3.56589\n\nColumns: term, contrast, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n\nMarginal Means are a special case of predictions, which are marginalized (or averaged) across a balanced grid of categorical predictors. To illustrate, we estimate a new model with categorical predictors:\n\ndat <- mtcars\ndat$am <- as.logical(dat$am)\ndat$cyl <- as.factor(dat$cyl)\nmod_cat <- lm(mpg ~ am + cyl + hp, data = dat)\n\nWe can compute marginal means manually using the functions already described:\n\navg_predictions(\n  mod_cat,\n  newdata = datagrid(cyl = unique, am = unique),\n  by = \"am\")\n\n\n    am Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n  TRUE     22.5      0.834 26.9   <0.001 528.6  20.8   24.1\n FALSE     18.3      0.785 23.3   <0.001 397.4  16.8   19.9\n\nColumns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nFor convenience, the marginaleffects package also includes a marginal_means() function:\n\nmarginal_means(mod_cat, variables = \"am\")\n\n\n Term Value Mean Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n   am  TRUE 22.5      0.834 26.9   <0.001 528.6  20.8   24.1\n   am FALSE 18.3      0.785 23.3   <0.001 397.4  16.8   19.9\n\nResults averaged over levels of: cyl, am \nColumns: term, value, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nThe Marginal Means vignette offers more detail."
  },
  {
    "objectID": "articles/marginaleffects.html#hypothesis-and-equivalence-tests",
    "href": "articles/marginaleffects.html#hypothesis-and-equivalence-tests",
    "title": "\n1  Get Started\n",
    "section": "\n1.5 Hypothesis and equivalence tests",
    "text": "1.5 Hypothesis and equivalence tests\nThe hypotheses() function and the hypothesis argument can be used to conduct linear and non-linear hypothesis tests on model coefficients, or on any of the quantities computed by the functions introduced above.\nConsider this model:\n\nmod <- lm(mpg ~ qsec * drat, data = mtcars)\ncoef(mod)\n\n(Intercept)        qsec        drat   qsec:drat \n 12.3371987  -1.0241183  -3.4371461   0.5973153 \n\n\nCan we reject the null hypothesis that the drat coefficient is 2 times the size of the qsec coefficient?\n\nhypotheses(mod, \"drat = 2 * qsec\")\n\n\n            Term Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %   S\n drat = 2 * qsec    -1.39       10.8 -0.129    0.897 -22.5   19.7 0.2\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n\nWe can ask the same question but refer to parameters by position, with indices b1, b2, b3, etc.:\n\nhypotheses(mod, \"b3 = 2 * b2\")\n\n\n        Term Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %   S\n b3 = 2 * b2    -1.39       10.8 -0.129    0.897 -22.5   19.7 0.2\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n\nThe main functions in marginaleffects all have a hypothesis argument, which means that we can do complex model testing. For example, consider two slope estimates:\n\nslopes(\n  mod,\n  variables = \"drat\",\n  newdata = datagrid(qsec = range))\n\n\n Term Estimate Std. Error    z Pr(>|z|)   S  2.5 % 97.5 % drat qsec\n drat     5.22       3.79 1.38   0.1682 2.6 -2.206   12.7  3.6 14.5\n drat    10.24       5.16 1.98   0.0472 4.4  0.125   20.4  3.6 22.9\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, drat, qsec \n\n\nAre these two slopes significantly different from one another? To test this, we can use the hypothesis argument:\n\nslopes(\n  mod,\n  hypothesis = \"b1 = b2\",\n  variables = \"drat\",\n  newdata = datagrid(qsec = range))\n\n\n  Term Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n b1=b2    -5.02       8.52 -0.589    0.556 0.8 -21.7   11.7\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nNow, imagine that for theoretical (or substantive or clinical) reasons, we only care about slopes larger than 2. We can use the hypotheses() function to conduct an equivalence test:\n\navg_slopes(mod) |> hypotheses(equivalence = c(-2, 2))\n\n\n Term Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 % p (NonInf) p (NonSup) p (Equiv)\n qsec     1.12      0.433 2.60  0.00945  6.7 0.275   1.97     <0.001     0.0216    0.0216\n drat     7.22      1.365 5.29  < 0.001 23.0 4.548   9.90     <0.001     0.9999    0.9999\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n\n\nSee the Hypothesis Tests and Custom Contrasts vignette for background, details, and for instructions on how to conduct hypothesis tests in more complex situations."
  },
  {
    "objectID": "articles/marginaleffects.html#more",
    "href": "articles/marginaleffects.html#more",
    "title": "\n1  Get Started\n",
    "section": "\n1.6 More!",
    "text": "1.6 More!\nThere is much more you can do with marginaleffects. Return to the Table of Contents to read the vignettes, learn how to report marginal effects in nice tables with the modelsummary package, how to define your own prediction “grid”, and much more. ****"
  },
  {
    "objectID": "articles/predictions.html#prediction-type-or-scale",
    "href": "articles/predictions.html#prediction-type-or-scale",
    "title": "\n2  Predictions\n",
    "section": "\n2.1 Prediction type (or scale)",
    "text": "2.1 Prediction type (or scale)\nUsing the type argument of the predictions() function we can specify the “scale” on which to make predictions. This refers to either the scale used to estimate the model (i.e., link scale) or to a more interpretable scale (e.g., response scale). For example, when fitting a linear regression model using the lm() function, the link scale and the response scale are identical. An “Adjusted Prediction” computed on either scale will be expressed as the mean value of the response variable at the given values of the predictor variables.\nOn the other hand, when fitting a binary logistic regression model using the glm() function (which uses a binomial family and a logit link ), the link scale and the response scale will be different: an “Adjusted Prediction” computed on the link scale will be expressed as a log odds of a “successful” response at the given values of the predictor variables, whereas an “Adjusted Prediction” computed on the response scale will be expressed as a probability that the response variable equals 1.\nThe default value of the type argument for most models is “response”, which means that the predictions() function will compute predicted probabilities (binomial family), Poisson means (poisson family), etc."
  },
  {
    "objectID": "articles/predictions.html#prediction-grid",
    "href": "articles/predictions.html#prediction-grid",
    "title": "\n2  Predictions\n",
    "section": "\n2.2 Prediction grid",
    "text": "2.2 Prediction grid\nTo compute adjusted predictions we must first specify the values of the predictors to consider: a “reference grid.” For example, if our model is a linear model fitted with the lm() function which relates the response variable Happiness with the predictor variables Age, Gender and Income, the reference grid could be a data.frame with values for Age, Gender and Income: Age = 40, Gender = Male, Income = 60000.\nThe “reference grid” may or may not correspond to actual observations in the dataset used to fit the model; the example values given above could match the mean values of each variable, or they could represent a specific observed (or hypothetical) individual. The reference grid can include many different rows if we want to make predictions for different combinations of predictors. By default, the predictions() function uses the full original dataset as a reference grid, which means it will compute adjusted predictions for each of the individuals observed in the dataset that was used to fit the model."
  },
  {
    "objectID": "articles/predictions.html#the-predictions-function",
    "href": "articles/predictions.html#the-predictions-function",
    "title": "\n2  Predictions\n",
    "section": "\n2.3 The predictions() function",
    "text": "2.3 The predictions() function\nBy default, predictions() calculates the regression-adjusted predicted values for every observation in the original dataset:\n\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\npred <- predictions(mod)\n\nhead(pred)\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      20.0      1.204 16.6   <0.001 204.1  17.7   22.4\n#>      20.0      1.204 16.6   <0.001 204.1  17.7   22.4\n#>      26.4      0.962 27.5   <0.001 549.0  24.5   28.3\n#>      20.0      1.204 16.6   <0.001 204.1  17.7   22.4\n#>      15.9      0.992 16.0   <0.001 190.0  14.0   17.9\n#>      20.2      1.219 16.5   <0.001 201.8  17.8   22.5\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl\n\nIn many cases, this is too limiting, and researchers will want to specify a grid of “typical” values over which to compute adjusted predictions."
  },
  {
    "objectID": "articles/predictions.html#adjusted-predictions-at-user-specified-values-aka-adjusted-predictions-at-representative-values-apr",
    "href": "articles/predictions.html#adjusted-predictions-at-user-specified-values-aka-adjusted-predictions-at-representative-values-apr",
    "title": "\n2  Predictions\n",
    "section": "\n2.4 Adjusted Predictions at User-Specified values (aka Adjusted Predictions at Representative values, APR)",
    "text": "2.4 Adjusted Predictions at User-Specified values (aka Adjusted Predictions at Representative values, APR)\nThere are two main ways to select the reference grid over which we want to compute adjusted predictions. The first is using the variables argument. The second is with the newdata argument and the datagrid() function\n\n2.4.1 variables: Counterfactual predictions\nThe variables argument is a handy way to create and make predictions on counterfactual datasets. For example, here the dataset that we used to fit the model has 32 rows. The counterfactual dataset with two distinct values of hp has 64 rows: each of the original rows appears twice, that is, once with each of the values that we specified in the variables argument:\n\np <- predictions(mod, variables = list(hp = c(100, 120)))\nhead(p)\n#> \n#>  Estimate Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n#>      20.3      1.238 16.38   <0.001 198.0  17.9   22.7\n#>      20.3      1.238 16.38   <0.001 198.0  17.9   22.7\n#>      26.2      0.986 26.63   <0.001 516.6  24.3   28.2\n#>      20.3      1.238 16.38   <0.001 198.0  17.9   22.7\n#>      17.7      1.881  9.42   <0.001  67.6  14.0   21.4\n#>      20.3      1.238 16.38   <0.001 198.0  17.9   22.7\n#> \n#> Columns: rowid, rowidcf, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, hp\nnrow(p)\n#> [1] 64\n\n\n2.4.2 newdata and datagrid\n\nA second strategy to construct grids of predictors for adjusted predictions is to combine the newdata argument and the datagrid() function. Recall that this function creates a “typical” dataset with all variables at their means or modes, except those we explicitly define:\n\ndatagrid(cyl = c(4, 6, 8), model = mod)\n#>        mpg       hp cyl\n#> 1 20.09062 146.6875   4\n#> 2 20.09062 146.6875   6\n#> 3 20.09062 146.6875   8\n\nWe can also use this datagrid() function in a predictions() call (omitting the model argument):\n\npredictions(mod, newdata = datagrid())\n#> \n#>  Estimate Std. Error  z Pr(>|z|)     S 2.5 % 97.5 %  hp cyl\n#>      16.6       1.28 13   <0.001 125.6  14.1   19.1 147   8\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl\n\npredictions(mod, newdata = datagrid(cyl = c(4, 6, 8)))\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp cyl\n#>      25.1       1.37 18.4   <0.001 247.5  22.4   27.8 147   4\n#>      19.2       1.25 15.4   <0.001 174.5  16.7   21.6 147   6\n#>      16.6       1.28 13.0   <0.001 125.6  14.1   19.1 147   8\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl\n\nUsers can change the summary function used to summarize each type of variables using the FUN_numeric, FUN_factor, and related arguments. For example:\n\nm <- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp drat cyl am\n#>      22.0       1.29 17.0   <0.001 214.0  19.4   24.5 123  3.7   6  1\n#>      18.2       1.27 14.3   <0.001 151.9  15.7   20.7 123  3.7   6  0\n#>      25.5       1.32 19.3   <0.001 274.0  23.0   28.1 123  3.7   4  1\n#>      21.8       1.54 14.1   <0.001 148.3  18.8   24.8 123  3.7   4  0\n#>      22.6       2.14 10.6   <0.001  84.2  18.4   26.8 123  3.7   8  1\n#>      18.9       1.73 10.9   <0.001  89.0  15.5   22.3 123  3.7   8  0\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, drat, cyl, am\n\nThe data.frame produced by predictions() is “tidy”, which makes it easy to manipulate with other R packages and functions:\n\nlibrary(kableExtra)\nlibrary(tidyverse)\n\npredictions(\n    mod,\n    newdata = datagrid(cyl = mtcars$cyl, hp = c(100, 110))) |>\n    select(hp, cyl, estimate) |>\n    pivot_wider(values_from = estimate, names_from = cyl) |>\n    kbl(caption = \"A table of Adjusted Predictions\") |>\n    kable_styling() |>\n    add_header_above(header = c(\" \" = 1, \"cyl\" = 3))\n\n\n\nA table of Adjusted Predictions\n \n\n\ncyl\n\n\n hp \n    6 \n    4 \n    8 \n  \n\n\n\n 100 \n    20.27858 \n    26.24623 \n    17.72538 \n  \n\n 110 \n    20.03819 \n    26.00585 \n    17.48500 \n  \n\n\n\n\n\n2.4.3 counterfactual data grid\nAn alternative approach to construct grids of predictors is to use grid_type = \"counterfactual\" argument value. This will duplicate the whole dataset, with the different values specified by the user.\nFor example, the mtcars dataset has 32 rows. This command produces a new dataset with 64 rows, with each row of the original dataset duplicated with the two values of the am variable supplied (0 and 1):\n\nmod <- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\nnd <- datagrid(model = mod, am = 0:1, grid_type = \"counterfactual\")\n\ndim(nd)\n#> [1] 64  4\n\nThen, we can use this dataset and the predictions() function to create interesting visualizations:\n\npred <- predictions(mod, newdata = datagrid(am = 0:1, grid_type = \"counterfactual\")) |>\n    select(am, estimate, rowidcf) |>\n    pivot_wider(id_cols = rowidcf, \n                names_from = am,\n                values_from = estimate)\n\nggplot(pred, aes(x = `0`, y = `1`)) +\n    geom_point() +\n    geom_abline(intercept = 0, slope = 1) +\n    labs(x = \"Predicted Pr(vs=1), when am = 0\",\n         y = \"Predicted Pr(vs=1), when am = 1\")\n\n\n\n\nIn this graph, each dot represents the predicted probability that vs=1 for one observation of the dataset, in the counterfactual worlds where am is either 0 or 1."
  },
  {
    "objectID": "articles/predictions.html#adjusted-prediction-at-the-mean-apm",
    "href": "articles/predictions.html#adjusted-prediction-at-the-mean-apm",
    "title": "\n2  Predictions\n",
    "section": "\n2.5 Adjusted Prediction at the Mean (APM)",
    "text": "2.5 Adjusted Prediction at the Mean (APM)\nSome analysts may want to calculate an “Adjusted Prediction at the Mean,” that is, the predicted outcome when all the regressors are held at their mean (or mode). To achieve this, we use the datagrid() function. By default, this function produces a grid of data with regressors at their means or modes, so all we need to do to get the APM is:\n\npredictions(mod, newdata = \"mean\")\n#> \n#>  Estimate Pr(>|z|)   S   2.5 % 97.5 %  hp    am\n#>    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#> \n#> Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am\n\nThis is equivalent to calling:\n\npredictions(mod, newdata = datagrid())\n#> \n#>  Estimate Pr(>|z|)   S   2.5 % 97.5 %  hp    am\n#>    0.0631   0.0656 3.9 0.00379  0.543 147 0.406\n#> \n#> Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, vs, hp, am"
  },
  {
    "objectID": "articles/predictions.html#average-adjusted-predictions-aap",
    "href": "articles/predictions.html#average-adjusted-predictions-aap",
    "title": "\n2  Predictions\n",
    "section": "\n2.6 Average Adjusted Predictions (AAP)",
    "text": "2.6 Average Adjusted Predictions (AAP)\nAn “Average Adjusted Prediction” is the outcome of a two step process:\n\nCreate a new dataset with each of the original regressor values, but fixing some regressors to values of interest.\nTake the average of the predicted values in this new dataset.\n\nWe can obtain AAPs by applying the avg_*() functions or by argument:\n\nmodlin <- lm(mpg ~ hp + factor(cyl), mtcars)\navg_predictions(modlin)\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      20.1      0.556 36.1   <0.001 946.7    19   21.2\n#> \n#> Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThis is equivalent to:\n\npred <- predictions(modlin)\nmean(pred$estimate)\n#> [1] 20.09062\n\nNote that in GLM models with a non-linear link function, the predictions are first made on the link scale, averaged, and then back transformed. Thus, the average prediction may not be exactly identical to the average of predictions:\n\nmod <- glm(vs ~ hp + am, data = mtcars, family = binomial)\n\navg_predictions(mod)$estimate\n#> [1] 0.06308965\n\n## Step 1: predict on the link scale\np <- predictions(mod, type = \"link\")$estimate\n## Step 2: average\np <- mean(p)\n## Step 3: backtransform\nmod$family$linkinv(p)\n#> [1] 0.06308965\n\nUsers who want the average of individual-level predictions on the response scale can specify the type argument explicitly:\n\navg_predictions(mod, type = \"response\")\n#> \n#>  Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>     0.437     0.0429 10.2   <0.001 78.8 0.353  0.522\n#> \n#> Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/predictions.html#average-adjusted-predictions-by-group",
    "href": "articles/predictions.html#average-adjusted-predictions-by-group",
    "title": "\n2  Predictions\n",
    "section": "\n2.7 Average Adjusted Predictions by Group",
    "text": "2.7 Average Adjusted Predictions by Group\nWe can compute average adjusted predictions for different subsets of the data with the by argument.\n\npredictions(mod, by = \"am\")\n#> \n#>  am Estimate Pr(>|z|)   S   2.5 % 97.5 %\n#>   1   0.0694   0.0755 3.7 0.00424  0.566\n#>   0   0.0591   0.1163 3.1 0.00198  0.665\n#> \n#> Columns: am, estimate, p.value, s.value, conf.low, conf.high\n\nIn the next example, we create a “counterfactual” data grid where each observation of the dataset is repeated twice, with different values of the am variable, and all other variables held at the observed values. We also show the equivalent results using dplyr:\n\npredictions(\n    mod,\n    type = \"response\",\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#> \n#>  am Estimate Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n#>   0    0.526     0.0330 15.93   <0.001 187.3 0.461  0.591\n#>   1    0.330     0.0646  5.11   <0.001  21.6 0.204  0.457\n#> \n#> Columns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\npredictions(\n    mod,\n    type = \"response\",\n    newdata = datagridcf(am = 0:1)) |>\n    group_by(am) |>\n    summarize(AAP = mean(estimate))\n#> # A tibble: 2 × 2\n#>      am   AAP\n#>   <int> <dbl>\n#> 1     0 0.526\n#> 2     1 0.330\n\nNote that the two results are exactly identical when we specify type=\"response\" explicitly. However, they will differ slightly when we leave type unspecified, because marginaleffects will then automatically make predictions and average on the link scale, before backtransforming:\n\npredictions(\n    mod,\n    by = \"am\",\n    newdata = datagridcf(am = 0:1))\n#> \n#>  am Estimate Pr(>|z|)   S    2.5 % 97.5 %\n#>   0  0.24043   0.3922 1.4 2.22e-02  0.815\n#>   1  0.00696   0.0359 4.8 6.81e-05  0.419\n#> \n#> Columns: am, estimate, p.value, s.value, conf.low, conf.high\n\npredictions(\n    mod,\n    type = \"link\",\n    newdata = datagridcf(am = 0:1)) |>\n    group_by(am) |>\n    summarize(AAP = mod$family$linkinv(mean(estimate)))\n#> # A tibble: 2 × 2\n#>      am     AAP\n#>   <int>   <dbl>\n#> 1     0 0.240  \n#> 2     1 0.00696\n\n\n2.7.1 Multinomial models\nOne place where this is particularly useful is in multinomial models with different response levels. For example, here we compute the average predicted outcome for each outcome level in a multinomial logit model. Note that response levels are identified by the “group” column.\n\nlibrary(nnet)\nnom <- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n## first 5 raw predictions\npredictions(nom, type = \"probs\") |> head()\n#> \n#>  Group Estimate Std. Error        z Pr(>|z|)   S     2.5 %   97.5 %\n#>      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#>      3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n#>      3 9.35e-08   6.91e-06   0.0135   0.9892 0.0 -1.35e-05 1.36e-05\n#>      3 4.04e-01   1.97e-01   2.0569   0.0397 4.7  1.91e-02 7.90e-01\n#>      3 1.00e+00   1.25e-03 802.4451   <0.001 Inf  9.98e-01 1.00e+00\n#>      3 5.18e-01   2.90e-01   1.7884   0.0737 3.8 -4.97e-02 1.09e+00\n#> \n#> Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, am, vs\n\n## average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n#> \n#>  Group Estimate Std. Error     z Pr(>|z|)     S  2.5 % 97.5 %\n#>      3    0.469     0.0404 11.60   <0.001 100.9 0.3895  0.548\n#>      4    0.375     0.0614  6.11   <0.001  29.9 0.2546  0.495\n#>      5    0.156     0.0462  3.38   <0.001  10.4 0.0656  0.247\n#> \n#> Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe can use custom aggregations by supplying a data frame to the by argument. All columns of this data frame must be present in the output of predictions(), and the data frame must also include a by column of labels. In this example, we “collapse” response groups:\n\nby <- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n#> \n#>  Estimate Std. Error     z Pr(>|z|)     S  2.5 % 97.5 %  By\n#>     0.422     0.0231 18.25   <0.001 244.7 0.3766  0.467 3,4\n#>     0.156     0.0462  3.38   <0.001  10.4 0.0656  0.247 5  \n#> \n#> Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by\n\nThis can be very useful in combination with the hypothesis argument. For example, here we compute the difference between average adjusted predictions for the 3 and 4 response levels, compared to the 5 response level:\n\npredictions(nom, type = \"probs\", by = by, hypothesis = \"sequential\")\n#> \n#>     Term Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>  5 - 3,4   -0.266     0.0694 -3.83   <0.001 12.9 -0.402  -0.13\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe can also use more complicated aggregations. Here, we compute the predicted probability of outcome levels for each value of cyl, by collapsing the “3” and “4” outcome levels:\n\nnom <- multinom(factor(gear) ~ mpg + factor(cyl), data = mtcars, trace = FALSE)\n\nby <- expand.grid(\n    group = 3:5,\n    cyl = c(4, 6, 8),\n    stringsAsFactors = TRUE) |>\n    # define labels\n    transform(by = ifelse(\n        group %in% 3:4,\n        sprintf(\"3/4 Gears & %s Cylinders\", cyl),\n        sprintf(\"5 Gears & %s Cylinders\", cyl)))\n\npredictions(nom, by = by)\n#> \n#>  Estimate Std. Error    z Pr(>|z|)    S   2.5 % 97.5 %                      By\n#>     0.429     0.0661 6.49   <0.001 33.4  0.2991  0.558 3/4 Gears & 6 Cylinders\n#>     0.409     0.0580 7.06   <0.001 39.1  0.2956  0.523 3/4 Gears & 4 Cylinders\n#>     0.429     0.0458 9.35   <0.001 66.6  0.3387  0.518 3/4 Gears & 8 Cylinders\n#>     0.143     0.1321 1.08    0.280  1.8 -0.1161  0.402 5 Gears & 6 Cylinders  \n#>     0.182     0.1159 1.57    0.117  3.1 -0.0457  0.409 5 Gears & 4 Cylinders  \n#>     0.143     0.0917 1.56    0.119  3.1 -0.0368  0.323 5 Gears & 8 Cylinders  \n#> \n#> Columns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by\n\nAnd we can then compare the different groups using the hypothesis argument:\n\npredictions(nom, by = by, hypothesis = \"pairwise\")\n#> \n#>                                               Term  Estimate Std. Error         z Pr(>|z|)   S    2.5 % 97.5 %\n#>  3/4 Gears & 6 Cylinders - 3/4 Gears & 4 Cylinders  1.93e-02     0.0879  0.219838   0.8260 0.3 -0.15294  0.192\n#>  3/4 Gears & 6 Cylinders - 3/4 Gears & 8 Cylinders  2.84e-05     0.0804  0.000353   0.9997 0.0 -0.15757  0.158\n#>  3/4 Gears & 6 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1982  1.441537   0.1494 2.7 -0.10275  0.674\n#>  3/4 Gears & 6 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1334  1.851410   0.0641 4.0 -0.01449  0.509\n#>  3/4 Gears & 6 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1130  2.527631   0.0115 6.4  0.06415  0.507\n#>  3/4 Gears & 4 Cylinders - 3/4 Gears & 8 Cylinders -1.93e-02     0.0739 -0.261054   0.7941 0.3 -0.16414  0.126\n#>  3/4 Gears & 4 Cylinders - 5 Gears & 6 Cylinders    2.66e-01     0.1443  1.846188   0.0649 3.9 -0.01642  0.549\n#>  3/4 Gears & 4 Cylinders - 5 Gears & 4 Cylinders    2.28e-01     0.1739  1.309479   0.1904 2.4 -0.11312  0.569\n#>  3/4 Gears & 4 Cylinders - 5 Gears & 8 Cylinders    2.66e-01     0.1085  2.455117   0.0141 6.1  0.05371  0.479\n#>  3/4 Gears & 8 Cylinders - 5 Gears & 6 Cylinders    2.86e-01     0.1399  2.042634   0.0411 4.6  0.01156  0.560\n#>  3/4 Gears & 8 Cylinders - 5 Gears & 4 Cylinders    2.47e-01     0.1247  1.981364   0.0476 4.4  0.00267  0.491\n#>  3/4 Gears & 8 Cylinders - 5 Gears & 8 Cylinders    2.86e-01     0.1375  2.076746   0.0378 4.7  0.01606  0.555\n#>  5 Gears & 6 Cylinders - 5 Gears & 4 Cylinders     -3.86e-02     0.1758 -0.219838   0.8260 0.3 -0.38317  0.306\n#>  5 Gears & 6 Cylinders - 5 Gears & 8 Cylinders     -5.68e-05     0.1608 -0.000353   0.9997 0.0 -0.31526  0.315\n#>  5 Gears & 4 Cylinders - 5 Gears & 8 Cylinders      3.86e-02     0.1478  0.261054   0.7941 0.3 -0.25112  0.328\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n2.7.2 Bayesian models\nThe same strategy works for bayesian models:\n\nlibrary(brms)\nmod <- brm(am ~ mpg * vs, data = mtcars, family = bernoulli)\n\n\npredictions(mod, by = \"vs\")\n#> \n#>  vs Estimate 2.5 % 97.5 %\n#>   0    0.327 0.182  0.507\n#>   1    0.499 0.366  0.672\n#> \n#> Columns: vs, estimate, conf.low, conf.high\n\nThe results above show the median of the posterior distribution of group-wise means. Note that we take the mean of predicted values for each MCMC draw before computing quantiles. This is equivalent to:\n\ndraws <- posterior_epred(mod)\nquantile(rowMeans(draws[, mtcars$vs == 0]), probs = c(.5, .025, .975))\n#>       50%      2.5%     97.5% \n#> 0.3271836 0.1824479 0.5072074\nquantile(rowMeans(draws[, mtcars$vs == 1]), probs = c(.5, .025, .975))\n#>       50%      2.5%     97.5% \n#> 0.4993250 0.3657956 0.6721267"
  },
  {
    "objectID": "articles/predictions.html#conditional-adjusted-predictions-plot",
    "href": "articles/predictions.html#conditional-adjusted-predictions-plot",
    "title": "\n2  Predictions\n",
    "section": "\n2.8 Conditional Adjusted Predictions (Plot)",
    "text": "2.8 Conditional Adjusted Predictions (Plot)\nFirst, we download the ggplot2movies dataset from the RDatasets archive. Then, we create a variable called certified_fresh for movies with a rating of at least 8. Finally, we discard some outliers and fit a logistic regression model:\n\nlibrary(tidyverse)\ndat <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/ggplot2movies/movies.csv\") |>\n    mutate(style = case_when(Action == 1 ~ \"Action\",\n                             Comedy == 1 ~ \"Comedy\",\n                             Drama == 1 ~ \"Drama\",\n                             TRUE ~ \"Other\"),\n           style = factor(style),\n           certified_fresh = rating >= 8) |>\n    dplyr::filter(length < 240)\n\nmod <- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nWe can plot adjusted predictions, conditional on the length variable using the plot_predictions() function:\n\nmod <- glm(certified_fresh ~ length, data = dat, family = binomial)\n\nplot_predictions(mod, condition = \"length\")\n\n\n\n\nWe can also introduce another condition which will display a categorical variable like style in different colors. This can be useful in models with interactions:\n\nmod <- glm(certified_fresh ~ length * style, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"length\", \"style\"))\n\n\n\n\nSince the output of plot_predictions() is a ggplot2 object, it is very easy to customize. For example, we can add points for the actual observations of our dataset like so:\n\n##| fig.asp = .7\nlibrary(ggplot2)\nlibrary(ggrepel)\n\nmt <- mtcars\nmt$label <- row.names(mt)\n\nmod <- lm(mpg ~ hp, data = mt)\n\nplot_predictions(mod, condition = \"hp\") +\n    geom_point(aes(x = hp, y = mpg), data = mt) +\n    geom_rug(aes(x = hp, y = mpg), data = mt) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp > 250),\n                    nudge_y = 2) +\n    theme_classic()\n\n\n\n\nWe can also use plot_predictions() in models with multinomial outcomes or grouped coefficients. For example, notice that when we call draw=FALSE, the result includes a group column:\n\nlibrary(MASS)\nlibrary(ggplot2)\n\nmod <- nnet::multinom(factor(gear) ~ mpg, data = mtcars, trace = FALSE)\n\np <- plot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\",\n    draw = FALSE)\n\nhead(p)\n#>   rowid group  estimate  std.error statistic       p.value  s.value  conf.low conf.high gear      mpg\n#> 1     1     3 0.9714990 0.03872773  25.08536 7.185838e-139 458.9028 0.8955941  1.047404    3 10.40000\n#> 2     2     3 0.9656724 0.04393256  21.98079 4.397495e-107 353.3096 0.8795661  1.051779    3 10.87959\n#> 3     3     3 0.9586759 0.04963084  19.31613  3.930287e-83 273.7454 0.8614012  1.055951    3 11.35918\n#> 4     4     3 0.9502914 0.05579948  17.03047  4.880938e-65 213.6382 0.8409265  1.059656    3 11.83878\n#> 5     5     3 0.9402691 0.06238682  15.07160  2.490123e-51 168.1021 0.8179932  1.062545    3 12.31837\n#> 6     6     3 0.9283274 0.06930549  13.39472  6.492421e-41 133.5003 0.7924911  1.064164    3 12.79796\n\nNow we use the group column:\n\nplot_predictions(\n    mod,\n    type = \"probs\",\n    condition = \"mpg\") +\n    facet_wrap(~group)"
  },
  {
    "objectID": "articles/predictions.html#prediction-types",
    "href": "articles/predictions.html#prediction-types",
    "title": "\n2  Predictions\n",
    "section": "\n2.9 Prediction types",
    "text": "2.9 Prediction types\nThe predictions() function computes model-adjusted means on the scale of the output of the predict(model) function. By default, predict produces predictions on the \"response\" scale, so the adjusted predictions should be interpreted on that scale. However, users can pass a string to the type argument, and predictions() will consider different outcomes.\nTypical values include \"response\" and \"link\", but users should refer to the documentation of the predict of the package they used to fit the model to know what values are allowable. documentation.\n\nmod <- glm(am ~ mpg, family = binomial, data = mtcars)\npred <- predictions(mod, type = \"response\")\nhead(pred)\n#> \n#>  Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>     0.461     0.1158 3.98  < 0.001 13.8 0.2340  0.688\n#>     0.461     0.1158 3.98  < 0.001 13.8 0.2340  0.688\n#>     0.598     0.1324 4.52  < 0.001 17.3 0.3384  0.857\n#>     0.492     0.1196 4.11  < 0.001 14.6 0.2573  0.726\n#>     0.297     0.1005 2.95  0.00314  8.3 0.0999  0.494\n#>     0.260     0.0978 2.66  0.00787  7.0 0.0682  0.452\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg\n\npred <- predictions(mod, type = \"link\")\nhead(pred)\n#> \n#>  Estimate Std. Error       z Pr(>|z|)   S  2.5 %  97.5 %\n#>   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#>   -0.1559      0.466 -0.3345   0.7380 0.4 -1.070  0.7578\n#>    0.3967      0.551  0.7204   0.4713 1.1 -0.683  1.4761\n#>   -0.0331      0.479 -0.0692   0.9448 0.1 -0.971  0.9049\n#>   -0.8621      0.482 -1.7903   0.0734 3.8 -1.806  0.0817\n#>   -1.0463      0.509 -2.0575   0.0396 4.7 -2.043 -0.0496\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, am, mpg\n\nWe can also plot predictions on different outcome scales:\n\nplot_predictions(mod, condition = \"mpg\", type = \"response\")\n\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"link\")"
  },
  {
    "objectID": "articles/comparisons.html#simple-example-titanic",
    "href": "articles/comparisons.html#simple-example-titanic",
    "title": "\n3  Comparisons\n",
    "section": "\n3.1 Simple example: Titanic",
    "text": "3.1 Simple example: Titanic\nConsider a logistic regression model estimated using the Titanic mortality data:\n\nlibrary(marginaleffects)\n\ndat <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat <- read.csv(dat)\ndat$PClass[dat$PClass == \"*\"] <- NA\nmod <- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\n\n3.1.1 Step 1: Quantity\nThe question that interests us is:\n\nHow does the probability of survival (outcome) change if a passenger travels in 1st class vs. 3rd class?\n\nSince we are comparing two predicted outcomes, we will use the comparisons(). To indicate that our focal variable is PClass and that we are interested in the comparison between 1st and 3rd class, we will use the variables argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n\n\n3.1.2 Step 2: Grid\nIn GLM models, most quantities of interest are conditional, in the sense that they will typically depend on the values of all the predictors in the model. Therefore, we need to decide where in the predictor space we want to evaluate the quantity of interest described above.\nBy default, comparisons() will compute estimates for every row of the original dataset that was used to fit a model. There are 1313 observations in the titanic dataset. Therefore, if we just execute the code in the previous section, we will obtain 1313 estimates of the difference between the probability of survival in 3rd and 1st class:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#> \n#>    Term  Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>  PClass 3rd - 1st   -0.496     0.0610 -8.13  < 0.001 51.0 -0.616 -0.376\n#>  PClass 3rd - 1st   -0.472     0.1247 -3.79  < 0.001 12.7 -0.716 -0.228\n#>  PClass 3rd - 1st   -0.353     0.0641 -5.51  < 0.001 24.7 -0.478 -0.227\n#>  PClass 3rd - 1st   -0.493     0.0583 -8.45  < 0.001 54.9 -0.607 -0.379\n#>  PClass 3rd - 1st   -0.445     0.1452 -3.07  0.00216  8.9 -0.730 -0.161\n#> --- 746 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#>  PClass 3rd - 1st   -0.377     0.0703 -5.36  < 0.001 23.5 -0.515 -0.239\n#>  PClass 3rd - 1st   -0.384     0.0726 -5.30  < 0.001 23.0 -0.527 -0.242\n#>  PClass 3rd - 1st   -0.412     0.0821 -5.02  < 0.001 20.9 -0.573 -0.251\n#>  PClass 3rd - 1st   -0.399     0.0773 -5.16  < 0.001 22.0 -0.550 -0.247\n#>  PClass 3rd - 1st   -0.361     0.0661 -5.47  < 0.001 24.4 -0.490 -0.232\n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, Survived, PClass, SexCode, Age\n\nNotice that the contrast between 3rd and 1st is different from row to row. This reflects the fact that, in our model, moving from 1st to 3rd would have a different effect on the predicted probability of survival for different individuals.\nWe can be more specific in our query. Instead of using the empirical distribution as our “grid”, we can specify exactly where we want to evaluate the comparison in the predictor space, by using the newdata argument and the datagrid() function. For example, say I am interested in:\n\nThe effect of moving from 1st to 3rd class on the probability of survival for a 50 year old man and a 50 year old woman.\n\nI can type:\n\ncmp <- comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\ncmp\n#> \n#>    Term  Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 %  97.5 % Age SexCode\n#>  PClass 3rd - 1st   -0.184     0.0535 -3.45   <0.001 10.8 -0.289 -0.0796  50       0\n#>  PClass 3rd - 1st   -0.511     0.1242 -4.12   <0.001 14.7 -0.755 -0.2679  50       1\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, Survived, PClass, Age, SexCode\n\nWe now know that moving from 1st to 3rd changes by -0.184 the probability of survival for 50 year old men (SexCode=0), and by -0.511 the probability of survival for 50 year old women (SexCode=1).\n\n3.1.3 Step 3: Averaging\nAgain, by default comparisons() estimates quantities for all the actually observed units in our dataset. Sometimes, it is convenient to marginalize those conditional estimates, in order to obtain an “average contrast”:\n\navg_comparisons(mod,                          # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\"))) # Step 1: Quantity\n#> \n#>    Term  Contrast Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>  PClass 3rd - 1st   -0.396     0.0425 -9.3   <0.001 66.0 -0.479 -0.312\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nAlternatively, we could also take the average, but just of the two estimates that we computed above for the 50 year old man and 50 year old woman.\n\navg_comparisons(mod,                           # Step 3: Average\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1)) # Step 2: Grid\n#> \n#>    Term  Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  PClass 3rd - 1st   -0.348     0.0676 -5.15   <0.001 21.8 -0.48 -0.215\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNotice that this is exactly the same as the average in the estimates from the previous section, which we had stored as cmp:\n\ncmp$estimate\n#> [1] -0.1844289 -0.5113098\n\nmean(cmp$estimate)\n#> [1] -0.3478694\n\n\n3.1.4 Hypothesis\nFinally, imagine we are interested in this question:\n\nDoes moving from 1st to 3rd class have a bigger effect on the probability of survival for 50 year old men, or for 50 year old women?\n\nTo answer this, we use the hypothesis argument:\n\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  newdata = datagrid(Age = 50, SexCode = 0:1), # Step 2: Grid\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#> \n#>   Term Estimate Std. Error    z Pr(>|z|)   S  2.5 % 97.5 %\n#>  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThis result maps directly onto the estimates we had above. It is the difference in the contrast for 50-men and 50-women:\n\ndiff(cmp$estimate)\n#> [1] -0.3268809\n\nThis result can be interpreted as a “difference-in-differences”: Moving from 1st to 3rd has a much larger negative effect on the probability of survival for a 50 year old woman than for a 50 year old man. This difference is statistically significant.\nWe can do a similar comparison, but instead of fixing a conditional grid, we can average over subgroups of the empirical distribution, using the by argument:\n\navg_comparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),  # Step 1: Quantity\n  by = \"SexCode\",                              # Step 3: Average\n  hypothesis = \"b1 = b2\")                      # Step 4: Hypothesis\n#> \n#>   Term Estimate Std. Error     z Pr(>|z|)   S  2.5 %  97.5 %\n#>  b1=b2   -0.162     0.0845 -1.91   0.0558 4.2 -0.327 0.00402\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n3.1.5 Manual computation\nNow we show how to use the base R predict() function to compute some of the same quantities as above. This exercise may be clarifying for some users.\n\ngrid_50_1_3 <- data.frame(Age = 50, SexCode = 1, PClass = \"3rd\")\ngrid_50_1_1 <- data.frame(Age = 50, SexCode = 1, PClass = \"1st\")\ngrid_50_0_3 <- data.frame(Age = 50, SexCode = 0, PClass = \"3rd\")\ngrid_50_0_1 <- data.frame(Age = 50, SexCode = 0, PClass = \"1st\")\n\n\nyhat_50_1_3 <- predict(mod, newdata = grid_50_1_3, type = \"response\")\nyhat_50_1_1 <- predict(mod, newdata = grid_50_1_1, type = \"response\")\nyhat_50_0_3 <- predict(mod, newdata = grid_50_0_3, type = \"response\")\nyhat_50_0_1 <- predict(mod, newdata = grid_50_0_1, type = \"response\")\n\n## prediction on a grid\npredictions(mod, newdata = datagrid(Age = 50, SexCode = 1, PClass = \"3rd\"))\n#> \n#>  Estimate Pr(>|z|)   S 2.5 % 97.5 % Age SexCode PClass\n#>     0.446    0.661 0.6 0.235  0.679  50       1    3rd\n#> \n#> Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, Survived, Age, SexCode, PClass\nyhat_50_1_3\n#>         1 \n#> 0.4463379\n\n## contrast on a grid\ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1))\n#> \n#>    Term  Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 %  97.5 % Age SexCode\n#>  PClass 3rd - 1st   -0.184     0.0535 -3.45   <0.001 10.8 -0.289 -0.0796  50       0\n#>  PClass 3rd - 1st   -0.511     0.1242 -4.12   <0.001 14.7 -0.755 -0.2679  50       1\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, Survived, PClass, Age, SexCode\n\nyhat_50_0_3 - yhat_50_0_1\n#>          1 \n#> -0.1844289\nyhat_50_1_3 - yhat_50_1_1\n#>          1 \n#> -0.5113098\n\n## difference-in-differences \ncomparisons(mod,\n  variables = list(PClass = c(\"1st\", \"3rd\")),\n  newdata = datagrid(Age = 50, SexCode = 0:1),\n  hypothesis = \"b1 = b2\")\n#> \n#>   Term Estimate Std. Error    z Pr(>|z|)   S  2.5 % 97.5 %\n#>  b1=b2    0.327      0.135 2.42   0.0156 6.0 0.0618  0.592\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n(yhat_50_0_3 - yhat_50_0_1) - (yhat_50_1_3 - yhat_50_1_1)\n#>         1 \n#> 0.3268809\n\n## average of the empirical distribution of contrasts\navg_comparisons(mod, variables = list(PClass = c(\"1st\", \"3rd\")), by = \"SexCode\")\n#> \n#>    Term              Contrast SexCode Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>  PClass mean(3rd) - mean(1st)       1   -0.496     0.0623 -7.95   <0.001 49.0 -0.618 -0.374\n#>  PClass mean(3rd) - mean(1st)       0   -0.334     0.0570 -5.86   <0.001 27.7 -0.446 -0.222\n#> \n#> Columns: term, contrast, SexCode, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\ngrid_empirical_1_3 <- dat |> subset(SexCode == 1) |> transform(PClass = \"3rd\")\ngrid_empirical_1_1 <- dat |> subset(SexCode == 1) |> transform(PClass = \"1st\")\ngrid_empirical_0_3 <- dat |> subset(SexCode == 0) |> transform(PClass = \"3rd\")\ngrid_empirical_0_1 <- dat |> subset(SexCode == 0) |> transform(PClass = \"1st\")\nyhat_empirical_0_1 <- predict(mod, newdata = grid_empirical_0_1, type = \"response\")\nyhat_empirical_0_3 <- predict(mod, newdata = grid_empirical_0_3, type = \"response\")\nyhat_empirical_1_1 <- predict(mod, newdata = grid_empirical_1_1, type = \"response\")\nyhat_empirical_1_3 <- predict(mod, newdata = grid_empirical_1_3, type = \"response\")\nmean(yhat_empirical_0_3, na.rm = TRUE) - mean(yhat_empirical_0_1, na.rm = TRUE)\n#> [1] -0.3341426\nmean(yhat_empirical_1_3, na.rm = TRUE) - mean(yhat_empirical_1_1, na.rm = TRUE)\n#> [1] -0.4956673"
  },
  {
    "objectID": "articles/comparisons.html#predictor-types",
    "href": "articles/comparisons.html#predictor-types",
    "title": "\n3  Comparisons\n",
    "section": "\n3.2 Predictor types",
    "text": "3.2 Predictor types\n\n3.2.1 Logical and factor predictors\nConsider a simple model with a logical and a factor variable:\n\nlibrary(marginaleffects)\n\ntmp <- mtcars\ntmp$am <- as.logical(tmp$am)\nmod <- lm(mpg ~ am + factor(cyl), tmp)\n\nThe comparisons() function automatically computes contrasts for each level of the categorical variables, relative to the baseline category (FALSE for logicals, and the reference level for factors), while holding all other values at their observed values. The avg_comparisons() does the same, but then marginalizes by taking the average of unit-level estimates:\n\ncmp <- avg_comparisons(mod)\ncmp\n#> \n#>  Term     Contrast Estimate Std. Error     z Pr(>|z|)    S    2.5 % 97.5 %\n#>   am  TRUE - FALSE     2.56       1.30  1.97   0.0485  4.4   0.0167   5.10\n#>   cyl 6 - 4           -6.16       1.54 -4.01   <0.001 14.0  -9.1661  -3.15\n#>   cyl 8 - 4          -10.07       1.45 -6.93   <0.001 37.8 -12.9136  -7.22\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThe summary printed above says that moving from the reference category 4 to the level 6 on the cyl factor variable is associated with a change of -6.156 in the adjusted prediction. Similarly, the contrast from FALSE to TRUE on the am variable is equal to 2.560.\nWe can obtain different contrasts by using the comparisons() function. For example:\n\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>   cyl    6 - 4    -6.16       1.54 -4.01  < 0.001 14.0 -9.17  -3.15\n#>   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0 -6.79  -1.03\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>   cyl    6 - 4    -6.16       1.54 -4.01  < 0.001 14.0  -9.17  -3.15\n#>   cyl    8 - 4   -10.07       1.45 -6.93  < 0.001 37.8 -12.91  -7.22\n#>   cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0  -6.79  -1.03\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod, variables = list(cyl = \"reference\"))\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>   cyl    6 - 4    -6.16       1.54 -4.01   <0.001 14.0  -9.17  -3.15\n#>   cyl    8 - 4   -10.07       1.45 -6.93   <0.001 37.8 -12.91  -7.22\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nFor comparison, this code produces the same results using the emmeans package:\n\nlibrary(emmeans)\nemm <- emmeans(mod, specs = \"cyl\")\ncontrast(emm, method = \"revpairwise\")\n#>  contrast    estimate   SE df t.ratio p.value\n#>  cyl6 - cyl4    -6.16 1.54 28  -4.009  0.0012\n#>  cyl8 - cyl4   -10.07 1.45 28  -6.933  <.0001\n#>  cyl8 - cyl6    -3.91 1.47 28  -2.660  0.0331\n#> \n#> Results are averaged over the levels of: am \n#> P value adjustment: tukey method for comparing a family of 3 estimates\n\nemm <- emmeans(mod, specs = \"am\")\ncontrast(emm, method = \"revpairwise\")\n#>  contrast     estimate  SE df t.ratio p.value\n#>  TRUE - FALSE     2.56 1.3 28   1.973  0.0585\n#> \n#> Results are averaged over the levels of: cyl\n\nNote that these commands also work on for other types of models, such as GLMs, on different scales:\n\nmod_logit <- glm(am ~ factor(gear), data = mtcars, family = binomial)\n\navg_comparisons(mod_logit)\n#> \n#>  Term Contrast Estimate Std. Error       z Pr(>|z|)    S 2.5 % 97.5 %\n#>  gear    4 - 3    0.667   1.36e-01     4.9   <0.001 20.0   0.4  0.933\n#>  gear    5 - 3    1.000   1.07e-05 93380.2   <0.001  Inf   1.0  1.000\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod_logit, type = \"link\")\n#> \n#>  Term Contrast Estimate Std. Error       z Pr(>|z|)   S  2.5 % 97.5 %\n#>  gear    4 - 3     21.3       4578 0.00464    0.996 0.0  -8951   8994\n#>  gear    5 - 3     41.1       9156 0.00449    0.996 0.0 -17904  17986\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n3.2.2 Character predictors\nAll functions of the marginaleffects package attempt to treat character predictors as factor predictors. However, using factors instead of characters when modeling is strongly encouraged, because they are much safer and faster. This is because factors hold useful information about the full list of levels, which makes them easier to track and handle internally by marginaleffects. Users are strongly encouraged to convert their character variables to factor before fitting their models and using slopes() functions.\n\n3.2.3 Numeric predictors\nWe can also compute contrasts for differences in numeric variables. For example, we can see what happens to the adjusted predictions when we increment the hp variable by 1 unit (default) or by 5 units:\n\nmod <- lm(mpg ~ hp, data = mtcars)\n\navg_comparisons(mod)\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %\n#>    hp       +1  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod, variables = list(hp = 5))\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>    hp       +5   -0.341     0.0506 -6.74   <0.001 35.9 -0.44 -0.242\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nCompare adjusted predictions for a change in the regressor between two arbitrary values:\n\navg_comparisons(mod, variables = list(hp = c(90, 110)))\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>    hp 110 - 90    -1.36      0.202 -6.74   <0.001 35.9 -1.76 -0.968\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nCompare adjusted predictions when the regressor changes across the interquartile range, across one or two standard deviations about its mean, or from across its full range:\n\navg_comparisons(mod, variables = list(hp = \"iqr\"))\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>    hp  Q3 - Q1     -5.7      0.845 -6.74   <0.001 35.9 -7.35  -4.04\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod, variables = list(hp = \"sd\"))\n#> \n#>  Term                Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>    hp (x + sd/2) - (x - sd/2)    -4.68      0.694 -6.74   <0.001 35.9 -6.04  -3.32\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod, variables = list(hp = \"2sd\"))\n#> \n#>  Term            Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>    hp (x + sd) - (x - sd)    -9.36       1.39 -6.74   <0.001 35.9 -12.1  -6.64\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n#> \n#>  Term  Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>    hp Max - Min    -19.3       2.86 -6.74   <0.001 35.9 -24.9  -13.7\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/comparisons.html#interactions-and-cross-contrasts",
    "href": "articles/comparisons.html#interactions-and-cross-contrasts",
    "title": "\n3  Comparisons\n",
    "section": "\n3.3 Interactions and Cross-Contrasts",
    "text": "3.3 Interactions and Cross-Contrasts\nIn some contexts we are interested in whether the “effect” of a variable changes, as a function of another variable. A very simple strategy to tackle this question is to estimate a model with a multiplicative interaction like this one:\n\nmod <- lm(mpg ~ am * factor(cyl), data = mtcars)\n\nCalling avg_comparisons() with the by argument shows that the estimated comparisons differ based on cyl:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\")\n#> \n#>  Term          Contrast cyl Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>    am mean(1) - mean(0)   6     1.44       2.32 0.623   0.5336 0.9 -3.10   5.98\n#>    am mean(1) - mean(0)   4     5.18       2.05 2.521   0.0117 6.4  1.15   9.20\n#>    am mean(1) - mean(0)   8     0.35       2.32 0.151   0.8799 0.2 -4.19   4.89\n#> \n#> Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nHowever, using the hypothesis argument for pairwise contrasts between the above comparisons reveals that the heterogeneity is not statistically significant:\n\navg_comparisons(mod, variables = \"am\", by = \"cyl\", hypothesis = \"pairwise\")\n#> \n#>   Term Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  6 - 4    -3.73       3.09 -1.206    0.228 2.1 -9.80   2.33\n#>  6 - 8     1.09       3.28  0.333    0.739 0.4 -5.33   7.51\n#>  4 - 8     4.82       3.09  1.559    0.119 3.1 -1.24  10.89\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nIn other contexts, we are interested in a “cross-contrast” or “cross-comparisons”; we would like to know what happens when two (or more) predictors change at the same time. To assess this, we can specify the regressors of interest in the variables argument, and set the cross=TRUE:\n\navg_comparisons(mod, variables = c(\"cyl\", \"am\"), cross = TRUE)\n#> \n#>  C: cyl C: am Estimate Std. Error      z Pr(>|z|)   S  2.5 % 97.5 %\n#>   6 - 4 1 - 0    -2.33       2.48 -0.942  0.34596 1.5  -7.19   2.52\n#>   8 - 4 1 - 0    -7.50       2.77 -2.709  0.00674 7.2 -12.93  -2.07\n#> \n#> Columns: term, contrast_cyl, contrast_am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/comparisons.html#quantities-of-interest",
    "href": "articles/comparisons.html#quantities-of-interest",
    "title": "\n3  Comparisons\n",
    "section": "\n3.4 Quantities of interest",
    "text": "3.4 Quantities of interest\nThis section compares 4 quantities:\n\nUnit-Level Contrasts\nAverage Contrast\nContrast at the Mean\nContrast Between Marginal Means\n\nThe ideas discussed in this section focus on contrasts, but they carry over directly to analogous types of marginal effects.\n\n3.4.1 Unit-level contrasts\nIn models with interactions or non-linear components (e.g., link function), the value of a contrast or marginal effect can depend on the value of all the predictors in the model. As a result, contrasts and marginal effects are fundamentally unit-level quantities. The effect of a 1 unit increase in \\(X\\) can be different for Mary or John. Every row of a dataset has a different contrast and marginal effect.\nThe mtcars dataset has 32 rows, so the comparisons() function produces 32 contrast estimates:\n\nlibrary(marginaleffects)\nmod <- glm(vs ~ factor(gear) + mpg, family = binomial, data = mtcars)\ncmp <- comparisons(mod, variables = \"mpg\")\nnrow(cmp)\n#> [1] 32\n\n\n3.4.2 Average contrasts\nBy default, the slopes() and comparisons() functions compute marginal effects and contrasts for every row of the original dataset. These unit-level estimates can be of great interest, as discussed in another vignette. Nevertheless, one may want to focus on one-number summaries: the avg_*() functions or the by argument compute the “Average Marginal Effect” or “Average Contrast,” by taking the mean of all the unit-level estimates.\n\navg_comparisons(mod, variables = \"mpg\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>   mpg       +1   0.0608     0.0128 4.74   <0.001 18.8 0.0356  0.086\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\ncomparisons(mod, variables = \"mpg\", by = TRUE)\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>   mpg       +1   0.0608     0.0128 4.74   <0.001 18.8 0.0356  0.086\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nwhich are equivalent to:\n\nmean(cmp$estimate)\n#> [1] 0.06080995\n\nWe could also show the full distribution of contrasts across our dataset with a histogram:\n\n##| fig.asp = .4\nlibrary(ggplot2)\n\ncmp <- comparisons(mod, variables = \"gear\")\n\nggplot(cmp, aes(estimate)) +\n    geom_histogram(bins = 30) +\n    facet_wrap(~contrast, scale = \"free_x\") +\n    labs(x = \"Distribution of unit-level contrasts\")\n\n\n\n\nThis graph display the effect of a change of 1 unit in the mpg variable, for each individual in the observed data.\n\n3.4.3 Contrasts at the mean\nAn alternative which used to be very common but has now fallen into a bit of disfavor is to compute “Contrasts at the mean.” The idea is to create a “synthetic” or “hypothetical” individual (row of the dataset) whose characteristics are completely average. Then, we compute and report the contrast for this specific hypothetical individual.\nThis can be achieved by setting newdata=\"mean\" or to newdata=datagrid(), both of which fix variables to their means or modes:\n\ncomparisons(mod, variables = \"mpg\", newdata = \"mean\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)   S  2.5 % 97.5 %\n#>   mpg       +1    0.166     0.0627 2.65  0.00794 7.0 0.0436  0.289\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, vs, gear, mpg\n\nContrasts at the mean can differ substantially from average contrasts.\nThe advantage of this approach is that it is very cheap and fast computationally. The disadvantage is that the interpretation is somewhat ambiguous. Often times, there simply does not exist an individual who is perfectly average across all dimensions of the dataset. It is also not clear why the analyst should be particularly interested in the contrast for this one, synthetic, perfectly average individual.\n\n3.4.4 Contrasts between marginal means\nYet another type of contrast is the “Contrast between marginal means.” This type of contrast is closely related to the “Contrast at the mean”, with a few wrinkles. It is the default approach used by the emmeans package for R.\nRoughly speaking, the procedure is as follows:\n\nCreate a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.\nMake adjusted predictions in each cell of the prediction grid.\nTake the average of those predictions (marginal means) for each combination of btype (focal variable) and resp (group by variable).\nCompute pairwise differences (contrasts) in marginal means across different levels of the focal variable btype.\n\nThe contrast obtained through this approach has two critical characteristics:\n\nIt is the contrast for a synthetic individual with perfectly average qualities on every (numeric) predictor.\nIt is a weighted average of unit-level contrasts, where weights assume a perfectly balanced dataset across every categorical predictor.\n\nWith respect to (a), the analyst should ask themselves: Is my quantity of interest the contrast for a perfectly average hypothetical individual? With respect to (b), the analyst should ask themselves: Is my quantity of interest the contrast in a model estimated using (potentially) unbalanced data, but interpreted as if the data were perfectly balanced?\nFor example, imagine that one of the control variables in your model is a variable measuring educational attainment in 4 categories: No high school, High school, Some college, Completed college. The contrast between marginal is a weighted average of contrasts estimated in the 4 cells, and each of those contrasts will be weighted equally in the overall estimate. If the population of interest is highly unbalanced in the educational categories, then the estimate computed in this way will not be most useful.\nIf the contrasts between marginal means is really the quantity of interest, it is easy to use the comparisons() to estimate contrasts between marginal means. The newdata determines the values of the predictors at which we want to compute contrasts. We can set newdata=\"marginalmeans\" to emulate the emmeans behavior. For example, here we compute contrasts in a model with an interaction:\n\ndat <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\nmod <- lm(bill_length_mm ~ species * sex + island + body_mass_g, data = dat)\n\navg_comparisons(\n    mod,\n    newdata = \"marginalmeans\",\n    variables = c(\"species\", \"island\"))\n#> \n#>     Term           Contrast Estimate Std. Error      z Pr(>|z|)     S  2.5 % 97.5 %\n#>  species Chinstrap - Adelie  10.2693      0.407 25.252   <0.001 465.0  9.472 11.066\n#>  species Gentoo - Adelie      5.8957      0.677  8.705   <0.001  58.1  4.568  7.223\n#>  island  Dream - Biscoe      -0.4557      0.453 -1.005    0.315   1.7 -1.344  0.433\n#>  island  Torgersen - Biscoe   0.0851      0.470  0.181    0.856   0.2 -0.836  1.006\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWhich is equivalent to this in emmeans:\n\nemm <- emmeans(\n    mod,\n    specs = c(\"species\", \"island\"))\ncontrast(emm, method = \"trt.vs.ctrl1\")\n#>  contrast                            estimate    SE  df t.ratio p.value\n#>  Chinstrap Biscoe - Adelie Biscoe     10.2693 0.407 324  25.252  <.0001\n#>  Gentoo Biscoe - Adelie Biscoe         5.8957 0.677 324   8.705  <.0001\n#>  Adelie Dream - Adelie Biscoe         -0.4557 0.453 324  -1.005  0.8274\n#>  Chinstrap Dream - Adelie Biscoe       9.8136 0.434 324  22.630  <.0001\n#>  Gentoo Dream - Adelie Biscoe          5.4400 0.941 324   5.779  <.0001\n#>  Adelie Torgersen - Adelie Biscoe      0.0851 0.470 324   0.181  0.9994\n#>  Chinstrap Torgersen - Adelie Biscoe  10.3544 0.622 324  16.656  <.0001\n#>  Gentoo Torgersen - Adelie Biscoe      5.9808 0.954 324   6.268  <.0001\n#> \n#> Results are averaged over the levels of: sex \n#> P value adjustment: dunnettx method for 8 tests\n\nThe emmeans section of the Alternative Software vignette shows further examples.\nThe excellent vignette of the emmeans package discuss the same issues in a slightly different (and more positive) way:\n\nThe point is that the marginal means of cell.means give equal weight to each cell. In many situations (especially with experimental data), that is a much fairer way to compute marginal means, in that they are not biased by imbalances in the data. We are, in a sense, estimating what the marginal means would be, had the experiment been balanced. Estimated marginal means (EMMs) serve that need.\n\n\nAll this said, there are certainly situations where equal weighting is not appropriate. Suppose, for example, we have data on sales of a product given different packaging and features. The data could be unbalanced because customers are more attracted to some combinations than others. If our goal is to understand scientifically what packaging and features are inherently more profitable, then equally weighted EMMs may be appropriate; but if our goal is to predict or maximize profit, the ordinary marginal means provide better estimates of what we can expect in the marketplace."
  },
  {
    "objectID": "articles/comparisons.html#conditional-contrasts",
    "href": "articles/comparisons.html#conditional-contrasts",
    "title": "\n3  Comparisons\n",
    "section": "\n3.5 Conditional contrasts",
    "text": "3.5 Conditional contrasts\nConsider a model with an interaction term. What happens to the dependent variable when the hp variable increases by 10 units?\n\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ hp * wt, data = mtcars)\n\nplot_comparisons(\n    mod,\n    variables = list(hp = 10),\n    condition = \"wt\")"
  },
  {
    "objectID": "articles/comparisons.html#transformations",
    "href": "articles/comparisons.html#transformations",
    "title": "\n3  Comparisons\n",
    "section": "\n3.6 Transformations",
    "text": "3.6 Transformations\nSo far we have focused on simple differences between adjusted predictions. Now, we show how to use ratios, back transformations, and arbitrary functions to estimate a slew of quantities of interest. Powerful transformations and custom contrasts are made possible by using three arguments which act at different stages of the computation process:\n\ncomparison\ntransform\n\nConsider the case of a model with a single predictor \\(x\\). To compute average contrasts, we proceed as follows:\n\nCompute adjusted predictions for each row of the dataset for the observed values \\(x\\): \\(\\hat{y}_x\\)\n\nCompute adjusted predictions for each row of the dataset for the observed values \\(x + 1\\): \\(\\hat{y}_{x+1}\\)\n\n\ncomparison: Compute unit-level contrasts by taking the difference between (or some other function of) adjusted predictions: \\(\\hat{y}_{x+1} - \\hat{y}_x\\)\n\nCompute the average contrast by taking the mean of unit-level contrasts: \\(1/N \\sum_{i=1}^N \\hat{y}_{x+1} - \\hat{y}_x\\)\n\n\ntransform: Transform the average contrast or return them as-is.\n\nThe comparison argument of the comparisons() function determines how adjusted predictions are combined to create a contrast. By default, we take a simple difference between predictions with hi value of \\(x\\), and predictions with a lo value of \\(x\\): function(hi, lo) hi-lo.\nThe transform argument of the comparisons() function applies a custom transformation to the unit-level contrasts.\nThe transform argument applies a custom transformation to the final quantity, as would be returned if we evaluated the same call without transform."
  },
  {
    "objectID": "articles/comparisons.html#differences",
    "href": "articles/comparisons.html#differences",
    "title": "\n3  Comparisons\n",
    "section": "\n3.7 Differences",
    "text": "3.7 Differences\nThe default contrast calculate by the comparisons() function is a (untransformed) difference between two adjusted predictions. For instance, to estimate the effect of a change of 1 unit, we do:\n\nlibrary(marginaleffects)\n\nmod <- glm(vs ~ mpg, data = mtcars, family = binomial)\n\n## construct data\n\nmtcars_minus <- mtcars_plus <- mtcars\nmtcars_minus$mpg <- mtcars_minus$mpg - 0.5\nmtcars_plus$mpg <- mtcars_plus$mpg + 0.5\n\n## adjusted predictions\nyhat_minus <- predict(mod, newdata = mtcars_minus, type = \"response\")\nyhat_plus <- predict(mod, newdata = mtcars_plus, type = \"response\")\n\n## unit-level contrasts\ncon <- yhat_plus - yhat_minus\n\n## average contrasts\nmean(con)\n#> [1] 0.05540227\n\nWe can use the avg_comparisons() function , or the by argument to obtain the same results:\n\navg_comparisons(mod)\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>   mpg       +1   0.0554    0.00834 6.64   <0.001 34.9 0.0391 0.0717\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\ncomparisons(mod, by = TRUE)\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>   mpg       +1   0.0554    0.00834 6.64   <0.001 34.9 0.0391 0.0717\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/comparisons.html#difference-in-differences-in-differences",
    "href": "articles/comparisons.html#difference-in-differences-in-differences",
    "title": "\n3  Comparisons\n",
    "section": "\n3.8 Difference-in-Differences(-in-Differences)",
    "text": "3.8 Difference-in-Differences(-in-Differences)\nGoing back to our Titanic example:\n\ndat <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat <- read.csv(dat)\ntitanic <- glm(Survived ~ PClass * SexCode * Age, data = dat, family = binomial)\n\nIn this case, a contrast is a difference between predicted probabilities. We can compute that contrast for different types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#> \n#>     Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %  Age PClass\n#>  SexCode    1 - 0    0.483     0.0631 7.65   <0.001 45.5 0.359  0.606 30.4    1st\n#>  SexCode    1 - 0    0.335     0.0634 5.29   <0.001 22.9 0.211  0.459 30.4    3rd\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, Survived, SexCode, Age, PClass\n\nOne we can notice above, is that the gap in predicted probabilities of survival between men and women is larger in 1st class than in 3rd class. Being a woman matters more for your chances of survival if you travel in first class. Is the difference between those contrasts (diff-in-diff) statistically significant?\nTo answer this question, we can compute a difference-in-difference using the hypothesis argument (see the Hypothesis vignette for details). For example, using b1 and b2 to refer to the contrasts in the first and second rows of the output above, we can test if the difference between the two quantities is different from 0:\n\ncomparisons(\n  titanic,\n  hypothesis = \"b1 - b2 = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\")))\n#> \n#>     Term Estimate Std. Error    z Pr(>|z|)   S   2.5 % 97.5 %\n#>  b1-b2=0    0.148     0.0894 1.65   0.0987 3.3 -0.0276  0.323\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNow, let’s say we consider more types of individuals:\n\ncomparisons(\n  titanic,\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#> \n#>     Term Contrast Estimate Std. Error      z Pr(>|z|)     S   2.5 % 97.5 % PClass   Age\n#>  SexCode    1 - 0   0.1081      0.122  0.883   0.3774   1.4 -0.1319  0.348    1st  0.17\n#>  SexCode    1 - 0   0.8795      0.057 15.437   <0.001 176.2  0.7679  0.991    1st 71.00\n#>  SexCode    1 - 0   0.0805      0.157  0.513   0.6081   0.7 -0.2272  0.388    3rd  0.17\n#>  SexCode    1 - 0   0.4265      0.203  2.101   0.0356   4.8  0.0287  0.824    3rd 71.00\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, Survived, SexCode, PClass, Age\n\nWith these results, we could compute a triple difference:\n\ncomparisons(\n  titanic,\n  hypothesis = \"(b1 - b3) - (b2 - b4) = 0\",\n  variables = \"SexCode\",\n  newdata = datagrid(PClass = c(\"1st\", \"3rd\"), Age = range))\n#> \n#>               Term Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  (b1-b3)-(b2-b4)=0   -0.425      0.359 -1.19    0.236 2.1 -1.13  0.278\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/comparisons.html#ratios",
    "href": "articles/comparisons.html#ratios",
    "title": "\n3  Comparisons\n",
    "section": "\n3.9 Ratios",
    "text": "3.9 Ratios\nInstead of taking simple differences between adjusted predictions, it can sometimes be useful to compute ratios or other functions of predictions. For example, the adjrr function the Stata software package can compute “adjusted risk ratios”, which are ratios of adjusted predictions. To do this in R, we use the comparison argument:\n\navg_comparisons(mod, comparison = \"ratio\")\n#> \n#>  Term Contrast Estimate Std. Error   z Pr(>|z|)    S 2.5 % 97.5 %\n#>   mpg       +1     1.29      0.133 9.7   <0.001 71.4  1.03   1.55\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThis result is the average adjusted risk ratio, that is, the adjusted predictions when the mpg are incremented by 1, divided by the adjusted predictions when mpg is at its original value.\nThe comparison accepts different values for common types of contrasts: ‘difference’, ‘ratio’, ‘lnratio’, ‘ratioavg’, ‘lnratioavg’, ‘lnoravg’, ‘differenceavg’. These strings are shortcuts for functions that accept two vectors of adjusted predictions and returns a single vector of contrasts. For example, these two commands yield identical results:\n\navg_comparisons(mod, comparison = \"ratio\")\n#> \n#>  Term Contrast Estimate Std. Error   z Pr(>|z|)    S 2.5 % 97.5 %\n#>   mpg       +1     1.29      0.133 9.7   <0.001 71.4  1.03   1.55\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(mod, comparison = function(hi, lo) hi / lo)\n#> \n#>  Term Contrast Estimate Std. Error   z Pr(>|z|)    S 2.5 % 97.5 %\n#>   mpg       +1     1.29      0.133 9.7   <0.001 71.4  1.03   1.55\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThis mechanism is powerful, because it lets users create fully customized contrasts. Here is a non-sensical example:\n\navg_comparisons(mod, comparison = function(hi, lo) sqrt(hi) / log(lo + 10))\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>   mpg       +1    0.264     0.0261 10.1   <0.001 77.3 0.213  0.315\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThe same arguments work in the plotting function plot_comparisons() as well, which allows us to plot various custom contrasts. Here is a comparison of Adjusted Risk Ratio and Adjusted Risk Difference in a model of the probability of survival aboard the Titanic:\n\nlibrary(ggplot2)\nlibrary(patchwork)\ntitanic <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ntitanic <- read.csv(titanic)\nmod_titanic <- glm(\n    Survived ~ Sex * PClass + Age + I(Age^2),\n    family = binomial,\n    data = titanic)\n\navg_comparisons(mod_titanic)\n#> \n#>    Term      Contrast Estimate Std. Error      z Pr(>|z|)     S    2.5 %   97.5 %\n#>  Sex    male - female  -0.4847    0.03004 -16.14   <0.001 192.2 -0.54355 -0.42580\n#>  PClass 2nd - 1st      -0.2058    0.03954  -5.20   <0.001  22.3 -0.28328 -0.12828\n#>  PClass 3rd - 1st      -0.4043    0.03958 -10.21   <0.001  78.9 -0.48187 -0.32670\n#>  Age    +1             -0.0065    0.00108  -6.03   <0.001  29.2 -0.00862 -0.00439\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\np1 <- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survival | Age + 1) / P(Survival | Age)\")\n\np2 <- plot_comparisons(\n    mod_titanic,\n    variables = \"Age\",\n    condition = \"Age\") +\n    ylab(\"Adjusted Risk Difference\\nP(Survival | Age + 1) - P(Survival | Age)\")\n\np1 + p2\n\n\n\n\nBy default, the standard errors around contrasts are computed using the delta method on the scale determined by the type argument (e.g., “link” or “response”). Some analysts may prefer to proceed differently. For example, in Stata, the adjrr computes adjusted risk ratios (ARR) in two steps:\n\nCompute the natural log of the ratio between the mean of adjusted predictions with \\(x+1\\) and the mean of adjusted predictions with \\(x\\).\nExponentiate the estimate and confidence interval bounds.\n\nStep 1 is easy to achieve with the comparison argument described above. Step 2 can be achieved with the transform argument:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#> \n#>  Term Contrast Estimate Pr(>|z|)   S 2.5 % 97.5 %\n#>   mpg       +1     1.27  0.00936 6.7  1.06   1.53\n#> \n#> Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high\n\nNote that we can use the lnratioavg shortcut instead of defining the function ourselves.\nThe order of operations in previous command was:\n\nCompute the custom unit-level log ratios\nExponentiate them\nTake the average using the avg_comparisons()\n\n\nThere is a very subtle difference between the procedure above and this code:\n\navg_comparisons(\n    mod,\n    comparison = function(hi, lo) log(hi / lo),\n    transform = exp)\n#> \n#>  Term Contrast Estimate Pr(>|z|)   S 2.5 % 97.5 %\n#>   mpg       +1     1.27  0.00936 6.7  1.06   1.53\n#> \n#> Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high\n\nSince the exp function is now passed to the transform argument of the comparisons() function, the exponentiation is now done only after unit-level contrasts have been averaged. This is what Stata appears to do under the hood, and the results are slightly different.\n\ncomparisons(\n    mod,\n    comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n    transform = exp)\n#> \n#>  Term Contrast Estimate Pr(>|z|)    S 2.5 % 97.5 %\n#>   mpg       +1     1.14   <0.001 31.9  1.09   1.18\n#> \n#> Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nNote that equivalent results can be obtained using shortcut strings in the comparison argument: “ratio”, “lnratio”, “lnratioavg”.\n\ncomparisons(\n    mod,\n    comparison = \"lnratioavg\",\n    transform = exp)\n#> \n#>  Term Contrast Estimate Pr(>|z|)    S 2.5 % 97.5 %\n#>   mpg mean(+1)     1.14   <0.001 31.9  1.09   1.18\n#> \n#> Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nAll the same arguments apply to the plotting functions of the marginaleffects package as well. For example we can plot the Adjusted Risk Ratio in a model with a quadratic term:\n\nlibrary(ggplot2)\ndat_titanic <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\")\nmod2 <- glm(Survived  ~ Age, data = dat_titanic, family = binomial)\nplot_comparisons(\n    mod2,\n    variables = list(\"Age\" = 10),\n    condition = \"Age\",\n    comparison = \"ratio\") +\n    ylab(\"Adjusted Risk Ratio\\nP(Survived = 1 | Age + 10) / P(Survived = 1 | Age)\")"
  },
  {
    "objectID": "articles/comparisons.html#forward-backward-centered-and-custom-differences",
    "href": "articles/comparisons.html#forward-backward-centered-and-custom-differences",
    "title": "\n3  Comparisons\n",
    "section": "\n3.10 Forward, Backward, Centered, and Custom Differences",
    "text": "3.10 Forward, Backward, Centered, and Custom Differences\nBy default, the comparisons() function computes a “centered” difference. For example, if we ask comparisons() to estimate the effect of a 10-unit change in predictor x on outcome y, comparisons() will compare the predicted values with x-5 and x+5.\n\ndat <- mtcars\ndat$new_hp <- 49 * (mtcars$hp - min(mtcars$hp)) / (max(mtcars$hp) - min(mtcars$hp)) + 1\nmod <- lm(mpg ~ log(new_hp), data = dat)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = 10))\n#> \n#>    Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  new_hp      +10    -4.06      0.464 -8.74   <0.001 58.6 -4.97  -3.15\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nSince version 0.7.2 of marginaleffects, we can supply arbitrary functions to create custom differences. These functions must accept a vector of values for the predictor of interest, and return a data frame with the same number of rows as the length, and two columns with the values to compare. For example, we can do:\n\nforward_diff <- \\(x) data.frame(x, x + 10)\nbackward_diff <- \\(x) data.frame(x - 10, x)\ncenter_diff <- \\(x) data.frame(x - 5, x + 5)\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = forward_diff))\n#> \n#>    Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  new_hp   custom     -3.8      0.435 -8.74   <0.001 58.6 -4.65  -2.95\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = backward_diff))\n#> \n#>    Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  new_hp   custom    -6.51      0.744 -8.74   <0.001 58.6 -7.97  -5.05\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_comparisons(\n  mod,\n  variables = list(new_hp = center_diff))\n#> \n#>    Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  new_hp   custom    -4.06      0.464 -8.74   <0.001 58.6 -4.97  -3.15\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNotice that the last “centered” difference gives the same results as the default comparisons() call."
  },
  {
    "objectID": "articles/comparisons.html#lognormal-hurdle-model",
    "href": "articles/comparisons.html#lognormal-hurdle-model",
    "title": "\n3  Comparisons\n",
    "section": "\n3.11 Lognormal hurdle model",
    "text": "3.11 Lognormal hurdle model\nWith hurdle models, we can fit two separate models simultaneously:\n\nA model that predicts if the outcome is zero or not zero\nIf the outcome is not zero, a model that predicts what the value of the outcome is\n\nWe can calculate predictions and marginal effects for each of these hurdle model processes, but doing so requires some variable transformation since the stages of these models use different link functions.\nThe hurdle_lognormal() family in brms uses logistic regression (with a logit link) for the hurdle part of the model and lognormal regression (where the outcome is logged before getting used in the model) for the non-hurdled part. Let’s look at an example of predicting GDP per capita (which is distributed exponentially) using life expectancy. We’ll add some artificial zeros so that we can work with a hurdle stage of the model.\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\nlibrary(brms)\nlibrary(marginaleffects)\nlibrary(gapminder)\n\n## Build some 0s into the GDP column\nset.seed(1234)\ngapminder <- gapminder::gapminder |> \n  filter(continent != \"Oceania\") |> \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp < 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap0 = ifelse(will_be_zero, 0, gdpPercap)) |> \n  select(-prob_zero, -will_be_zero)\n\nmod <- brm(\n  bf(gdpPercap0 ~ lifeExp,\n     hu ~ lifeExp),\n  data = gapminder,\n  family = hurdle_lognormal(),\n  chains = 4, cores = 4, seed = 1234)\n\nWe have two different sets of coefficients here for the two different processes. The hurdle part (hu) uses a logit link, and the non-hurdle part (mu) uses an identity link. However, that’s a slight misnomer—a true identity link would show the coefficients on a non-logged dollar value scale. Because we’re using a lognormal family, GDP per capita is pre-logged, so the “original” identity scale is actually logged dollars.\n\nsummary(mod)\n\n\n#>  Family: hurdle_lognormal \n#>   Links: mu = identity; sigma = identity; hu = logit \n#> Formula: gdpPercap0 ~ lifeExp \n#>          hu ~ lifeExp\n#>    Data: gapminder (Number of observations: 1680) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Population-Level Effects: \n#>              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept        3.47      0.09     3.29     3.65 1.00     4757     3378\n#> hu_Intercept     3.16      0.40     2.37     3.96 1.00     2773     2679\n#> lifeExp          0.08      0.00     0.08     0.08 1.00     5112     3202\n#> hu_lifeExp      -0.10      0.01    -0.12    -0.08 1.00     2385     2652\n#> ...\n\nWe can get predictions for the hu part of the model on the link (logit) scale:\n\npredictions(mod, dpar = \"hu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#> \n#>  Estimate 2.5 % 97.5 % lifeExp\n#>    -0.817 -1.03 -0.604      40\n#>    -2.805 -3.06 -2.555      60\n#>    -4.790 -5.34 -4.275      80\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp\n\n…or on the response (percentage point) scale:\n\npredictions(mod, dpar = \"hu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#> \n#>  Estimate   2.5 % 97.5 % lifeExp\n#>   0.30630 0.26231 0.3534      40\n#>   0.05703 0.04466 0.0721      60\n#>   0.00824 0.00478 0.0137      80\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp\n\nWe can also get slopes for the hu part of the model on the link (logit) or response (percentage point) scales:\n\nslopes(mod, dpar = \"hu\", type = \"link\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#> \n#>     Term Estimate  2.5 %  97.5 % lifeExp\n#>  lifeExp  -0.0993 -0.116 -0.0837      40\n#>  lifeExp  -0.0993 -0.116 -0.0837      60\n#>  lifeExp  -0.0993 -0.116 -0.0837      80\n#> \n#> Columns: rowid, term, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, gdpPercap0, lifeExp\n\nslopes(mod, dpar = \"hu\", type = \"response\",\n                newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#> \n#>     Term  Estimate    2.5 %    97.5 % lifeExp\n#>  lifeExp -0.021078 -0.02591 -0.016588      40\n#>  lifeExp -0.005321 -0.00615 -0.004561      60\n#>  lifeExp -0.000812 -0.00115 -0.000543      80\n#> \n#> Columns: rowid, term, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, gdpPercap0, lifeExp\n\nWorking with the mu part of the model is trickier. Switching between type = \"link\" and type = \"response\" doesn’t change anything, since the outcome is pre-logged:\n\npredictions(mod, dpar = \"mu\", type = \"link\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#> \n#>  Estimate 2.5 % 97.5 % lifeExp\n#>      6.61  6.54   6.69      40\n#>      8.18  8.15   8.22      60\n#>      9.75  9.69   9.82      80\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp\npredictions(mod, dpar = \"mu\", type = \"response\",\n            newdata = datagrid(lifeExp = seq(40, 80, 20)))\n#> \n#>  Estimate 2.5 % 97.5 % lifeExp\n#>      6.61  6.54   6.69      40\n#>      8.18  8.15   8.22      60\n#>      9.75  9.69   9.82      80\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp\n\nFor predictions, we need to exponentiate the results to scale them back up to dollar amounts. We can do this by post-processing the results (e.g. with dplyr::mutate(predicted = exp(predicted))), or we can use the transform argument in predictions() to pass the results to exp() after getting calculated:\n\npredictions(mod, dpar = \"mu\", \n            newdata = datagrid(lifeExp = seq(40, 80, 20)),\n            transform = exp)\n#> \n#>  Estimate 2.5 % 97.5 % lifeExp\n#>       744   694    801      40\n#>      3581  3449   3718      60\n#>     17215 16110  18410      80\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap0, lifeExp\n\nWe can pass transform = exp to plot_predictions() too:\n\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"link\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       title = \"Hurdle part (hu)\",\n       subtitle = \"Logit-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"hu\",\n  type = \"response\",\n  condition = \"lifeExp\") +\n  labs(y = \"hu\",\n       subtitle = \"Percentage point-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       title = \"Non-hurdle part (mu)\",\n       subtitle = \"Log-scale predictions\") +\nplot_predictions(\n  mod,\n  dpar = \"mu\",\n  transform = exp,\n  condition = \"lifeExp\") +\n  labs(y = \"mu\",\n       subtitle = \"Dollar-scale predictions\")\n\n\n\n\nFor marginal effects, we need to transform the predictions before calculating the instantaneous slopes. We also can’t use the slopes() function directly—we need to use comparisons() and compute the numerical derivative ourselves (i.e. predict gdpPercap at lifeExp of 40 and 40.001 and calculate the slope between those predictions). We can use the comparison argument to pass the pair of predicted values to exp() before calculating the slopes:\n\n## step size of the numerical derivative\neps <- 0.001\n\ncomparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  # rescale the elements of the slope\n  # (exp(40.001) - exp(40)) / exp(0.001)\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps\n)\n#> \n#>     Term Contrast Estimate  2.5 % 97.5 %\n#>  lifeExp   +0.001     58.4   55.8     61\n#>  lifeExp   +0.001    280.9  266.6    296\n#>  lifeExp   +0.001   1349.4 1222.6   1490\n#> \n#> Columns: rowid, term, contrast, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, gdpPercap0, lifeExp\n\nWe can visually confirm that these are the instantaneous slopes at each of these levels of life expectancy:\n\npredictions_data <- predictions(\n  mod,\n  newdata = datagrid(lifeExp = seq(30, 80, 1)),\n  dpar = \"mu\",\n  transform = exp) |>\n  select(lifeExp, prediction = estimate)\n\nslopes_data <- comparisons(\n  mod,\n  dpar = \"mu\",\n  variables = list(lifeExp = eps),\n  newdata = datagrid(lifeExp = seq(40, 80, 20)),\n  comparison = function(hi, lo) ((exp(hi) - exp(lo)) / exp(eps)) / eps) |>\n  select(lifeExp, estimate) |>\n  left_join(predictions_data, by = \"lifeExp\") |>\n  # Point-slope formula: (y - y1) = m(x - x1)\n  mutate(intercept = estimate * (-lifeExp) + prediction)\n\nggplot(predictions_data, aes(x = lifeExp, y = prediction)) +\n  geom_line(size = 1) + \n  geom_abline(data = slopes_data, aes(slope = estimate, intercept = intercept), \n              size = 0.5, color = \"red\") +\n  geom_point(data = slopes_data) +\n  geom_label(data = slopes_data, aes(label = paste0(\"Slope: \", round(estimate, 1))),\n             nudge_x = -1, hjust = 1) +\n  theme_minimal()\n\n\n\n\nWe now have this in the experiments section"
  },
  {
    "objectID": "articles/comparisons.html#visual-examples",
    "href": "articles/comparisons.html#visual-examples",
    "title": "\n3  Comparisons\n",
    "section": "\n3.12 Visual examples",
    "text": "3.12 Visual examples\n\nokabeito <- c('#E69F00', '#56B4E9', '#009E73', '#F0E442', '#0072B2', '#D55E00', '#CC79A7', '#999999', '#000000')\noptions(ggplot2.discrete.fill = okabeito)\nlibrary(ggplot2)\ntheme_set(theme_minimal())\n\nlibrary(marginaleffects)\nlibrary(ggplot2)\n\nset.seed(1024)\nn <- 200\nd <- data.frame(\n  y = rnorm(n),\n  cond = as.factor(sample(0:1, n, TRUE)),\n  episode = as.factor(sample(0:4, n, TRUE)))\n\nmodel1 <- lm(y ~ cond * episode, data = d)\n\np <- predictions(model1, newdata = datagrid(cond = 0:1, episode = 1:3))\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point()\n\n\n\n\n## do episodes 1 and 2 differ when `cond=0`\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 1, y = p$estimate[1], yend = p$estimate[2]), color = \"black\") +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0))    # grid\n#> \n#>     Term Contrast Estimate Std. Error     z Pr(>|z|)   S  2.5 % 97.5 % cond\n#>  episode    2 - 1    0.241      0.396 0.609    0.542 0.9 -0.535   1.02    0\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, y, episode, cond\n\n## do cond=0 and cond=1 differ when episode = 1\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  geom_segment(aes(x = 1, xend = 2, y = p$estimate[1], yend = p$estimate[4]), color = okabeito[1]) +\n  ggtitle(\"What is the vertical distance between the linked points?\")\n\n\n\n\ncomparisons(model1,\n  variables = \"cond\",              # comparison of interest\n  newdata = datagrid(episode = 1)) # grid\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)   S  2.5 % 97.5 % episode\n#>  cond    1 - 0    0.546      0.347 1.57    0.115 3.1 -0.134   1.23       1\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, y, cond, episode\n\n## Is the difference between episode 1 and 2 larger in cond=0 or cond=1? \n## try this without the `hypothesis` argument to see what we are comparing more clearly\nggplot(p, aes(x = cond, y = estimate, shape = episode, color = episode)) +\n  geom_point() +\n  annotate(\"rect\", xmin = .9, xmax = 1.1, ymin = p$estimate[1], ymax = p$estimate[2], alpha = .2, fill = \"green\") +\n  annotate(\"rect\", xmin = 1.9, xmax = 2.1, ymin = p$estimate[4], ymax = p$estimate[5], alpha = .2, fill = \"orange\")  +\n  ggtitle(\"Is the green box taller than the orange box?\")\n\n\n\n\ncomparisons(model1,\n  variables = list(episode = 1:2), # comparison of interest\n  newdata = datagrid(cond = 0:1),  # grid\n  hypothesis = \"b1 = b2\")          # hypothesis\n#> \n#>   Term Estimate Std. Error     z Pr(>|z|)   S  2.5 % 97.5 %\n#>  b1=b2    0.413      0.508 0.812    0.417 1.3 -0.583   1.41\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/slopes.html#definition",
    "href": "articles/slopes.html#definition",
    "title": "\n4  Slopes\n",
    "section": "\n4.1 Definition",
    "text": "4.1 Definition\nSlopes are defined as:\n\nPartial derivatives of the regression equation with respect to a regressor of interest. a.k.a. Marginal effects, trends.\n\nThis vignette follows the econometrics tradition by referring to “slopes” and “marginal effects” interchangeably. In this context, the word “marginal” refers to the idea of a “small change,” in the calculus sense.\nA marginal effect measures the association between a change in a regressor \\(x\\), and a change in the response \\(y\\). Put differently, differently, the marginal effect is the slope of the prediction function, measured at a specific value of the regressor \\(x\\).\nMarginal effects are extremely useful, because they are intuitive and easy to interpret. They are often the main quantity of interest in an empirical analysis.\nIn scientific practice, the “Marginal Effect” falls in the same toolbox as the “Contrast.” Both try to answer a counterfactual question: What would happen to \\(y\\) if \\(x\\) were different? They allow us to model the “effect” of a change/difference in the regressor \\(x\\) on the response \\(y\\).1\nTo illustrate the concept, consider this quadratic function:\n\\[y = -x^2\\]\nFrom the definition above, we know that the marginal effect is the partial derivative of \\(y\\) with respect to \\(x\\):\n\\[\\frac{\\partial y}{\\partial x} = -2x\\]\nTo get intuition about how to interpret this quantity, consider the response of \\(y\\) to \\(x\\). It looks like this:\n\n\n\n\n\nWhen \\(x\\) increases, \\(y\\) starts to increase. But then, as \\(x\\) increases further, \\(y\\) creeps back down in negative territory.\nA marginal effect is the slope of this response function at a certain value of \\(x\\). The next plot adds three tangent lines, highlighting the slopes of the response function for three values of \\(x\\). The slopes of these tangents tell us three things:\n\nWhen \\(x<0\\), the slope is positive: an increase in \\(x\\) is associated with an increase in \\(y\\): The marginal effect is positive.\nWhen \\(x=0\\), the slope is null: a (small) change in \\(x\\) is associated with no change in \\(y\\). The marginal effect is null.\nWhen \\(x>0\\), the slope is negative: an increase in \\(x\\) is associated with a decrease in \\(y\\). The marginal effect is negative.\n\n\n\n\n\n\nBelow, we show how to reach the same conclusions in an estimation context, with simulated data and the slopes() function."
  },
  {
    "objectID": "articles/slopes.html#slopes-function",
    "href": "articles/slopes.html#slopes-function",
    "title": "\n4  Slopes\n",
    "section": "\n4.2 slopes() function",
    "text": "4.2 slopes() function\nThe marginal effect is a unit-level measure of association between changes in a regressor and changes in the response. Except in the simplest linear models, the value of the marginal effect will be different from individual to individual, because it will depend on the values of the other covariates for each individual.\nThe slopes() function thus produces distinct estimates of the marginal effect for each row of the data used to fit the model. The output of marginaleffects is a simple data.frame, which can be inspected with all the usual R commands.\nTo show this, we load the library, download the Palmer Penguins, and estimate a GLM model:\n\nlibrary(marginaleffects)\n\ndat <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\ndat$large_penguin <- ifelse(dat$body_mass_g > median(dat$body_mass_g, na.rm = TRUE), 1, 0)\n\nmod <- glm(large_penguin ~ bill_length_mm + flipper_length_mm + species,\n           data = dat, family = binomial)\n\n\nmfx <- slopes(mod)\nhead(mfx)\n#> \n#>            Term Contrast Estimate Std. Error    z Pr(>|z|)    S   2.5 % 97.5 %\n#>  bill_length_mm    dY/dX   0.0176    0.00830 2.12  0.03359  4.9 0.00137 0.0339\n#>  bill_length_mm    dY/dX   0.0359    0.01229 2.92  0.00354  8.1 0.01176 0.0600\n#>  bill_length_mm    dY/dX   0.0844    0.02108 4.01  < 0.001 14.0 0.04312 0.1258\n#>  bill_length_mm    dY/dX   0.0347    0.00642 5.41  < 0.001 23.9 0.02214 0.0473\n#>  bill_length_mm    dY/dX   0.0509    0.01350 3.77  < 0.001 12.6 0.02444 0.0773\n#>  bill_length_mm    dY/dX   0.0165    0.00770 2.14  0.03202  5.0 0.00142 0.0316\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, large_penguin, bill_length_mm, flipper_length_mm, species"
  },
  {
    "objectID": "articles/slopes.html#the-marginal-effects-zoo",
    "href": "articles/slopes.html#the-marginal-effects-zoo",
    "title": "\n4  Slopes\n",
    "section": "\n4.3 The Marginal Effects Zoo",
    "text": "4.3 The Marginal Effects Zoo\nA dataset with one marginal effect estimate per unit of observation is a bit unwieldy and difficult to interpret. There are ways to make this information easier to digest, by computing various quantities of interest. In a characteristically excellent blog post, Professor Andrew Heiss introduces many such quantities:\n\nAverage Marginal Effects\nGroup-Average Marginal Effects\nMarginal Effects at User-Specified Values (or Representative Values)\nMarginal Effects at the Mean\nCounterfactual Marginal Effects\nConditional Marginal Effects\n\nThe rest of this vignette defines each of those quantities and explains how to use the slopes() and plot_slopes() functions to compute them. The main differences between these quantities pertain to (a) the regressor values at which we estimate marginal effects, and (b) the way in which unit-level marginal effects are aggregated.\nHeiss drew this exceedingly helpful graph which summarizes the information in the rest of this vignette:"
  },
  {
    "objectID": "articles/slopes.html#average-marginal-effect-ame",
    "href": "articles/slopes.html#average-marginal-effect-ame",
    "title": "\n4  Slopes\n",
    "section": "\n4.4 Average Marginal Effect (AME)",
    "text": "4.4 Average Marginal Effect (AME)\nA dataset with one marginal effect estimate per unit of observation is a bit unwieldy and difficult to interpret. Many analysts like to report the “Average Marginal Effect”, that is, the average of all the observation-specific marginal effects. These are easy to compute based on the full data.frame shown above, but the avg_slopes() function is convenient:\n\navg_slopes(mod)\n#> \n#>               Term           Contrast Estimate Std. Error      z Pr(>|z|)    S    2.5 %  97.5 %\n#>  bill_length_mm    dY/dX                0.0276    0.00578  4.772   <0.001 19.1  0.01625  0.0389\n#>  flipper_length_mm dY/dX                0.0106    0.00235  4.509   <0.001 17.2  0.00598  0.0152\n#>  species           Chinstrap - Adelie  -0.4148    0.05659 -7.330   <0.001 42.0 -0.52570 -0.3039\n#>  species           Gentoo - Adelie      0.0617    0.10688  0.577    0.564  0.8 -0.14778  0.2712\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNote that since marginal effects are derivatives, they are only properly defined for continuous numeric variables. When the model also includes categorical regressors, the summary function will try to display relevant (regression-adjusted) contrasts between different categories, as shown above.\nYou can also extract average marginal effects using tidy and glance methods which conform to the broom package specification:\n\ntidy(mfx)\n#> # A tibble: 4 × 8\n#>   term              contrast                       estimate std.error statistic  p.value conf.low conf.high\n#>   <chr>             <chr>                             <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n#> 1 bill_length_mm    mean(dY/dX)                      0.0276   0.00578     4.77  1.82e- 6  0.0162     0.0389\n#> 2 flipper_length_mm mean(dY/dX)                      0.0106   0.00235     4.51  6.52e- 6  0.00598    0.0152\n#> 3 species           mean(Chinstrap) - mean(Adelie)  -0.415    0.0566     -7.33  2.31e-13 -0.526     -0.304 \n#> 4 species           mean(Gentoo) - mean(Adelie)      0.0617   0.107       0.577 5.64e- 1 -0.148      0.271\n\nglance(mfx)\n#> # A tibble: 1 × 7\n#>     aic   bic r2.tjur  rmse  nobs     F logLik   \n#>   <dbl> <dbl>   <dbl> <dbl> <int> <dbl> <logLik> \n#> 1  180.  199.   0.695 0.276   342  15.7 -84.92257"
  },
  {
    "objectID": "articles/slopes.html#group-average-marginal-effect-g-ame",
    "href": "articles/slopes.html#group-average-marginal-effect-g-ame",
    "title": "\n4  Slopes\n",
    "section": "\n4.5 Group-Average Marginal Effect (G-AME)",
    "text": "4.5 Group-Average Marginal Effect (G-AME)\nWe can also use the by argument the average marginal effects within different subgroups of the observed data, based on values of the regressors. For example, to compute the average marginal effects of Bill Length for each Species, we do:\n\navg_slopes(\n  mod,\n  by = \"species\",\n  variables = \"bill_length_mm\")\n#> \n#>            Term    Contrast   species Estimate Std. Error    z Pr(>|z|)    S   2.5 %  97.5 %\n#>  bill_length_mm mean(dY/dX) Adelie     0.04354    0.00879 4.95   <0.001 20.4  0.0263 0.06077\n#>  bill_length_mm mean(dY/dX) Gentoo     0.00287    0.00284 1.01    0.312  1.7 -0.0027 0.00844\n#>  bill_length_mm mean(dY/dX) Chinstrap  0.03680    0.00976 3.77   <0.001 12.6  0.0177 0.05594\n#> \n#> Columns: term, contrast, species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nThis is equivalent to manually taking the mean of the observation-level marginal effect for each species sub-group:\n\naggregate(\n  mfx$estimate,\n  by = list(mfx$species, mfx$term),\n  FUN = mean)\n#>     Group.1           Group.2            x\n#> 1    Adelie    bill_length_mm  0.043539914\n#> 2 Chinstrap    bill_length_mm  0.036801185\n#> 3    Gentoo    bill_length_mm  0.002871562\n#> 4    Adelie flipper_length_mm  0.016710631\n#> 5 Chinstrap flipper_length_mm  0.014124217\n#> 6    Gentoo flipper_length_mm  0.001102231\n#> 7    Adelie           species -0.054519623\n#> 8 Chinstrap           species -0.313337522\n#> 9    Gentoo           species -0.250726004\n\nNote that marginaleffects follows Stata and the margins package in computing standard errors using the group-wise averaged Jacobian."
  },
  {
    "objectID": "articles/slopes.html#marginal-effect-at-user-specified-values",
    "href": "articles/slopes.html#marginal-effect-at-user-specified-values",
    "title": "\n4  Slopes\n",
    "section": "\n4.6 Marginal Effect at User-Specified Values",
    "text": "4.6 Marginal Effect at User-Specified Values\nSometimes, we are not interested in all the unit-specific marginal effects, but would rather look at the estimated marginal effects for certain “typical” individuals, or for user-specified values of the regressors. The datagrid() function helps us build a data grid full of “typical” rows. For example, to generate artificial Adelies and Gentoos with 180mm flippers:\n\ndatagrid(flipper_length_mm = 180,\n         species = c(\"Adelie\", \"Gentoo\"),\n         model = mod)\n#>   large_penguin bill_length_mm flipper_length_mm species\n#> 1     0.4853801       43.92193               180  Adelie\n#> 2     0.4853801       43.92193               180  Gentoo\n\nThe same command can be used (omitting the model argument) to marginaleffects’s newdata argument to compute marginal effects for those (fictional) individuals:\n\nslopes(\n  mod,\n  newdata = datagrid(\n    flipper_length_mm = 180,\n    species = c(\"Adelie\", \"Gentoo\")))\n#> \n#>               Term           Contrast Estimate Std. Error      z Pr(>|z|)    S    2.5 %   97.5 % bill_length_mm flipper_length_mm species\n#>  bill_length_mm    dY/dX                0.0607    0.03323  1.827   0.0677  3.9 -0.00443  0.12581           43.9               180  Adelie\n#>  bill_length_mm    dY/dX                0.0847    0.03939  2.150   0.0316  5.0  0.00747  0.16187           43.9               180  Gentoo\n#>  flipper_length_mm dY/dX                0.0233    0.00550  4.232   <0.001 15.4  0.01250  0.03408           43.9               180  Adelie\n#>  flipper_length_mm dY/dX                0.0325    0.00851  3.817   <0.001 12.9  0.01581  0.04918           43.9               180  Gentoo\n#>  species           Chinstrap - Adelie  -0.2111    0.10618 -1.988   0.0468  4.4 -0.41916 -0.00294           43.9               180  Adelie\n#>  species           Chinstrap - Adelie  -0.2111    0.10618 -1.988   0.0468  4.4 -0.41916 -0.00294           43.9               180  Gentoo\n#>  species           Gentoo - Adelie      0.1591    0.30242  0.526   0.5988  0.7 -0.43361  0.75185           43.9               180  Adelie\n#>  species           Gentoo - Adelie      0.1591    0.30242  0.526   0.5988  0.7 -0.43361  0.75185           43.9               180  Gentoo\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, large_penguin, bill_length_mm, flipper_length_mm, species\n\nWhen variables are omitted from the datagrid() call, they will automatically be set at their mean or mode (depending on variable type)."
  },
  {
    "objectID": "articles/slopes.html#marginal-effect-at-the-mean-mem",
    "href": "articles/slopes.html#marginal-effect-at-the-mean-mem",
    "title": "\n4  Slopes\n",
    "section": "\n4.7 Marginal Effect at the Mean (MEM)",
    "text": "4.7 Marginal Effect at the Mean (MEM)\nThe “Marginal Effect at the Mean” is a marginal effect calculated for a hypothetical observation where each regressor is set at its mean or mode. By default, the datagrid() function that we used in the previous section sets all regressors to their means or modes. To calculate the MEM, we can set the newdata argument, which determines the values of predictors at which we want to compute marginal effects:\n\nslopes(mod, newdata = \"mean\")\n#> \n#>               Term           Contrast Estimate Std. Error       z Pr(>|z|)    S    2.5 %  97.5 %\n#>  bill_length_mm    dY/dX                0.0502    0.01238   4.059   <0.001 14.3  0.02598  0.0745\n#>  flipper_length_mm dY/dX                0.0193    0.00553   3.487   <0.001 11.0  0.00844  0.0301\n#>  species           Chinstrap - Adelie  -0.8070    0.07636 -10.569   <0.001 84.3 -0.95670 -0.6574\n#>  species           Gentoo - Adelie      0.0829    0.11453   0.723    0.469  1.1 -0.14161  0.3073\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, large_penguin, bill_length_mm, flipper_length_mm, species"
  },
  {
    "objectID": "articles/slopes.html#counterfactual-marginal-effects",
    "href": "articles/slopes.html#counterfactual-marginal-effects",
    "title": "\n4  Slopes\n",
    "section": "\n4.8 Counterfactual Marginal Effects",
    "text": "4.8 Counterfactual Marginal Effects\nThe datagrid() function allowed us look at completely fictional individuals. Setting the grid_type argument of this function to \"counterfactual\" lets us compute the marginal effects for the actual observations in our dataset, but with a few manipulated values. For example, this code will create a data.frame twice as long as the original dat, where each observation is repeated with different values of the flipper_length_mm variable:\n\nnd <- datagrid(flipper_length_mm = c(160, 180),\n               model = mod,\n               grid_type = \"counterfactual\")\n\nWe see that the rows 1, 2, and 3 of the original dataset have been replicated twice, with different values of the flipper_length_mm variable:\n\nnd[nd$rowid %in% 1:3,]\n#>     rowidcf large_penguin bill_length_mm species flipper_length_mm\n#> 1         1             0           39.1  Adelie               160\n#> 2         2             0           39.5  Adelie               160\n#> 3         3             0           40.3  Adelie               160\n#> 343       1             0           39.1  Adelie               180\n#> 344       2             0           39.5  Adelie               180\n#> 345       3             0           40.3  Adelie               180\n\nWe can use the observation-level marginal effects to compute average (or median, or anything else) marginal effects over the counterfactual individuals:\n\nlibrary(dplyr)\n\nslopes(mod, newdata = nd) |>\n    group_by(term) |>\n    summarize(estimate = median(estimate))\n#> # A tibble: 3 × 2\n#>   term               estimate\n#>   <chr>                 <dbl>\n#> 1 bill_length_mm    0.00985  \n#> 2 flipper_length_mm 0.00378  \n#> 3 species           0.0000226"
  },
  {
    "objectID": "articles/slopes.html#conditional-marginal-effects-plot",
    "href": "articles/slopes.html#conditional-marginal-effects-plot",
    "title": "\n4  Slopes\n",
    "section": "\n4.9 Conditional Marginal Effects (Plot)",
    "text": "4.9 Conditional Marginal Effects (Plot)\nThe plot_slopes() function can be used to draw “Conditional Marginal Effects.” This is useful when a model includes interaction terms and we want to plot how the marginal effect of a variable changes as the value of a “condition” (or “moderator”) variable changes:\n\nmod <- lm(mpg ~ hp * wt + drat, data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"wt\")\n\n\n\n\nThe marginal effects in the plot above were computed with values of all regressors – except the variables and the condition – held at their means or modes, depending on variable type.\nSince plot_slopes() produces a ggplot2 object, it is easy to customize. For example:\n\nplot_slopes(mod, variables = \"hp\", condition = \"wt\") +\n    geom_rug(aes(x = wt), data = mtcars) +\n    theme_classic()"
  },
  {
    "objectID": "articles/slopes.html#example-quadratic",
    "href": "articles/slopes.html#example-quadratic",
    "title": "\n4  Slopes\n",
    "section": "\n4.10 Example: Quadratic",
    "text": "4.10 Example: Quadratic\nIn the “Definition” section of this vignette, we considered how marginal effects can be computed analytically in a simple quadratic equation context. We can now use the slopes() function to replicate our analysis of the quadratic function in a regression application.\nSay you estimate a linear regression model with a quadratic term:\n\\[Y = \\beta_0 + \\beta_1 X^2 + \\varepsilon\\]\nand obtain estimates of \\(\\beta_0=1\\) and \\(\\beta_1=2\\). Taking the partial derivative with respect to \\(X\\) and plugging in our estimates gives us the marginal effect of \\(X\\) on \\(Y\\):\n\\[\\partial Y / \\partial X = \\beta_0 + 2 \\cdot \\beta_1 X\\] \\[\\partial Y / \\partial X = 1 + 4X\\]\nThis result suggests that the effect of a change in \\(X\\) on \\(Y\\) depends on the level of \\(X\\). When \\(X\\) is large and positive, an increase in \\(X\\) is associated to a large increase in \\(Y\\). When \\(X\\) is small and positive, an increase in \\(X\\) is associated to a small increase in \\(Y\\). When \\(X\\) is a large negative value, an increase in \\(X\\) is associated with a decrease in \\(Y\\).\nmarginaleffects arrives at the same conclusion in simulated data:\n\nlibrary(tidyverse)\nN <- 1e5\nquad <- data.frame(x = rnorm(N))\nquad$y <- 1 + 1 * quad$x + 2 * quad$x^2 + rnorm(N)\nmod <- lm(y ~ x + I(x^2), quad)\n\nslopes(mod, newdata = datagrid(x = -2:2))  |>\n    mutate(truth = 1 + 4 * x) |>\n    select(estimate, truth)\n#> \n#>  Estimate\n#>        -7\n#>        -3\n#>         1\n#>         5\n#>         9\n#> \n#> Columns: estimate, truth\n\nWe can plot conditional adjusted predictions with plot_predictions() function:\n\nplot_predictions(mod, condition = \"x\")\n\n\n\n\nWe can plot conditional marginal effects with the plot_slopes() function (see section below):\n\nplot_slopes(mod, variables = \"x\", condition = \"x\")\n\n\n\n\nAgain, the conclusion is the same. When \\(x<0\\), an increase in \\(x\\) is associated with an decrease in \\(y\\). When \\(x>1/4\\), the marginal effect is positive, which suggests that an increase in \\(x\\) is associated with an increase in \\(y\\)."
  },
  {
    "objectID": "articles/slopes.html#slopes-vs-predictions-a-visual-interpretation",
    "href": "articles/slopes.html#slopes-vs-predictions-a-visual-interpretation",
    "title": "\n4  Slopes\n",
    "section": "\n4.11 Slopes vs Predictions: A Visual Interpretation",
    "text": "4.11 Slopes vs Predictions: A Visual Interpretation\nOften, analysts will plot predicted values of the outcome with a best fit line:\n\nlibrary(ggplot2)\n\nmod <- lm(mpg ~ hp * qsec, data = mtcars)\n\nplot_predictions(mod, condition = \"hp\", vcov = TRUE) +\n  geom_point(data = mtcars, aes(hp, mpg)) \n\n\n\n\nThe slope of this line is calculated using the same technique we all learned in grade school: dividing rise over run.\n\np <- plot_predictions(mod, condition = \"hp\", vcov = TRUE, draw = FALSE)\nplot_predictions(mod, condition = \"hp\", vcov = TRUE) +\n  geom_segment(aes(x = p$hp[10], xend = p$hp[10], y = p$estimate[10], yend = p$estimate[20])) +\n  geom_segment(aes(x = p$hp[10], xend = p$hp[20], y = p$estimate[20], yend = p$estimate[20])) +\n  annotate(\"text\", label = \"Rise\", y = 10, x = 140) +\n  annotate(\"text\", label = \"Run\", y = 2, x = 200)\n\n\n\n\nInstead of computing this slope manually, we can just call:\n\navg_slopes(mod, variables = \"hp\")\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S  2.5 %  97.5 %\n#>    hp   -0.112     0.0126 -8.92   <0.001 60.9 -0.137 -0.0874\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNow, consider the fact that our model includes an interaction between hp and qsec. This means that the slope will actually differ based on the value of the moderator variable qsec:\n\nplot_predictions(mod, condition = list(\"hp\", \"qsec\" = \"quartile\"))\n\n\n\n\nWe can estimate the slopes of these three fit lines easily:\n\nslopes(\n  mod,\n  variables = \"hp\",\n  newdata = datagrid(qsec = quantile(mtcars$qsec, probs = c(.25, .5, .75))))\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S  2.5 %  97.5 %  hp qsec\n#>    hp  -0.0934     0.0111 -8.43   <0.001 54.7 -0.115 -0.0717 147 16.9\n#>    hp  -0.1093     0.0123 -8.92   <0.001 60.9 -0.133 -0.0853 147 17.7\n#>    hp  -0.1325     0.0154 -8.60   <0.001 56.9 -0.163 -0.1023 147 18.9\n#> \n#> Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, qsec\n\nAs we see in the graph, all three slopes are negative, but the Q3 slope is steepest.\nWe could then push this one step further, and measure the slope of mpg with respect to hp, for all observed values of qsec. This is achieved with the plot_slopes() function:\n\nplot_slopes(mod, variables = \"hp\", condition = \"qsec\") +\n  geom_hline(yintercept = 0, linetype = 3)\n\n\n\n\nThis plot shows that the marginal effect of hp on mpg is always negative (the slope is always below zero), and that this effect becomes even more negative as qsec increases."
  },
  {
    "objectID": "articles/slopes.html#prediction-types",
    "href": "articles/slopes.html#prediction-types",
    "title": "\n4  Slopes\n",
    "section": "\n4.12 Prediction types",
    "text": "4.12 Prediction types\nThe marginaleffect function takes the derivative of the fitted (or predicted) values of the model, as is typically generated by the predict(model) function. By default, predict produces predictions on the \"response\" scale, so the marginal effects should be interpreted on that scale. However, users can pass a string or a vector of strings to the type argument, and marginaleffects will consider different outcomes.\nTypical values include \"response\" and \"link\", but users should refer to the documentation of the predict of the package they used to fit the model to know what values are allowable. documentation.\n\nmod <- glm(am ~ mpg, family = binomial, data = mtcars)\navg_slopes(mod, type = \"response\")\n#> \n#>  Term Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>   mpg   0.0465    0.00887 5.24   <0.001 22.6 0.0291 0.0639\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_slopes(mod, type = \"link\")\n#> \n#>  Term Estimate Std. Error    z Pr(>|z|)   S  2.5 % 97.5 %\n#>   mpg    0.307      0.115 2.67  0.00751 7.1 0.0819  0.532\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/marginalmeans.html#marginal-means-vs.-average-predictions",
    "href": "articles/marginalmeans.html#marginal-means-vs.-average-predictions",
    "title": "\n5  Marginal Means\n",
    "section": "\n5.1 Marginal Means vs. Average Predictions",
    "text": "5.1 Marginal Means vs. Average Predictions\nWhat should scientists report? Marginal means or average predictions?\nMany analysts ask this question, but unfortunately there isn’t a single answer. As explained above, marginal means are a special case of predictions, made on a perfectly balanced grid of categorical predictors, with numeric predictors held at their means, and marginalized with respect to some focal variables. Whether the analyst prefers to report this specific type of marginal means or another kind of average prediction will depend on the characteristics of the sample and the population to which they want to generalize.\nAfter reading this vignette and the discussion of emmeans in the Alternative Software vignette, you may want to consult with a statistician to discuss your specific real-world problem and make an informed choice."
  },
  {
    "objectID": "articles/marginalmeans.html#interactions",
    "href": "articles/marginalmeans.html#interactions",
    "title": "\n5  Marginal Means\n",
    "section": "\n5.2 Interactions",
    "text": "5.2 Interactions\nBy default, the marginal_means() function calculates marginal means for each categorical predictor one after the other. We can also compute marginal means for combinations of categories by setting cross=TRUE:\n\nlibrary(lme4)\n\ndat <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat <- read.csv(dat)\ntitanic <- glmer(\n    Survived ~ Sex * PClass + Age + (1 | PClass),\n    family = binomial,\n    data = dat)\n\nRegardless of the scale of the predictions (type argument), marginal_means() always computes standard errors using the Delta Method:\n\nmarginal_means(\n    titanic,\n    type = \"response\",\n    variables = c(\"Sex\", \"PClass\"))\n#> \n#>    Term  Value  Mean Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n#>  Sex    female 0.738     0.0207 35.68   <0.001 923.8 0.698  0.779\n#>  Sex    male   0.235     0.0203 11.62   <0.001 101.3 0.196  0.275\n#>  PClass 1st    0.708     0.0273 25.95   <0.001 490.9 0.654  0.761\n#>  PClass 2nd    0.511     0.0235 21.76   <0.001 346.4 0.465  0.557\n#>  PClass 3rd    0.242     0.0281  8.59   <0.001  56.7 0.187  0.297\n#> \n#> Results averaged over levels of: Sex, PClass \n#> Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWhen the model is linear or on the link scale, it also produces confidence intervals:\n\nmarginal_means(\n    titanic,\n    type = \"link\",\n    variables = c(\"Sex\", \"PClass\"))\n#> \n#>    Term  Value    Mean Std. Error       z Pr(>|z|)    S  2.5 % 97.5 %\n#>  Sex    female  1.6407      0.206   7.984   <0.001 49.3  1.238  2.043\n#>  Sex    male   -1.3399      0.124 -10.828   <0.001 88.3 -1.582 -1.097\n#>  PClass 1st     1.6307      0.271   6.028   <0.001 29.2  1.100  2.161\n#>  PClass 2nd     0.0997      0.211   0.472    0.637  0.7 -0.314  0.513\n#>  PClass 3rd    -1.2792      0.155  -8.255   <0.001 52.6 -1.583 -0.975\n#> \n#> Results averaged over levels of: Sex, PClass \n#> Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nIt is easy to transform those link-scale marginal means with arbitrary functions using the transform argument:\n\nmarginal_means(\n    titanic,\n    type = \"link\",\n    transform = insight::link_inverse(titanic),\n    variables = c(\"Sex\", \"PClass\"))\n#> \n#>    Term  Value  Mean Pr(>|z|)    S 2.5 % 97.5 %\n#>  Sex    female 0.838   <0.001 49.3 0.775  0.885\n#>  Sex    male   0.208   <0.001 88.3 0.170  0.250\n#>  PClass 1st    0.836   <0.001 29.2 0.750  0.897\n#>  PClass 2nd    0.525    0.637  0.7 0.422  0.626\n#>  PClass 3rd    0.218   <0.001 52.6 0.170  0.274\n#> \n#> Results averaged over levels of: Sex, PClass \n#> Columns: term, value, estimate, p.value, s.value, conf.low, conf.high\n\nmarginal_means() defaults to reporting EMMs for each category individually, without cross-margins:\n\ntitanic2 <- glmer(\n    Survived ~ Sex + PClass + Age + (1 | PClass),\n    family = binomial,\n    data = dat)\n\nmarginal_means(\n    titanic2,\n    variables = c(\"Sex\", \"PClass\"))\n#> \n#>    Term  Value  Mean Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>  Sex    female 0.741     0.0240 30.8   <0.001 691.0 0.694  0.788\n#>  Sex    male   0.253     0.0203 12.5   <0.001 116.0 0.213  0.293\n#>  PClass 1st    0.707     0.0289 24.5   <0.001 436.5 0.650  0.763\n#>  PClass 2nd    0.494     0.0287 17.2   <0.001 217.5 0.437  0.550\n#>  PClass 3rd    0.291     0.0268 10.9   <0.001  88.8 0.238  0.344\n#> \n#> Results averaged over levels of: Sex, PClass \n#> Columns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe can force the cross:\n\nmarginal_means(\n    titanic2,\n    cross = TRUE,\n    variables = c(\"Sex\", \"PClass\"))\n#> \n#>    Mean Std. Error     z Pr(>|z|)     S  2.5 % 97.5 %\n#>  0.9288     0.0161 57.71   <0.001   Inf 0.8973 0.9604\n#>  0.7819     0.0357 21.93   <0.001 351.8 0.7120 0.8518\n#>  0.5118     0.0458 11.17   <0.001  93.8 0.4220 0.6017\n#>  0.4844     0.0468 10.35   <0.001  81.0 0.3926 0.5761\n#>  0.2051     0.0308  6.66   <0.001  35.1 0.1448 0.2655\n#>  0.0702     0.0135  5.18   <0.001  22.1 0.0436 0.0967\n#> \n#> Columns: Sex, PClass, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/marginalmeans.html#group-averages-with-the-by-argument",
    "href": "articles/marginalmeans.html#group-averages-with-the-by-argument",
    "title": "\n5  Marginal Means\n",
    "section": "\n5.3 Group averages with the by argument",
    "text": "5.3 Group averages with the by argument\nWe can collapse marginal means via averaging using the by argument:\n\ndat <- mtcars\ndat$am <- factor(dat$am)\ndat$vs <- factor(dat$vs)\ndat$cyl <- factor(dat$cyl)\n\nmod <- glm(gear ~ cyl + vs + am, data = dat, family = poisson)\n\nby <- data.frame(\n    by = c(\"(4 & 6)\", \"(4 & 6)\", \"(8)\"),\n    cyl = c(4, 6, 8))\n\nmarginal_means(mod, by = by, variables = \"cyl\")\n#> \n#>       By Mean Pr(>|z|)    S 2.5 % 97.5 %\n#>  (4 & 6) 3.86   <0.001 59.1  2.86   5.22\n#>  (8)     3.59   <0.001 18.5  2.11   6.13\n#> \n#> Results averaged over levels of: vs, am, cyl \n#> Columns: by, estimate, p.value, s.value, conf.low, conf.high\n\nAnd we can use the hypothesis argument to compare those new collapsed subgroups:\n\nmarginal_means(mod, by = by, variables = \"cyl\", hypothesis = \"pairwise\")\n#> \n#>           Term  Mean Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  (4 & 6) - (8) 0.271       1.39 0.195    0.845 0.2 -2.45      3\n#> \n#> Results averaged over levels of: vs, am, cyl \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/marginalmeans.html#custom-contrasts-and-linear-combinations",
    "href": "articles/marginalmeans.html#custom-contrasts-and-linear-combinations",
    "title": "\n5  Marginal Means\n",
    "section": "\n5.4 Custom Contrasts and Linear Combinations",
    "text": "5.4 Custom Contrasts and Linear Combinations\nSee the vignette on Custom Contrasts and Combinations"
  },
  {
    "objectID": "articles/marginalmeans.html#tidy-summaries",
    "href": "articles/marginalmeans.html#tidy-summaries",
    "title": "\n5  Marginal Means\n",
    "section": "\n5.5 Tidy summaries",
    "text": "5.5 Tidy summaries\nThe summary, tidy, and glance functions are also available to summarize and manipulate the results:\n\nmm <- marginal_means(mod)\n\ntidy(mm)\n#> # A tibble: 7 × 7\n#>   term  value estimate  p.value conf.low conf.high s.value\n#>   <chr> <fct>    <dbl>    <dbl>    <dbl>     <dbl>   <dbl>\n#> 1 cyl   6         3.90 1.57e-12     2.67      5.68    39.2\n#> 2 cyl   4         3.83 2.34e- 8     2.39      6.13    25.4\n#> 3 cyl   8         3.59 2.66e- 6     2.11      6.13    18.5\n#> 4 vs    0         3.79 1.88e-12     2.62      5.50    39.0\n#> 5 vs    1         3.75 5.82e-10     2.47      5.69    30.7\n#> 6 am    1         4.34 6.83e-19     3.14      6.01    60.3\n#> 7 am    0         3.27 3.96e-15     2.43      4.40    47.8\n\nglance(mm)\n#> # A tibble: 1 × 7\n#>     aic   bic r2.nagelkerke  rmse  nobs     F logLik   \n#>   <dbl> <dbl>         <dbl> <dbl> <int> <dbl> <logLik> \n#> 1  113.  120.         0.672 0.437    32 0.737 -51.50168\n\nsummary(mm)\n#> \n#>  Term Value Mean Pr(>|z|)    S 2.5 % 97.5 %\n#>   cyl     6 3.90   <0.001 39.2  2.67   5.68\n#>   cyl     4 3.83   <0.001 25.4  2.39   6.13\n#>   cyl     8 3.59   <0.001 18.5  2.11   6.13\n#>   vs      0 3.79   <0.001 39.0  2.62   5.50\n#>   vs      1 3.75   <0.001 30.7  2.47   5.69\n#>   am      1 4.34   <0.001 60.3  3.14   6.01\n#>   am      0 3.27   <0.001 47.8  2.43   4.40\n#> \n#> Results averaged over levels of: cyl, vs, am \n#> Columns: term, value, estimate, p.value, s.value, conf.low, conf.high\n\nThanks to those tidiers, we can also present the results in the style of a regression table using the modelsummary package."
  },
  {
    "objectID": "articles/marginalmeans.html#case-study-multinomial-logit",
    "href": "articles/marginalmeans.html#case-study-multinomial-logit",
    "title": "\n5  Marginal Means\n",
    "section": "\n5.6 Case study: Multinomial Logit",
    "text": "5.6 Case study: Multinomial Logit\nThis example requires version 0.2.0 of the marginaleffects package.\nTo begin, we generate data and estimate a large model:\n\nlibrary(nnet)\nlibrary(marginaleffects)\n\nset.seed(1839)\nn <- 1200\nx <- factor(sample(letters[1:3], n, TRUE))\ny <- vector(length = n)\ny[x == \"a\"] <- sample(letters[4:6], sum(x == \"a\"), TRUE)\ny[x == \"b\"] <- sample(letters[4:6], sum(x == \"b\"), TRUE, c(1 / 4, 2 / 4, 1 / 4))\ny[x == \"c\"] <- sample(letters[4:6], sum(x == \"c\"), TRUE, c(1 / 5, 3 / 5, 2 / 5))\n\ndat <- data.frame(x = x, y = factor(y))\ntmp <- as.data.frame(replicate(20, factor(sample(letters[7:9], n, TRUE))))\ndat <- cbind(dat, tmp)\nvoid <- capture.output({\n    mod <- multinom(y ~ ., dat)\n})\n\nTry to compute marginal means, but realize that your grid won’t fit in memory:\n\nmarginal_means(mod, type = \"probs\")\n#> Error: You are trying to create a prediction grid with more than 1 billion rows, which is likely to exceed the memory and computational power available on your local machine. Presumably this is because you are considering many variables with many levels. All of the functions in the `marginaleffects` package include arguments to specify a restricted list of variables over which to create a prediction grid.\n\nUse the variables and variables_grid arguments to compute marginal means over a more reasonably sized grid:\n\nmarginal_means(mod,\n              type = \"probs\",\n              variables = c(\"x\", \"V1\"),\n              variables_grid = paste0(\"V\", 2:3))"
  },
  {
    "objectID": "articles/marginalmeans.html#plot-conditional-marginal-means",
    "href": "articles/marginalmeans.html#plot-conditional-marginal-means",
    "title": "\n5  Marginal Means\n",
    "section": "\n5.7 Plot conditional marginal means",
    "text": "5.7 Plot conditional marginal means\nThe marginaleffects package offers several functions to plot how some quantities vary as a function of others:\n\n\nplot_predictions: Conditional adjusted predictions – how does the predicted outcome change as a function of regressors?\n\nplot_comparisons: Conditional comparisons – how do contrasts change as a function of regressors?\n\nplot_slopes: Conditional marginal effects – how does the slope change as a function of regressors?\n\nThere is no analogous function for marginal means. However, it is very easy to achieve a similar effect using the predictions() function, its by argument, and standard plotting functions. In the example below, we take these steps:\n\nEstimate a model with one continuous (hp) and one categorical regressor (cyl).\nCreate a perfectly “balanced” data grid for each combination of hp and cyl. This is specified by the user in the datagrid() call.\nCompute fitted values (aka “adjusted predictions”) for each cell of the grid.\nUse the by argument to take the average of predicted values for each value of hp, across margins of cyl.\nCompute standard errors around the averaged predicted values (i.e., marginal means).\nCreate symmetric confidence intervals in the usual manner.\nPlot the results.\n\n\nlibrary(ggplot2)\n\nmod <- lm(mpg ~ hp + factor(cyl), data = mtcars)\n\np <- predictions(mod,\n    by = \"hp\",\n    newdata = datagrid(\n        model = mod,\n        hp = seq(100, 120, length.out = 10),\n        cyl = mtcars$cyl))\n\nggplot(p) +\n    geom_ribbon(aes(hp, ymin = conf.low, ymax = conf.high), alpha = .2) +\n    geom_line(aes(hp, estimate))"
  },
  {
    "objectID": "articles/plot.html#predictions",
    "href": "articles/plot.html#predictions",
    "title": "\n6  Plots\n",
    "section": "\n6.1 Predictions",
    "text": "6.1 Predictions\n\n6.1.1 Conditional predictions\nWe call a prediction “conditional” when it is made on a grid of user-specified values. For example, we predict penguins’ body mass for different values of flipper length and species:\n\npre <- predictions(mod, newdata = datagrid(flipper_length_mm = c(172, 231), species = unique))\npre\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 % bill_length_mm island flipper_length_mm   species\n#>      3859        204 18.9   <0.001 263.0  3460   4259           43.9 Biscoe               172 Adelie   \n#>      2545        369  6.9   <0.001  37.5  1822   3268           43.9 Biscoe               172 Gentoo   \n#>      3146        234 13.5   <0.001 134.6  2688   3604           43.9 Biscoe               172 Chinstrap\n#>      4764        362 13.2   <0.001 128.9  4054   5474           43.9 Biscoe               231 Adelie   \n#>      5597        155 36.0   <0.001 940.9  5292   5901           43.9 Biscoe               231 Gentoo   \n#>      4086        469  8.7   <0.001  58.1  3166   5006           43.9 Biscoe               231 Chinstrap\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, body_mass_g, bill_length_mm, island, flipper_length_mm, species\n\nThe condition argument of the plot_predictions() function can be used to build meaningful grids of predictor values somewhat more easily:\n\nplot_predictions(mod, condition = c(\"flipper_length_mm\", \"species\"))\n\n\n\n\nNote that the values at each end of the x-axis correspond to the numerical results produced above. For example, the predicted outcome for a Gentoo with 231mm flippers is 5597.\nWe can include a 3rd conditioning variable, specify what values we want to consider, supply R functions to compute summaries, and use one of several string shortcuts for common reference values (“threenum”, “minmax”, “quartile”, etc.):\n\nplot_predictions(\n    mod,\n    condition = list(\n        \"flipper_length_mm\" = 180:220,\n        \"bill_length_mm\" = \"threenum\",\n        \"species\" = unique))\n\n\n\n\nSee ?plot_predictions for more information.\n\n6.1.2 Marginal predictions\nWe call a prediction “marginal” when it is the result of a two step process: (1) make predictions for each observed unit in the original dataset, and (2) average predictions across one or more categorical predictors. For example:\n\npredictions(mod, by = \"species\")\n#> \n#>    species Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  Adelie        3701       27.2 136.1   <0.001 Inf  3647   3754\n#>  Gentoo        5076       30.1 168.5   <0.001 Inf  5017   5135\n#>  Chinstrap     3733       40.5  92.2   <0.001 Inf  3654   3812\n#> \n#> Columns: species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe can plot those predictions by using the analogous command:\n\nplot_predictions(mod, by = \"species\")\n\n\n\n\nWe can also make predictions at the intersections of different variables:\n\npredictions(mod, by = c(\"species\", \"island\"))\n#> \n#>    species    island Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  Adelie    Torgersen     3706       46.8  79.2   <0.001 Inf  3615   3798\n#>  Adelie    Biscoe        3710       50.4  73.7   <0.001 Inf  3611   3808\n#>  Adelie    Dream         3688       44.6  82.6   <0.001 Inf  3601   3776\n#>  Gentoo    Biscoe        5076       30.1 168.5   <0.001 Inf  5017   5135\n#>  Chinstrap Dream         3733       40.5  92.2   <0.001 Inf  3654   3812\n#> \n#> Columns: species, island, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNote that certain species only live on certain islands. Visually:\n\nplot_predictions(mod, by = c(\"species\", \"island\"))"
  },
  {
    "objectID": "articles/plot.html#comparisons",
    "href": "articles/plot.html#comparisons",
    "title": "\n6  Plots\n",
    "section": "\n6.2 Comparisons",
    "text": "6.2 Comparisons\n\n6.2.1 Conditional comparisons\nThe syntax for conditional comparisons is the same as the syntax for conditional predictions, except that we now need to specify the variable(s) of interest using an additional argument:\n\ncomparisons(mod,\n  variables = \"flipper_length_mm\",\n  newdata = datagrid(flipper_length_mm = c(172, 231), species = unique))\n#> \n#>               Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 % bill_length_mm island   species\n#>  flipper_length_mm       +1     15.3       9.25 1.66   0.0976  3.4 -2.81   33.5           43.9 Biscoe Adelie   \n#>  flipper_length_mm       +1     51.7       8.70 5.95   <0.001 28.5 34.68   68.8           43.9 Biscoe Gentoo   \n#>  flipper_length_mm       +1     15.9      11.37 1.40   0.1610  2.6 -6.34   38.2           43.9 Biscoe Chinstrap\n#>  flipper_length_mm       +1     15.3       9.25 1.66   0.0976  3.4 -2.81   33.5           43.9 Biscoe Adelie   \n#>  flipper_length_mm       +1     51.7       8.70 5.95   <0.001 28.5 34.68   68.8           43.9 Biscoe Gentoo   \n#>  flipper_length_mm       +1     15.9      11.37 1.40   0.1610  2.6 -6.34   38.2           43.9 Biscoe Chinstrap\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, body_mass_g, bill_length_mm, island, flipper_length_mm, species\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  condition = c(\"bill_length_mm\", \"species\"))\n\n\n\n\nWe can specify custom comparisons, as we would using the variables argument of the comparisons() function. For example, see what happens to the predicted outcome when flipper_length_mm increases by 1 standard deviation or by 10mm:\n\nplot_comparisons(mod,\n  variables = list(\"flipper_length_mm\" = \"sd\"),\n  condition = c(\"bill_length_mm\", \"species\")) +\n\nplot_comparisons(mod,\n  variables = list(\"flipper_length_mm\" = 10),\n  condition = c(\"bill_length_mm\", \"species\"))\n\n\n\n\nNotice that the vertical scale is different in the plots above, reflecting the fact that we are plotting the effect of a change of 1 standard deviation on the left vs 10 units on the right.\nLike the comparisons() function, plot_comparisons() is a very powerful tool because it allows us to compute and display custom comparisons such as differences, ratios, odds, and arbitrary functions of predicted outcomes. For example, if we want to plot the ratio of predicted body mass for different species of penguins, we could do:\n\nplot_comparisons(mod,\n  variables = \"species\",\n  condition = \"bill_length_mm\",\n  comparison = \"ratio\")\n\n\n\n\nThe left panel shows that the ratio of Chinstrap body mass to Adelie body mass is approximately constant, at slightly above 0.8. The right panel shows that the ratio of Gentoo to Adelie body mass is depends on their bill length. For birds with short bills, Gentoos seem to have smaller body mass than Adelies. For birds with long bills, Gentoos seem heavier than Adelies, although the null ratio (1) is not outside the confidence interval.\n\n6.2.2 Marginal comparisons\nAs above, we can also display marginal comparisons, by subgroups:\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  by = \"species\") +\n\nplot_comparisons(mod,\n  variables = \"flipper_length_mm\",\n  by = c(\"species\", \"island\"))\n\n\n\n\nMultiple contrasts at once:\n\nplot_comparisons(mod,\n  variables = c(\"flipper_length_mm\", \"bill_length_mm\"),\n  by = c(\"species\", \"island\"))"
  },
  {
    "objectID": "articles/plot.html#slopes",
    "href": "articles/plot.html#slopes",
    "title": "\n6  Plots\n",
    "section": "\n6.3 Slopes",
    "text": "6.3 Slopes\nIf you have read the sections above, the behavior of the plot_slopes() function should not surprise. Here we give two examples in which we compute display the elasticity of body mass with respect to bill length:\n\n## conditional\nplot_slopes(mod,\n  variables = \"bill_length_mm\",\n  slope = \"eyex\",\n  condition = c(\"species\", \"island\"))\n\n\n\n\n## marginal\nplot_slopes(mod,\n  variables = \"bill_length_mm\",\n  slope = \"eyex\",\n  by = c(\"species\", \"island\"))\n\n\n\n\nAnd here is an example of a marginal effects (aka “slopes” or “partial derivatives”) plot for a model with multiplicative interactions between continuous variables:\n\nmod2 <- lm(mpg ~ wt * qsec * factor(gear), data = mtcars)\n\nplot_slopes(mod2, variables = \"qsec\", condition = c(\"wt\", \"gear\"))"
  },
  {
    "objectID": "articles/plot.html#uncertainty-estimates",
    "href": "articles/plot.html#uncertainty-estimates",
    "title": "\n6  Plots\n",
    "section": "\n6.4 Uncertainty estimates",
    "text": "6.4 Uncertainty estimates\nAs with all the other functions in the package, the plot_*() functions have a conf_level argument and a vcov argument which can be used to control the size of confidence intervals and the types of standard errors used:\n\nplot_slopes(mod,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200)) +\n\n## clustered standard errors\nplot_slopes(mod,\n  vcov = ~island,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200)) +\n\n## alpha level\nplot_slopes(mod,\n  conf_level = .8,\n  variables = \"bill_length_mm\", condition = \"flipper_length_mm\") +\n  ylim(c(-150, 200))"
  },
  {
    "objectID": "articles/plot.html#customization",
    "href": "articles/plot.html#customization",
    "title": "\n6  Plots\n",
    "section": "\n6.5 Customization",
    "text": "6.5 Customization\nA very useful feature of the plotting functions in this package is that they produce normal ggplot2 objects. So we can customize them to our heart’s content, using ggplot2 itself, or one of the many packages designed to augment its functionalities:\n\nlibrary(ggrepel)\n\nmt <- mtcars\nmt$label <- row.names(mt)\n\nmod <- lm(mpg ~ hp * factor(cyl), data = mt)\n\nplot_predictions(mod, condition = c(\"hp\", \"cyl\"), points = .5, rug = TRUE, vcov = FALSE) +\n    geom_text_repel(aes(x = hp, y = mpg, label = label),\n                    data = subset(mt, hp > 250),\n                    nudge_y = 2) +\n    theme_classic()\n\n\n\n\nAll the plotting functions work with all the model supported by the marginaleffects package, so we can plot the output of a logistic regression model. This plot shows the probability of survival aboard the Titanic, for different ages and different ticket classes:\n\nlibrary(ggdist)\nlibrary(ggplot2)\n\ndat <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat <- read.csv(dat)\n\nmod <- glm(Survived ~ Age * SexCode * PClass, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"Age\", \"PClass\")) +\n    geom_dots(\n        alpha = .8,\n        scale = .3,\n        pch = 18,\n        data = dat, aes(\n        x = Age,\n        y = Survived,\n        side = ifelse(Survived == 1, \"bottom\", \"top\")))\n\n\n\n\nThanks to Andrew Heiss who inspired this plot.\nDesigning effective data visualizations requires a lot of customization to the specific context and data. The plotting functions in marginaleffects offer a powerful way to iterate quickly between plots and models, but they obviously cannot support all the features that users may want. Thankfully, it is very easy to use the slopes() functions to generate datasets that can then be used in ggplot2 or any other data visualization tool. Just use the draw argument:\n\np <- plot_predictions(mod, condition = c(\"Age\", \"PClass\"), draw = FALSE)\nhead(p)\n#>   rowid  estimate      p.value   s.value  conf.low conf.high  Survived   SexCode     Age PClass\n#> 1     1 0.8679723 0.0013307148  9.553583 0.6754794 0.9540527 0.4140212 0.3809524 0.17000    1st\n#> 2     2 0.8956789 0.0001333343 12.872665 0.7401973 0.9627887 0.4140212 0.3809524 0.17000    2nd\n#> 3     3 0.4044513 0.2667759616  1.906299 0.2554245 0.5734603 0.4140212 0.3809524 0.17000    3rd\n#> 4     4 0.8631027 0.0011563592  9.756195 0.6749549 0.9503543 0.4140212 0.3809524 1.61551    1st\n#> 5     5 0.8813224 0.0001728862 12.497890 0.7228529 0.9548415 0.4140212 0.3809524 1.61551    2nd\n#> 6     6 0.3934924 0.1899483112  2.396321 0.2535791 0.5533716 0.4140212 0.3809524 1.61551    3rd\n\nThis allows us to feed the data easily to other functions, such as those in the useful ggdist and distributional packages:\n\nlibrary(ggdist)\nlibrary(distributional)\nplot_slopes(mod, variables = \"SexCode\", condition = c(\"Age\", \"PClass\"), type = \"link\", draw = FALSE) |>\n  ggplot() +\n  stat_lineribbon(aes(\n    x = Age,\n    ydist = dist_normal(mu = estimate, sigma = std.error),\n    fill = PClass),\n    alpha = 1 / 4)"
  },
  {
    "objectID": "articles/plot.html#fits-and-smooths",
    "href": "articles/plot.html#fits-and-smooths",
    "title": "\n6  Plots\n",
    "section": "\n6.6 Fits and smooths",
    "text": "6.6 Fits and smooths\nWe can compare the model predictors with fits and smoothers using the geom_smooth() function from the ggplot2 package:\n\ndat <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Stat2Data/Titanic.csv\"\ndat <- read.csv(dat)\nmod <- glm(Survived ~ Age * PClass, data = dat, family = binomial)\n\nplot_predictions(mod, condition = c(\"Age\", \"PClass\")) +\n    geom_smooth(data = dat, aes(Age, Survived), method = \"lm\", se = FALSE, color = \"black\") +\n    geom_smooth(data = dat, aes(Age, Survived), se = FALSE, color = \"black\")"
  },
  {
    "objectID": "articles/plot.html#groups-and-categorical-outcomes",
    "href": "articles/plot.html#groups-and-categorical-outcomes",
    "title": "\n6  Plots\n",
    "section": "\n6.7 Groups and categorical outcomes",
    "text": "6.7 Groups and categorical outcomes\nIn some models, marginaleffects functions generate different estimates for different groups, such as categorical outcomes. For example,\n\nlibrary(MASS)\nmod <- polr(factor(gear) ~ mpg + hp, data = mtcars)\n\npredictions(mod)\n#> \n#>  Group Estimate Std. Error    z Pr(>|z|)    S   2.5 % 97.5 %\n#>      3   0.5316     0.1127 4.72   <0.001 18.7  0.3107  0.753\n#>      3   0.5316     0.1127 4.72   <0.001 18.7  0.3107  0.753\n#>      3   0.4492     0.1200 3.74   <0.001 12.4  0.2140  0.684\n#>      3   0.4944     0.1111 4.45   <0.001 16.8  0.2765  0.712\n#>      3   0.4213     0.1142 3.69   <0.001 12.1  0.1974  0.645\n#> --- 86 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#>      5   0.6894     0.1956 3.52   <0.001 11.2  0.3059  1.073\n#>      5   0.1650     0.1290 1.28   0.2010  2.3 -0.0879  0.418\n#>      5   0.1245     0.0698 1.78   0.0744  3.7 -0.0123  0.261\n#>      5   0.3779     0.3244 1.17   0.2440  2.0 -0.2579  1.014\n#>      5   0.0667     0.0458 1.46   0.1455  2.8 -0.0231  0.157\n#> Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, hp\n\nWe can plot those estimates in the same way as before, by specifying group as one of the conditional variable, or by adding that column to a facet_wrap() call:\n\nplot_predictions(mod, condition = c(\"mpg\", \"group\"), type = \"probs\", vcov = FALSE)\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"probs\", vcov = FALSE) +\n  facet_wrap(~ group)"
  },
  {
    "objectID": "articles/plot.html#plot-and-marginaleffects-objects",
    "href": "articles/plot.html#plot-and-marginaleffects-objects",
    "title": "\n6  Plots\n",
    "section": "\n6.8 plot() and marginaleffects objects",
    "text": "6.8 plot() and marginaleffects objects\nSome users may feel inclined to call plot() on a object produced by marginaleffects object. Doing so will generate an informative error like this one:\n\nmod <- lm(mpg ~ hp * wt * factor(cyl), data = mtcars)\np <- predictions(mod)\nplot(p)\n#> Error: Please use the `plot_predictions()` function.\n\nThe reason for this error is that the user query is underspecified. marginaleffects allows users to compute so many quantities of interest that it is not clear what the user wants when they simply call plot(). Adding several new arguments would compete with the main plotting functions, and risk sowing confusion. The marginaleffects developers thus decided to support one main path to plotting: plot_predictions(), plot_comparisons(), and plot_slopes().\nThat said, it may be useful to remind users that all marginaleffects output are standard “tidy” data frames. Although they get pretty-printed to the console, all the listed columns are accessible via standard R operators. For example:\n\np <- avg_predictions(mod, by = \"cyl\")\np\n#> \n#>  cyl Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>    6     19.7      0.871 22.7   <0.001 375.1  18.0   21.5\n#>    4     26.7      0.695 38.4   <0.001   Inf  25.3   28.0\n#>    8     15.1      0.616 24.5   <0.001 438.2  13.9   16.3\n#> \n#> Columns: cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\np$estimate\n#> [1] 19.74286 26.66364 15.10000\n\np$std.error\n#> [1] 0.8713835 0.6951236 0.6161612\n\np$conf.low\n#> [1] 18.03498 25.30122 13.89235\n\nThis allows us to plot all results very easily with standard plotting functions:\n\nplot_predictions(mod, by = \"cyl\")\n\n\n\n\nplot(p$cyl, p$estimate)\n\n\n\n\nggplot(p, aes(x = cyl, y = estimate, ymin = conf.low, ymax = conf.high)) +\n  geom_pointrange()"
  },
  {
    "objectID": "articles/hypothesis.html#null-hypothesis",
    "href": "articles/hypothesis.html#null-hypothesis",
    "title": "\n7  Hypothesis Tests\n",
    "section": "\n7.1 Null hypothesis",
    "text": "7.1 Null hypothesis\nThe simplest way to modify a hypothesis test is to change the null hypothesis. By default, all functions in the marginaleffects package assume that the null is 0. This can be changed by changing the hypothesis argument.\nFor example, consider a logistic regression model:\n\nlibrary(marginaleffects)\nmod <- glm(am ~ hp + drat, data = mtcars, family = binomial)\n\nWe can compute the predicted outcome for a hypothetical unit where all regressors are fixed to their sample means:\n\npredictions(mod, newdata = \"mean\")\n#> \n#>  Estimate Pr(>|z|)   S  2.5 % 97.5 %  hp drat\n#>     0.231    0.135 2.9 0.0584  0.592 147  3.6\n#> \n#> Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, am, hp, drat\n\nThe Z statistic and p value reported above assume that the null hypothesis equals zero. We can change the null with the hypothesis argument:\n\npredictions(mod, newdata = \"mean\", hypothesis = .5)\n#> \n#>  Estimate Pr(>|z|)   S  2.5 % 97.5 %  hp drat\n#>     0.231   0.0343 4.9 0.0584  0.592 147  3.6\n#> \n#> Columns: rowid, estimate, p.value, s.value, conf.low, conf.high, am, hp, drat\n\nThis can obviously be useful in other contexts. For instance, if we compute risk ratios (at the mean) associated with an increase of 1 unit in hp, it makes more sense to test the null hypothesis that the ratio of predictions is 1 rather than 0:\n\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    variables = \"hp\",\n    comparison = \"ratio\",\n    hypothesis = 1) |>\n    print(digits = 3)\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %\n#>    hp       +1     1.01    0.00793 1.05    0.293 1.8 0.993   1.02\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, hp, drat\n\nWarning: Z statistics and p values are computed before applying functions in transform."
  },
  {
    "objectID": "articles/hypothesis.html#hypothesis-tests-with-the-delta-method",
    "href": "articles/hypothesis.html#hypothesis-tests-with-the-delta-method",
    "title": "\n7  Hypothesis Tests\n",
    "section": "\n7.2 Hypothesis tests with the delta method",
    "text": "7.2 Hypothesis tests with the delta method\nVersion 0.6.0 of marginaleffects includes powerful function called hypotheses(). This function emulates the behavior of the well-established car::deltaMethod and car::linearHypothesis functions, but it supports more models, requires fewer dependencies, and offers some convenience features like shortcuts for robust standard errors.\nhypotheses() can be used to compute estimates and standard errors of arbitrary functions of model parameters. For example, it can be used to conduct tests of equality between coefficients, or to test the value of some linear or non-linear combination of quantities of interest. hypotheses() can also be used to conduct hypothesis tests on other functions of a model’s parameter, such as adjusted predictions or marginal effects.\nLet’s start by estimating a simple model:\n\nlibrary(marginaleffects)\nmod <- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n\nWhen the FUN and hypothesis arguments of hypotheses() equal NULL (the default), the function returns a data.frame of raw estimates:\n\nhypotheses(mod)\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)   2.5 %    97.5 %     S\n#>    b1  35.8460      2.041 17.56   <0.001 31.8457 39.846319 227.0\n#>    b2  -0.0231      0.012 -1.93   0.0531 -0.0465  0.000306   4.2\n#>    b3  -3.1814      0.720 -4.42   <0.001 -4.5918 -1.771012  16.6\n#>    b4  -3.3590      1.402 -2.40   0.0166 -6.1062 -0.611803   5.9\n#>    b5  -3.1859      2.170 -1.47   0.1422 -7.4399  1.068169   2.8\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nTest of equality between coefficients:\n\nhypotheses(mod, \"hp = wt\")\n#> \n#>     Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %    S\n#>  hp = wt     3.16       0.72 4.39   <0.001  1.75   4.57 16.4\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nNon-linear function of coefficients\n\nhypotheses(mod, \"exp(hp + wt) = 0.1\")\n#> \n#>                Term Estimate Std. Error     z Pr(>|z|)  2.5 %  97.5 %   S\n#>  exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 -0.117 -0.0022 4.6\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nThe vcov argument behaves in the same was as in the slopes() function. It allows us to easily compute robust standard errors:\n\nhypotheses(mod, \"hp = wt\", vcov = \"HC3\")\n#> \n#>     Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %    S\n#>  hp = wt     3.16      0.805 3.92   <0.001  1.58   4.74 13.5\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nWe can use shortcuts like b1, b2, ... to identify the position of each parameter in the output of FUN. For example, b2=b3 is equivalent to hp=wt because those term names appear in the 2nd and 3rd row when we call hypotheses(mod).\n\nhypotheses(mod, \"b2 = b3\")\n#> \n#>     Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %    S\n#>  b2 = b3     3.16       0.72 4.39   <0.001  1.75   4.57 16.4\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nTerm names with special characters must be enclosed in backticks:\n\nhypotheses(mod, \"`factor(cyl)6` = `factor(cyl)8`\")\n#> \n#>                             Term Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %   S\n#>  `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 -3.41   3.07 0.1\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nThe FUN argument can be used to compute standard errors for arbitrary functions of model parameters. This user-supplied function must accept a single model object, and return a numeric vector or a data.frame with two columns named term and estimate.\n\nmod <- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf <- function(x) predict(x, type = \"link\", newdata = mtcars)\np <- hypotheses(mod, FUN = f)\nhead(p)\n#> \n#>  Term Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %   S\n#>    b1   -1.098      0.716 -1.534    0.125 -2.50  0.305 3.0\n#>    b2   -1.098      0.716 -1.534    0.125 -2.50  0.305 3.0\n#>    b3    0.233      0.781  0.299    0.765 -1.30  1.764 0.4\n#>    b4   -0.595      0.647 -0.919    0.358 -1.86  0.674 1.5\n#>    b5   -0.418      0.647 -0.645    0.519 -1.69  0.851 0.9\n#>    b6   -5.026      2.195 -2.290    0.022 -9.33 -0.725 5.5\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nTest of equality between two predictions (row 2 vs row 3):\n\nf <- function(x) predict(x, newdata = mtcars)\nhypotheses(mod, FUN = f, hypothesis = \"b2 = b3\")\n#> \n#>     Term Estimate Std. Error     z Pr(>|z|) 2.5 % 97.5 %   S\n#>  b2 = b3    -1.33      0.616 -2.16   0.0305 -2.54 -0.125 5.0\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nNote that we specified the newdata argument in the f function. This is because the predict() method associated with lm objects will automatically the original fitted values when newdata is NULL, instead of returning the slightly altered fitted values which we need to compute numerical derivatives in the delta method.\nWe can also use numeric vectors to specify linear combinations of parameters. For example, there are 3 coefficients in the last model we estimated. To test the null hypothesis that the sum of the 2nd and 3rd coefficients is equal to 0, we can do:\n\nhypotheses(mod, hypothesis = c(0, 1, 1))\n#> \n#>    Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %   S\n#>  custom     1.31      0.593 2.22   0.0266 0.153   2.48 5.2\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\nSee below for more example of how to use string formulas, numeric vectors, or matrices to calculate custom contrasts, linear combinations, and linear or non-linear hypothesis tests."
  },
  {
    "objectID": "articles/hypothesis.html#hypotheses-formulas",
    "href": "articles/hypothesis.html#hypotheses-formulas",
    "title": "\n7  Hypothesis Tests\n",
    "section": "\n7.3 hypotheses() Formulas",
    "text": "7.3 hypotheses() Formulas\nEach of the 4 core functions of the package support a hypothesis argument which behaves similarly to the hypotheses() function. This argument allows users to specify custom hypothesis tests and contrasts, in order to test null hypotheses such as:\n\nThe coefficients \\(\\beta_1\\) and \\(\\beta_2\\) are equal.\nThe marginal effects of \\(X_1\\) and \\(X_2\\) equal.\nThe marginal effect of \\(X\\) when \\(W=0\\) is equal to the marginal effect of \\(X\\) when \\(W=1\\).\nA non-linear function of adjusted predictions is equal to 100.\nThe marginal mean in the control group is equal to the average of marginal means in the other 3 treatment arms.\nCross-level contrasts: In a multinomial model, the effect of \\(X\\) on the 1st outcome level is equal to the effect of \\(X\\) on the 2nd outcome level.\n\n\n7.3.1 Marginal effects\nFor example, let’s fit a model and compute some marginal effects at the mean:\n\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ am + vs, data = mtcars)\n\nmfx <- slopes(mod, newdata = \"mean\")\nmfx\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>    am    1 - 0     6.07       1.27 4.76   <0.001 19.0  3.57   8.57\n#>    vs    1 - 0     6.93       1.26 5.49   <0.001 24.6  4.46   9.40\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, am, vs\n\nIs the marginal effect of am different from the marginal effect of vs? To answer this question we can run a linear hypothesis test using the hypotheses() function:\n\nhypotheses(mfx, hypothesis = \"am = vs\")\n#> \n#>   Term Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nAlternatively, we can specify the hypothesis directly in the original call:\n\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ am + vs, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"am = vs\")\n#> \n#>   Term Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThe hypotheses() string can include any valid R expression, so we can run some silly non-linear tests:\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"exp(am) - 2 * vs = -400\")\n#> \n#>               Term Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %\n#>  exp(am)-2*vs=-400      817        550 1.49    0.137 2.9  -261   1896\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n7.3.2 Adjusted Predictions\nNow consider the case of adjusted predictions:\n\np <- predictions(\n    mod,\n    newdata = datagrid(am = 0:1, vs = 0:1))\np\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 % am vs\n#>      14.6      0.926 15.8   <0.001 183.4  12.8   16.4  0  0\n#>      21.5      1.130 19.0   <0.001 266.3  19.3   23.7  0  1\n#>      20.7      1.183 17.5   <0.001 224.5  18.3   23.0  1  0\n#>      27.6      1.130 24.4   <0.001 435.0  25.4   29.8  1  1\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, am, vs\n\nSince there is no term column in the output of the predictions() function, we must use parameter identifiers like b1, b2, etc. to determine which estimates we want to compare:\n\nhypotheses(p, hypothesis = \"b1 = b2\")\n#> \n#>   Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  b1=b2    -6.93       1.26 -5.49   <0.001 24.6  -9.4  -4.46\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nOr directly:\n\npredictions(\n    mod,\n    hypothesis = \"b1 = b2\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#> \n#>   Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  b1=b2    -6.93       1.26 -5.49   <0.001 24.6  -9.4  -4.46\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\np$estimate[1] - p$estimate[2]\n#> [1] -6.929365\n\nIn the next section, we will see that we can get equivalent results by using a vector of contrast weights, which will be used to compute a linear combination of estimates:\n\npredictions(\n    mod,\n    hypothesis = c(1, -1, 0, 0),\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#> \n#>    Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n#>  custom    -6.93       1.26 -5.49   <0.001 24.6  -9.4  -4.46\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThere are many more possibilities:\n\npredictions(\n    mod,\n    hypothesis = \"b1 + b2 = 30\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#> \n#>      Term Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>  b1+b2=30     6.12       1.64 3.74   <0.001 12.4  2.91   9.32\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\np$estimate[1] + p$estimate[2] - 30\n#> [1] 6.118254\n\npredictions(\n    mod,\n    hypothesis = \"(b2 - b1) / (b3 - b2) = 0\",\n    newdata = datagrid(am = 0:1, vs = 0:1))\n#> \n#>               Term Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  (b2-b1)/(b3-b2)=0    -8.03         17 -0.473    0.636 0.7 -41.3   25.2\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n7.3.3 Average contrasts or marginal effects\nThe standard workflow with the marginaleffects package is to call a function like predictions(), slopes() or comparisons() to compute unit-level quantities; or one of their cousins avg_predictions(), avg_comparisons(), or avg_slopes() to aggregate the unit-level quantities into “Average Marginal Effects” or “Average Contrasts.” We can also use the comparison argument to emulate the behavior of the avg_*() functions.\nFirst, note that these three commands produce the same results:\n\ncomparisons(mod, variables = \"vs\")$estimate |> mean()\n#> [1] 6.929365\n\navg_comparisons(mod, variables = \"vs\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>    vs    1 - 0     6.93       1.26 5.49   <0.001 24.6  4.46    9.4\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\ncomparisons(\n    mod,\n    variables = \"vs\",\n    comparison = \"differenceavg\")\n#> \n#>  Term          Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>    vs mean(1) - mean(0)     6.93       1.26 5.49   <0.001 24.6  4.46    9.4\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nSee the transformations section of the Contrasts vignette for more details.\nWith these results in hand, we can now conduct a linear hypothesis test between average marginal effects:\n\ncomparisons(\n    mod,\n    hypothesis = \"am = vs\",\n    comparison = \"differenceavg\")\n#> \n#>   Term Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nComputing contrasts between average marginal effects requires a little care to obtain the right scale. In particular, we need to specify both the variables and the comparison:\n\ncomparisons(\n    mod,\n    hypothesis = \"am = vs\",\n    variables = c(\"am\", \"vs\"),\n    comparison = \"dydxavg\")\n#> \n#>   Term Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  am=vs   -0.863       1.94 -0.445    0.656 0.6 -4.66   2.94\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/hypothesis.html#hypotheses-vectors-and-matrices",
    "href": "articles/hypothesis.html#hypotheses-vectors-and-matrices",
    "title": "\n7  Hypothesis Tests\n",
    "section": "\n7.4 hypotheses() Vectors and Matrices",
    "text": "7.4 hypotheses() Vectors and Matrices\nThe marginal_means() function computes estimated marginal means. The hypothesis argument of that function offers a powerful mechanism to estimate custom contrasts between marginal means, by way of linear combination.\nConsider a simple example:\n\nlibrary(marginaleffects)\nlibrary(emmeans)\nlibrary(nnet)\n\ndat <- mtcars\ndat$carb <- factor(dat$carb)\ndat$cyl <- factor(dat$cyl)\ndat$am <- as.logical(dat$am)\n\nmod <- lm(mpg ~ carb + cyl, dat)\nmm <- marginal_means(mod, variables = \"carb\")\nmm\n#> \n#>  Term Value Mean Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n#>  carb     4 18.9       1.21 15.59   <0.001 179.7  16.5   21.3\n#>  carb     1 21.7       1.44 15.06   <0.001 167.8  18.8   24.5\n#>  carb     2 21.3       1.23 17.29   <0.001 220.0  18.9   23.8\n#>  carb     3 21.4       2.19  9.77   <0.001  72.5  17.1   25.7\n#>  carb     6 19.8       3.55  5.56   <0.001  25.2  12.8   26.7\n#>  carb     8 20.1       3.51  5.73   <0.001  26.6  13.2   27.0\n#> \n#> Results averaged over levels of: cyl, carb \n#> Columns: term, value, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThe contrast between marginal means for carb==1 and carb==2 is:\n\n21.66232 - 21.34058 \n#> [1] 0.32174\n\nor\n\n21.66232 + -(21.34058)\n#> [1] 0.32174\n\nor\n\nsum(c(21.66232, 21.34058) * c(1, -1))\n#> [1] 0.32174\n\nor\n\nc(21.66232, 21.34058) %*% c(1, -1)\n#>         [,1]\n#> [1,] 0.32174\n\nThe last two commands express the contrast of interest as a linear combination of marginal means.\n\n7.4.1 Simple contrast\nIn the marginal_means() function, we can supply a hypothesis argument to compute linear combinations of marginal means. This argument must be a numeric vector of the same length as the number of rows in the output of marginal_means(). For example, in the previous there were six rows, and the two marginal means we want to compare are at in the first two positions:\n\nlc <- c(1, -1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#> \n#>    Term  Mean Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  custom -2.78       2.08 -1.34    0.181 2.5 -6.85    1.3\n#> \n#> Results averaged over levels of: cyl, carb \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n7.4.2 Complex contrast\nOf course, we can also estimate more complex contrasts:\n\nlc <- c(-2, 1, 1, 0, -1, 1)\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#> \n#>    Term Mean Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  custom 5.59        6.2 0.901    0.367 1.4 -6.57   17.7\n#> \n#> Results averaged over levels of: cyl, carb \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nemmeans produces similar results:\n\nlibrary(emmeans)\nem <- emmeans(mod, \"carb\")\nlc <- data.frame(custom_contrast = c(-2, 1, 1, 0, -1, 1))\ncontrast(em, method = lc)\n#>  contrast        estimate   SE df t.ratio p.value\n#>  custom_contrast   -0.211 6.93 24  -0.030  0.9760\n#> \n#> Results are averaged over the levels of: cyl\n\n\n7.4.3 Multiple contrasts\nUsers can also compute multiple linear combinations simultaneously by supplying a numeric matrix to hypotheses. This matrix must have the same number of rows as the output of slopes(), and each column represents a distinct set of weights for different linear combinations. The column names of the matrix become labels in the output. For example:\n\nlc <- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    1, -1, 0, 0, 0, 0\n    ), ncol = 2)\ncolnames(lc) <- c(\"Contrast A\", \"Contrast B\")\nlc\n#>      Contrast A Contrast B\n#> [1,]         -2          1\n#> [2,]          1         -1\n#> [3,]          1          0\n#> [4,]          0          0\n#> [5,]         -1          0\n#> [6,]          1          0\n\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n#> \n#>        Term  Mean Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n#>  Contrast A  5.59       6.20  0.901    0.367 1.4 -6.57   17.7\n#>  Contrast B -2.78       2.08 -1.337    0.181 2.5 -6.85    1.3\n#> \n#> Results averaged over levels of: cyl, carb \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\n\n7.4.4 Contrasts across response levels\nIn models with multinomial outcomes, one may be interested in comparing outcomes or contrasts across response levels. For example, in this model there are 18 estimated marginal means, across 6 outcome levels (the group column):\n\nlibrary(nnet)\nmod <- multinom(carb ~ mpg + cyl, data = dat, trace = FALSE)\nmm <- marginal_means(mod, type = \"probs\")\nmm\n#> \n#>  Group Term Value     Mean Std. Error       z Pr(>|z|)   S     2.5 %   97.5 %\n#>      1  cyl     6 2.83e-01   1.96e-01 1.44491   0.1485 2.8 -1.01e-01 6.67e-01\n#>      1  cyl     4 3.68e-01   2.60e-01 1.41643   0.1566 2.7 -1.41e-01 8.77e-01\n#>      1  cyl     8 4.63e-04   9.59e-03 0.04824   0.9615 0.1 -1.83e-02 1.93e-02\n#>      2  cyl     6 1.85e-06   2.40e-06 0.77087   0.4408 1.2 -2.85e-06 6.55e-06\n#>      2  cyl     4 6.31e-01   2.60e-01 2.42765   0.0152 6.0  1.22e-01 1.14e+00\n#>      2  cyl     8 6.65e-01   3.74e-01 1.77961   0.0751 3.7 -6.74e-02 1.40e+00\n#>      3  cyl     6 1.12e-05   1.32e-03 0.00848   0.9932 0.0 -2.58e-03 2.60e-03\n#>      3  cyl     4 6.85e-04   1.19e-02 0.05748   0.9542 0.1 -2.27e-02 2.40e-02\n#>      3  cyl     8 3.10e-01   3.71e-01 0.83676   0.4027 1.3 -4.17e-01 1.04e+00\n#>      4  cyl     6 5.56e-01   2.18e-01 2.55000   0.0108 6.5  1.29e-01 9.84e-01\n#>      4  cyl     4 2.12e-04   1.75e-02 0.01211   0.9903 0.0 -3.41e-02 3.45e-02\n#>      4  cyl     8 9.58e-03   2.28e-02 0.41999   0.6745 0.6 -3.51e-02 5.43e-02\n#>      6  cyl     6 1.61e-01   1.54e-01 1.04685   0.2952 1.8 -1.40e-01 4.62e-01\n#>      6  cyl     4 8.82e-06   8.39e-05 0.10505   0.9163 0.1 -1.56e-04 1.73e-04\n#>      6  cyl     8 4.35e-09   9.89e-08 0.04393   0.9650 0.1 -1.90e-07 1.98e-07\n#>      8  cyl     6 9.29e-06   7.98e-04 0.01164   0.9907 0.0 -1.56e-03 1.57e-03\n#>      8  cyl     4 1.50e-04   7.97e-03 0.01878   0.9850 0.0 -1.55e-02 1.58e-02\n#>      8  cyl     8 1.41e-02   4.66e-02 0.30317   0.7618 0.4 -7.72e-02 1.05e-01\n#> \n#> Results averaged over levels of: mpg, cyl \n#> Columns: group, term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nLet’s contrast the marginal means in the first outcome level when cyl equals 4 and 6. These marginal means are located in rows 1 and 7 respectively:\n\nlc <- rep(0, nrow(mm))\nlc[1] <- -1\nlc[7] <- 1\nmarginal_means(\n    mod,\n    type = \"probs\",\n    hypothesis = lc)\n#> \n#>    Term   Mean Std. Error     z Pr(>|z|)   S  2.5 % 97.5 %\n#>  custom -0.283      0.196 -1.44    0.149 2.8 -0.667  0.101\n#> \n#> Results averaged over levels of: mpg, cyl \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThis is indeed equal to the results we would have obtained manually:\n\n2.828726e-01 - 3.678521e-01\n#> [1] -0.0849795\n\nNow let’s say we want to calculate a “contrast in contrasts”, that is, the outcome of a 3-step process:\n\nContrast between cyl=6 and cyl=4 in the 1st outcome level\nContrast between cyl=6 and cyl=4 in the 2nd outcome level\nContrast between the contrasts defined in steps 1 and 2.\n\nWe create the linear combination weights as follows:\n\nlc <- rep(0, nrow(mm))\nlc[c(1, 8)] <- -1\nlc[c(7, 2)] <- 1\n\nTo make sure that the weights are correct, we can display them side by side with the original marginal_means() output:\n\ntransform(mm[, 1:3], lc = lc)\n#>    group term value lc\n#> 1      1  cyl     6 -1\n#> 2      1  cyl     4  1\n#> 3      1  cyl     8  0\n#> 4      2  cyl     6  0\n#> 5      2  cyl     4  0\n#> 6      2  cyl     8  0\n#> 7      3  cyl     6  1\n#> 8      3  cyl     4 -1\n#> 9      3  cyl     8  0\n#> 10     4  cyl     6  0\n#> 11     4  cyl     4  0\n#> 12     4  cyl     8  0\n#> 13     6  cyl     6  0\n#> 14     6  cyl     4  0\n#> 15     6  cyl     8  0\n#> 16     8  cyl     6  0\n#> 17     8  cyl     4  0\n#> 18     8  cyl     8  0\n\nCompute the results:\n\nmarginal_means(mod, type = \"probs\", hypothesis = lc)\n#> \n#>    Term   Mean Std. Error     z Pr(>|z|)   S  2.5 % 97.5 %\n#>  custom 0.0843      0.321 0.263    0.793 0.3 -0.545  0.714\n#> \n#> Results averaged over levels of: mpg, cyl \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/hypothesis.html#pairwise-contrasts-difference-in-differences",
    "href": "articles/hypothesis.html#pairwise-contrasts-difference-in-differences",
    "title": "\n7  Hypothesis Tests\n",
    "section": "\n7.5 Pairwise contrasts: Difference-in-Differences",
    "text": "7.5 Pairwise contrasts: Difference-in-Differences\nNow we illustrate how to use the machinery described above to do pairwise comparisons between contrasts, a type of analysis often associated with a “Difference-in-Differences” research design.\nFirst, we simulate data with two treatment groups and pre/post periods:\n\nlibrary(data.table)\n\nN <- 1000\ndid <- data.table(\n    id = 1:N,\n    pre = rnorm(N),\n    trt = sample(0:1, N, replace = TRUE))\ndid$post <- did$pre + did$trt * 0.3 + rnorm(N)\ndid <- melt(\n    did,\n    value.name = \"y\",\n    variable.name = \"time\",\n    id.vars = c(\"id\", \"trt\"))\nhead(did)\n#>    id trt time          y\n#> 1:  1   0  pre -0.2683173\n#> 2:  2   0  pre  0.1689210\n#> 3:  3   0  pre -1.1081249\n#> 4:  4   0  pre -0.7424166\n#> 5:  5   1  pre -0.6454390\n#> 6:  6   0  pre -0.3447009\n\nThen, we estimate a linear model with a multiple interaction between the time and the treatment indicators. We also compute contrasts at the mean for each treatment level:\n\ndid_model <- lm(y ~ time * trt, data = did)\n\ncomparisons(\n    did_model,\n    newdata = datagrid(trt = 0:1),\n    variables = \"time\")\n#> \n#>  Term   Contrast Estimate Std. Error      z Pr(>|z|)   S   2.5 % 97.5 % trt\n#>  time post - pre -0.00921     0.0798 -0.116  0.90804 0.1 -0.1655  0.147   0\n#>  time post - pre  0.22034     0.0804  2.741  0.00613 7.4  0.0628  0.378   1\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, y, time, trt\n\nFinally, we compute pairwise differences between contrasts. This is the Diff-in-Diff estimate:\n\ncomparisons(\n    did_model,\n    variables = \"time\",\n    newdata = datagrid(trt = 0:1),\n    hypothesis = \"pairwise\")\n#> \n#>           Term Estimate Std. Error     z Pr(>|z|)   S  2.5 %   97.5 %\n#>  Row 1 - Row 2    -0.23      0.113 -2.03   0.0426 4.6 -0.451 -0.00761\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/hypothesis.html#joint-hyotheses-tests",
    "href": "articles/hypothesis.html#joint-hyotheses-tests",
    "title": "\n7  Hypothesis Tests\n",
    "section": "\n7.6 Joint hyotheses tests",
    "text": "7.6 Joint hyotheses tests\nThe hypotheses() function can also test multiple hypotheses jointly. For example, consider this model:\n\nmodel <- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\ncoef(model)\n#>        (Intercept)    as.factor(cyl)6    as.factor(cyl)8                 hp as.factor(cyl)6:hp as.factor(cyl)8:hp \n#>        35.98302564       -15.30917451       -17.90295193        -0.11277589         0.10516262         0.09853177\n\nWe may want to test the null hypothesis that two of the coefficients are jointly (both) equal to zero.\n\n\nhypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n#> \n#> \n#> Joint hypothesis test:\n#> as.factor(cyl)6:hp = 0\n#> as.factor(cyl)8:hp = 0\n#>  \n#>     F Pr(>|F|) Df 1 Df 2\n#>  2.11    0.142    2   26\n#> \n#> Columns: statistic, p.value, df1, df2\n\nThe joint argument allows users to flexibly specify the parameters to be tested, using character vectors, integer indices, or Perl-compatible regular expressions. We can also specificy the null hypothesis for each parameter individually using the hypothesis argument.\nNaturally, the hypotheses() function also works with marginaleffects objects.\n\n# ## joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n#> \n#> \n#> Joint hypothesis test:\n#>  as.factor(cyl)6 = 0\n#>  as.factor(cyl)8 = 0\n#>  as.factor(cyl)6:hp = 0\n#>  as.factor(cyl)8:hp = 0\n#>  \n#>    F Pr(>|F|) Df 1 Df 2\n#>  5.7  0.00197    4   26\n#> \n#> Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n#> \n#> \n#> Joint hypothesis test:\n#>  as.factor(cyl)6 = 0\n#>  as.factor(cyl)8 = 0\n#>  \n#>     F Pr(>|F|) Df 1 Df 2\n#>  6.12  0.00665    2   26\n#> \n#> Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n#> \n#> \n#> Joint hypothesis test:\n#>  as.factor(cyl)6 = 1\n#>  as.factor(cyl)8 = 1\n#>  \n#>     F Pr(>|F|) Df 1 Df 2\n#>  6.84  0.00411    2   26\n#> \n#> Columns: statistic, p.value, df1, df2\nhypotheses(model, joint = 2:3, hypothesis = 1:2)\n#> \n#> \n#> Joint hypothesis test:\n#>  as.factor(cyl)6 = 1\n#>  as.factor(cyl)8 = 2\n#>  \n#>     F Pr(>|F|) Df 1 Df 2\n#>  7.47  0.00273    2   26\n#> \n#> Columns: statistic, p.value, df1, df2\n\n## joint hypotheses: marginaleffects object\ncmp <- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n#> \n#> \n#> Joint hypothesis test:\n#>  cyl 6 - 4 = 0\n#>  cyl 8 - 4 = 0\n#>  \n#>    F Pr(>|F|) Df 1 Df 2\n#>  1.6    0.219    2   29\n#> \n#> Columns: statistic, p.value, df1, df2\n\nWe can also combine multiple calls to hypotheses() to execute a joint test on linear combinations of coefficients:\n\n## fit model\nmod <- lm(mpg ~ factor(carb), mtcars)\n\n## hypothesis matrix for linear combinations\nH <- matrix(0, nrow = length(coef(mod)), ncol = 2)\nH[2:3, 1] <- H[4:6, 2] <- 1\n\n## test individual linear combinations\nhyp <- hypotheses(mod, hypothesis = H)\nhyp\n#> \n#>    Term Estimate Std. Error     z Pr(>|z|) 2.5 % 97.5 %   S\n#>  custom    -12.0       4.92 -2.44  0.01477 -21.6  -2.35 6.1\n#>  custom    -25.5       9.03 -2.83  0.00466 -43.2  -7.85 7.7\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value\n\n## test joint hypotheses\n#hypotheses(hyp, joint = TRUE, hypothesis = c(-10, -20))"
  },
  {
    "objectID": "articles/hypothesis.html#equivalence-non-inferiority-and-non-superiority-tests",
    "href": "articles/hypothesis.html#equivalence-non-inferiority-and-non-superiority-tests",
    "title": "\n7  Hypothesis Tests\n",
    "section": "\n7.7 Equivalence, non-inferiority, and non-superiority tests",
    "text": "7.7 Equivalence, non-inferiority, and non-superiority tests\nIn many contexts, analysts are less interested in rejecting a null hypothesis, and more interested in testing whether an estimate is “inferior”, “superior”, or “equivalent” to a given threshold or interval. For example, medical researchers may wish to determine if the estimated effect of a new treatment is larger than the effect of prior treatments, or larger than some threshold of “clinical significance.” Alternatively, researchers may wish to support a claim that an estimated parameter is “equivalent to” or “not meaningfully different from” a null hypothesis.\nTo answer these questions, we can use non-inferiority, non-superiority, or equivalence tests like the two-one-sided test (TOST). This article gives a primer and tutorial on TOST:\n\nLakens D, Scheel AM, Isager PM. Equivalence Testing for Psychological Research: A Tutorial. Advances in Methods and Practices in Psychological Science. 2018;1(2):259-269. doi:10.1177/2515245918770963\n\nThe hypotheses() function of the marginaleffects package includes an equivalence argument which allows users to apply these tests to any of the quantities generated by the package, as well as to arbitrary functions of a model’s parameters. To illustrate, we begin by estimating a simple linear regression model:\n\nmod <- lm(mpg ~ hp + factor(gear), data = mtcars)\n\nThe rest of this section considers several quantities estimated by marginaleffects.\n\n7.7.0.0.1 Example 1: Predictions\nConsider a single prediction, where all predictors are held at their median or mode:\n\np <- predictions(mod, newdata = \"median\")\np\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp gear\n#>      19.7          1 19.6   <0.001 281.3  17.7   21.6 123    3\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear\n\nNow we specify an equivalence interval (or “region”) for predictions between 17 and 18:\n\nhypotheses(p, equivalence = c(17, 18))\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp gear p (NonInf) p (NonSup) p (Equiv)\n#>      19.7          1 19.6   <0.001 281.3  17.7   21.6 123    3    0.00404      0.951     0.951\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n\nThe results allow us to draw three conclusions:\n\nThe p value for the non-inferiority test is 0.0040. This suggests that we can reject the null hypothesis that the parameter is below 17.\nThe p value for the non-superiority test is 0.9508. This suggests that we cannot reject the null hypothesis that the parameter (19.6589) is above 18.\nThe p value for the equivalence test is 0.9508. This suggests that we cannot reject the hypothesis that the parameter falls outside the equivalence interval.\n\n7.7.0.0.2 Example 2: Model coefficients\nThe hypotheses() function also allows users to conduct equivalence, non-inferiority, and non-superiority tests for model coefficients, and for arbitrary functions of model coefficients.\nOur estimate of the 4th coefficient in the model is:\n\ncoef(mod)[4]\n#> factor(gear)5 \n#>      6.574763\n\nWe can test if this parameter is likely to fall in the [5,7] interval by:\n\nhypotheses(mod, equivalence = c(5, 7))[4, ]\n#> \n#>  Term Estimate Std. Error z Pr(>|z|) 2.5 % 97.5 %    S p (NonInf) p (NonSup) p (Equiv)\n#>    b4     6.57       1.64 4   <0.001  3.36   9.79 14.0      0.169      0.398     0.398\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n\nThe p value is 0.3979, so we cannot reject the hypothesis that the factor(gear)5 parameter falls outside the [5,7] interval.\n\n7.7.0.0.3 Example 3: Slopes\nThe same syntax can be used to conduct tests for all the quantities produced by the marginaleffects package. For example, imagine that, for substantive or theoretical reasons, an average slope between -0.1 and 0.1 is uninteresting. We can conduct an equivalence test to check if this is the case:\n\navg_slopes(mod, variables = \"hp\", equivalence = c(-.1, .1))\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 % p (NonInf) p (NonSup) p (Equiv)\n#>    hp  -0.0669      0.011 -6.05   <0.001 29.4 -0.0885 -0.0452    0.00135     <0.001   0.00135\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n\nThe p value is 0.0013, which suggests that we can reject the hypothesis that the parameter falls outside the region of “substantive equivalence” that we have defined by the interval.\n\n7.7.0.0.4 Example 4: Difference between comparisons (contrasts)\nConsider a model with a multiplicative interaction:\n\nint <- lm(mpg ~ hp * factor(gear), data = mtcars)\n\nThe average contrast for a change of 1 unit in hp differs based on the value of gear:\n\navg_comparisons(int, variables = \"hp\", by = \"gear\")\n#> \n#>  Term Contrast gear Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %\n#>    hp mean(+1)    4  -0.1792     0.0303 -5.92   <0.001 28.2 -0.2385 -0.1199\n#>    hp mean(+1)    3  -0.0522     0.0146 -3.59   <0.001 11.6 -0.0808 -0.0237\n#>    hp mean(+1)    5  -0.0583     0.0126 -4.61   <0.001 17.9 -0.0830 -0.0335\n#> \n#> Columns: term, contrast, gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nAre these contrasts different from one another? Let’s look at the pairwise differences between them:\n\navg_comparisons(int, variables = \"hp\", by = \"gear\",\n    hypothesis = \"pairwise\")\n#> \n#>   Term Estimate Std. Error      z Pr(>|z|)    S   2.5 %  97.5 %\n#>  4 - 3 -0.12695     0.0336 -3.781   <0.001 12.6 -0.1928 -0.0611\n#>  4 - 5 -0.12092     0.0328 -3.688   <0.001 12.1 -0.1852 -0.0567\n#>  3 - 5  0.00603     0.0193  0.313    0.754  0.4 -0.0318  0.0438\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe consider that these pairwise comparisons are “equivalent to zero” when they fall in the [-.1, .1] interval:\n\navg_comparisons(int, variables = \"hp\", by = \"gear\",\n    hypothesis = \"pairwise\",\n    equivalence = c(-.1, .1))\n#> \n#>   Term Estimate Std. Error      z Pr(>|z|)    S   2.5 %  97.5 % p (NonInf) p (NonSup) p (Equiv)\n#>  4 - 3 -0.12695     0.0336 -3.781   <0.001 12.6 -0.1928 -0.0611      0.789     <0.001     0.789\n#>  4 - 5 -0.12092     0.0328 -3.688   <0.001 12.1 -0.1852 -0.0567      0.738     <0.001     0.738\n#>  3 - 5  0.00603     0.0193  0.313    0.754  0.4 -0.0318  0.0438     <0.001     <0.001    <0.001\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv\n\nThe p (Equiv) column shows that the difference between the average contrasts when gear is 3 and gear is 5 can be said to be equivalent to the specified interval. However, there are good reasons to think that the other two pairwise comparisons may fall outside the interval.\n\n7.7.0.0.5 Example 5: Marginal means and emmeans\n\nThis example shows the equivalence between results produced by the emmeans package and the marginal_means() function:\n\nlibrary(emmeans)\n\nmod <- lm(log(conc) ~ source + factor(percent), data = pigs)\n\n## {emmeans}\nemmeans(mod, specs = \"source\") |>\n    pairs() |>\n    test(df = Inf,\n         null = 0,\n         delta = log(1.25),\n         side = \"equivalence\",\n         adjust = \"none\")\n#>  contrast    estimate     SE  df z.ratio p.value\n#>  fish - soy    -0.273 0.0529 Inf   0.937  0.8257\n#>  fish - skim   -0.402 0.0542 Inf   3.308  0.9995\n#>  soy - skim    -0.130 0.0530 Inf  -1.765  0.0388\n#> \n#> Results are averaged over the levels of: percent \n#> Degrees-of-freedom method: user-specified \n#> Results are given on the log (not the response) scale. \n#> Statistics are tests of equivalence with a threshold of 0.22314 \n#> P values are left-tailed\n\n## {marginaleffects}\nmarginal_means(\n    mod,\n    variables = \"source\",\n    hypothesis = \"pairwise\",\n    equivalence = c(-log(1.25), log(1.25)))\n#> \n#>         Term   Mean Std. Error     z Pr(>|z|)    S  2.5 %  97.5 % p (Equiv) p (NonInf) p (NonSup)\n#>  fish - soy  -0.273     0.0529 -5.15   <0.001 21.9 -0.377 -0.1690    0.8257     0.8257     <0.001\n#>  fish - skim -0.402     0.0542 -7.43   <0.001 43.0 -0.508 -0.2961    0.9995     0.9995     <0.001\n#>  soy - skim  -0.130     0.0530 -2.44   0.0146  6.1 -0.233 -0.0255    0.0388     0.0388     <0.001\n#> \n#> Results averaged over levels of: percent, source \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, p.value.equiv, p.value.noninf, p.value.nonsup, statistic.noninf, statistic.nonsup\n\n\n7.7.0.0.6 Example 6: t-test\nNow we show that the results produced by hypotheses() are identical to the results produced by the equivalence package in the case of a simple t-test:\n\nlibrary(equivalence)\n\nset.seed(1024)\n\n## simulate data data\nN <- 20\ndat <- data.frame(\n    y = rnorm(N),\n    x = sample(c(rep(0, N / 2), rep(1, N / 2)), N))\n\n## fit model\nmod <- lm(y ~ x, data = dat)\n\n## test with the {equivalence} package\ne <- tost(\n    x = dat$y[dat$x == 0],\n    y = dat$y[dat$x == 1],\n    epsilon = 10)\ne\n#> \n#>  Welch Two Sample TOST\n#> \n#> data:  dat$y[dat$x == 0] and dat$y[dat$x == 1]\n#> df = 17.607\n#> sample estimates:\n#>  mean of x  mean of y \n#> -0.3788551 -0.2724594 \n#> \n#> Epsilon: 10 \n#> 95 percent two one-sided confidence interval (TOST interval):\n#>  -1.058539  0.845747\n#> Null hypothesis of statistical difference is: rejected \n#> TOST p-value: 4.248528e-13\n\n## test with {marginaleffects} package\nhypotheses(mod, equivalence = c(-10, 10), df = e$parameter)[2, ]\n#> \n#>  Term Estimate Std. Error     t Pr(>|t|) 2.5 % 97.5 %   Df   S p (NonInf) p (NonSup) p (Equiv)\n#>    b2    0.106      0.548 0.194    0.848 -1.05   1.26 17.6 0.2     <0.001     <0.001    <0.001\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, df, s.value, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv"
  },
  {
    "objectID": "articles/brms.html#logistic-regression-with-multiplicative-interactions",
    "href": "articles/brms.html#logistic-regression-with-multiplicative-interactions",
    "title": "\n8  Bayes\n",
    "section": "\n8.1 Logistic regression with multiplicative interactions",
    "text": "8.1 Logistic regression with multiplicative interactions\nLoad libraries and download data on passengers of the Titanic from the Rdatasets archive:\n\nlibrary(marginaleffects)\nlibrary(brms)\nlibrary(ggplot2)\nlibrary(ggdist)\n\ndat <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/carData/TitanicSurvival.csv\")\ndat$survived <- ifelse(dat$survived == \"yes\", 1, 0)\ndat$woman <- ifelse(dat$sex == \"female\", 1, 0)\n\nFit a logit model with a multiplicative interaction:\n\nmod <- brm(survived ~ woman * age + passengerClass,\n           family = bernoulli(link = \"logit\"),\n           data = dat)\n\n\n8.1.1 Adjusted predictions\nWe can compute adjusted predicted values of the outcome variable (i.e., probability of survival aboard the Titanic) using the predictions() function. By default, this function calculates predictions for each row of the dataset:\n\npredictions(mod)\n#> \n#>  Estimate  2.5 % 97.5 %\n#>    0.9367 0.9070 0.9590\n#>    0.8493 0.7453 0.9187\n#>    0.9433 0.8949 0.9704\n#>    0.5131 0.4302 0.6000\n#>    0.9375 0.9080 0.9601\n#> --- 1036 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#>    0.0376 0.0235 0.0581\n#>    0.5859 0.5017 0.6663\n#>    0.1043 0.0801 0.1337\n#>    0.1017 0.0779 0.1307\n#>    0.0916 0.0691 0.1189\n#> Columns: rowid, estimate, conf.low, conf.high, survived, woman, age, passengerClass\n\nTo visualize the relationship between the outcome and one of the regressors, we can plot conditional adjusted predictions with the plot_predictions() function:\n\nplot_predictions(mod, condition = \"age\")\n\n\n\n\nCompute adjusted predictions for some user-specified values of the regressors, using the newdata argument and the datagrid() function:\n\npred <- predictions(mod,\n                    newdata = datagrid(woman = 0:1,\n                                       passengerClass = c(\"1st\", \"2nd\", \"3rd\")))\npred\n#> \n#>  Estimate  2.5 % 97.5 %  age woman passengerClass\n#>    0.5149 0.4319  0.602 29.9     0            1st\n#>    0.2013 0.1536  0.261 29.9     0            2nd\n#>    0.0875 0.0656  0.114 29.9     0            3rd\n#>    0.9364 0.9066  0.959 29.9     1            1st\n#>    0.7783 0.7090  0.835 29.9     1            2nd\n#>    0.5701 0.4938  0.644 29.9     1            3rd\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, survived, age, woman, passengerClass\n\nThe posterior_draws() function samples from the posterior distribution of the model, and produces a data frame with drawid and draw columns.\n\npred <- posterior_draws(pred)\nhead(pred)\n#>   drawid       draw rowid   estimate   conf.low conf.high  survived      age woman passengerClass\n#> 1      1 0.46566713     1 0.51492993 0.43192231 0.6018749 0.4082218 29.88113     0            1st\n#> 2      1 0.16658900     2 0.20128833 0.15362308 0.2613351 0.4082218 29.88113     0            2nd\n#> 3      1 0.08750961     3 0.08750369 0.06555724 0.1141134 0.4082218 29.88113     0            3rd\n#> 4      1 0.93735755     4 0.93641346 0.90660921 0.9587589 0.4082218 29.88113     1            1st\n#> 5      1 0.77437334     5 0.77829290 0.70896643 0.8346419 0.4082218 29.88113     1            2nd\n#> 6      1 0.62216334     6 0.57010265 0.49377997 0.6441967 0.4082218 29.88113     1            3rd\n\nThis “long” format makes it easy to plots results:\n\nggplot(pred, aes(x = draw, fill = factor(woman))) +\n    geom_density() +\n    facet_grid(~ passengerClass, labeller = label_both) +\n    labs(x = \"Predicted probability of survival\", y = \"\", fill = \"Woman\")\n\n\n\n\n\n8.1.2 Marginal effects\nUse slopes() to compute marginal effects (slopes of the regression equation) for each row of the dataset, and use ) to compute “Average Marginal Effects”, that is, the average of all observation-level marginal effects:\n\nmfx <- slopes(mod)\nmfx\n#> \n#>            Term  Contrast Estimate   2.5 % 97.5 %\n#>  woman          1 - 0       0.4076  0.3339  0.478\n#>  woman          1 - 0       0.0919  0.0301  0.179\n#>  woman          1 - 0       0.0997  0.0371  0.187\n#>  woman          1 - 0       0.4220  0.3487  0.492\n#>  woman          1 - 0       0.3517  0.2762  0.426\n#> --- 4174 rows omitted. See ?avg_slopes and ?print.marginaleffects --- \n#>  passengerClass 3rd - 1st  -0.2643 -0.3397 -0.199\n#>  passengerClass 3rd - 1st  -0.3527 -0.4263 -0.282\n#>  passengerClass 3rd - 1st  -0.4593 -0.5507 -0.370\n#>  passengerClass 3rd - 1st  -0.4550 -0.5463 -0.365\n#>  passengerClass 3rd - 1st  -0.4365 -0.5261 -0.350\n#> Columns: rowid, term, contrast, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, survived, woman, age, passengerClass\n\nCompute marginal effects with some regressors fixed at user-specified values, and other regressors held at their means:\n\nslopes(\n    mod,\n    newdata = datagrid(\n        woman = 1,\n        passengerClass = \"1st\"))\n#> \n#>            Term  Contrast  Estimate    2.5 %    97.5 %  age woman passengerClass\n#>  woman          1 - 0      0.420368  0.34697  0.490373 29.9     1            1st\n#>  age            dY/dX     -0.000238 -0.00136  0.000871 29.9     1            1st\n#>  passengerClass 2nd - 1st -0.157442 -0.22327 -0.102890 29.9     1            1st\n#>  passengerClass 3rd - 1st -0.365376 -0.43832 -0.294769 29.9     1            1st\n#> \n#> Columns: rowid, term, contrast, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, survived, age, woman, passengerClass\n\nCompute and plot conditional marginal effects:\n\nplot_slopes(mod, variables = \"woman\", condition = \"age\")\n\n\n\n\nThe posterior_draws() produces a dataset with drawid and draw columns:\n\ndraws <- posterior_draws(mfx)\n\ndim(draws)\n#> [1] 16736000       16\n\nhead(draws)\n#>   drawid       draw rowid  term contrast   estimate   conf.low conf.high predicted predicted_hi predicted_lo tmp_idx survived woman     age passengerClass\n#> 1      1 0.45768070     1 woman    1 - 0 0.40758806 0.33394178 0.4783255 0.9366604    0.9366604    0.5276344       1        1     1 29.0000            1st\n#> 2      1 0.09282294     2 woman    1 - 0 0.09190500 0.03006249 0.1788661 0.8493050    0.9436352    0.8493050       2        1     0  0.9167            1st\n#> 3      1 0.10179526     3 woman    1 - 0 0.09973464 0.03710637 0.1873029 0.9433293    0.9433293    0.8409878       3        0     1  2.0000            1st\n#> 4      1 0.47357659     4 woman    1 - 0 0.42198871 0.34867425 0.4921402 0.5131011    0.9363704    0.5131011       4        0     0 30.0000            1st\n#> 5      1 0.39400824     5 woman    1 - 0 0.35167800 0.27617966 0.4262480 0.9374937    0.9374937    0.5851849       5        0     1 25.0000            1st\n#> 6      1 0.72093817     6 woman    1 - 0 0.65665127 0.57622574 0.7297775 0.2730542    0.9320972    0.2730542       6        1     0 48.0000            1st\n\nWe can use this dataset to plot our results. For example, to plot the posterior density of the marginal effect of age when the woman variable is equal to 0 or 1:\n\nmfx <- slopes(mod,\n    variables = \"age\",\n    newdata = datagrid(woman = 0:1)) |>\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, fill = factor(woman))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal Effect of Age on Survival\",\n         y = \"Posterior density\",\n         fill = \"Woman\")"
  },
  {
    "objectID": "articles/brms.html#random-effects-model",
    "href": "articles/brms.html#random-effects-model",
    "title": "\n8  Bayes\n",
    "section": "\n8.2 Random effects model",
    "text": "8.2 Random effects model\nThis section replicates some of the analyses of a random effects model published in Andrew Heiss’ blog post: “A guide to correctly calculating posterior predictions and average marginal effects with multilevel Bayesian models.” The objective is mainly to illustrate the use of marginaleffects. Please refer to the original post for a detailed discussion of the quantities computed below.\nLoad libraries and download data:\n\nlibrary(brms)\nlibrary(ggdist)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\nvdem_2015 <- read.csv(\"https://github.com/vincentarelbundock/marginaleffects/raw/main/data-raw/vdem_2015.csv\")\n\nhead(vdem_2015)\n#>   country_name country_text_id year                           region media_index party_autonomy_ord polyarchy civil_liberties party_autonomy\n#> 1       Mexico             MEX 2015  Latin America and the Caribbean       0.837                  3     0.631           0.704           TRUE\n#> 2     Suriname             SUR 2015  Latin America and the Caribbean       0.883                  4     0.777           0.887           TRUE\n#> 3       Sweden             SWE 2015 Western Europe and North America       0.956                  4     0.915           0.968           TRUE\n#> 4  Switzerland             CHE 2015 Western Europe and North America       0.939                  4     0.901           0.960           TRUE\n#> 5        Ghana             GHA 2015               Sub-Saharan Africa       0.858                  4     0.724           0.921           TRUE\n#> 6 South Africa             ZAF 2015               Sub-Saharan Africa       0.898                  4     0.752           0.869           TRUE\n\nFit a basic model:\n\nmod <- brm(\n  bf(media_index ~ party_autonomy + civil_liberties + (1 | region),\n     phi ~ (1 | region)),\n  data = vdem_2015,\n  family = Beta(),\n  control = list(adapt_delta = 0.9))\n\n\n8.2.1 Posterior predictions\nTo compute posterior predictions for specific values of the regressors, we use the newdata argument and the datagrid() function. We also use the type argument to compute two types of predictions: accounting for residual (observation-level) residual variance (prediction) or ignoring it (response).\n\nnd = datagrid(model = mod,\n              party_autonomy = c(TRUE, FALSE),\n              civil_liberties = .5,\n              region = \"Middle East and North Africa\")\np1 <- predictions(mod, type = \"response\", newdata = nd) |>\n    posterior_draws() |>\n    transform(type = \"Response\")\np2 <- predictions(mod, type = \"prediction\", newdata = nd) |>\n    posterior_draws() |>\n    transform(type = \"Prediction\")\npred <- rbind(p1, p2)\n\nExtract posterior draws and plot them:\n\nggplot(pred, aes(x = draw, fill = party_autonomy)) +\n    stat_halfeye(alpha = .5) +\n    facet_wrap(~ type) +\n    labs(x = \"Media index (predicted)\", \n         y = \"Posterior density\",\n         fill = \"Party autonomy\")\n\n\n\n\n\n8.2.2 Marginal effects and contrasts\nAs noted in the Marginal Effects vignette, there should be one distinct marginal effect for each combination of regressor values. Here, we consider only one combination of regressor values, where region is “Middle East and North Africa”, and civil_liberties is 0.5. Then, we calculate the mean of the posterior distribution of marginal effects:\n\nmfx <- slopes(mod,\n                       newdata = datagrid(civil_liberties = .5,\n                                          region = \"Middle East and North Africa\"))\nmfx\n#> \n#>             Term     Contrast Estimate 2.5 % 97.5 % party_autonomy civil_liberties                       region\n#>  party_autonomy  TRUE - FALSE    0.252 0.166  0.336           TRUE             0.5 Middle East and North Africa\n#>  civil_liberties dY/dX           0.816 0.621  1.007           TRUE             0.5 Middle East and North Africa\n#> \n#> Columns: rowid, term, contrast, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, media_index, party_autonomy, civil_liberties, region\n\nUse the posterior_draws() to extract draws from the posterior distribution of marginal effects, and plot them:\n\nmfx <- posterior_draws(mfx)\n\nggplot(mfx, aes(x = draw, y = term)) +\n  stat_halfeye() +\n  labs(x = \"Marginal effect\", y = \"\")\n\n\n\n\nPlot marginal effects, conditional on a regressor:\n\nplot_slopes(mod,\n         variables = \"civil_liberties\",\n         condition = \"party_autonomy\")\n\n\n\n\n\n8.2.3 Continuous predictors\n\npred <- predictions(mod,\n                    newdata = datagrid(party_autonomy = FALSE,\n                                       region = \"Middle East and North Africa\",\n                                       civil_liberties = seq(0, 1, by = 0.05))) |>\n        posterior_draws()\n\nggplot(pred, aes(x = civil_liberties, y = draw)) +\n    stat_lineribbon() +\n    scale_fill_brewer(palette = \"Reds\") +\n    labs(x = \"Civil liberties\",\n         y = \"Media index (predicted)\",\n         fill = \"\")\n\n\n\n\nThe slope of this line for different values of civil liberties can be obtained with:\n\nmfx <- slopes(mod,\n    newdata = datagrid(\n        civil_liberties = c(.2, .5, .8),\n        party_autonomy = FALSE,\n        region = \"Middle East and North Africa\"),\n    variables = \"civil_liberties\")\nmfx\n#> \n#>             Term Estimate 2.5 % 97.5 % civil_liberties party_autonomy                       region\n#>  civil_liberties    0.490 0.361  0.639             0.2          FALSE Middle East and North Africa\n#>  civil_liberties    0.807 0.612  0.993             0.5          FALSE Middle East and North Africa\n#>  civil_liberties    0.807 0.674  0.934             0.8          FALSE Middle East and North Africa\n#> \n#> Columns: rowid, term, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, media_index, civil_liberties, party_autonomy, region\n\nAnd plotted:\n\nmfx <- posterior_draws(mfx)\n\nggplot(mfx, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal effect of Civil Liberties on Media Index\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n\n\n\n\nThe slopes() function can use the ellipsis (...) to push any argument forward to the posterior_predict function. This can alter the types of predictions returned. For example, the re_formula=NA argument of the posterior_predict.brmsfit method will compute marginaleffects without including any group-level effects:\n\nmfx <- slopes(\n    mod,\n    newdata = datagrid(\n        civil_liberties = c(.2, .5, .8),\n        party_autonomy = FALSE,\n        region = \"Middle East and North Africa\"),\n    variables = \"civil_liberties\",\n    re_formula = NA) |>\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Marginal effect of Civil Liberties on Media Index\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n\n\n\n\n\n8.2.4 Global grand mean\n\npred <- predictions(\n    mod,\n    re_formula = NA,\n    newdata = datagrid(party_autonomy = c(TRUE, FALSE))) |>\n    posterior_draws()\n\nmfx <- slopes(\n    mod,\n    re_formula = NA,\n    variables = \"party_autonomy\") |>\n    posterior_draws()\n\nplot1 <- ggplot(pred, aes(x = draw, fill = party_autonomy)) +\n         stat_halfeye(slab_alpha = .5) +\n         labs(x = \"Media index (Predicted)\",\n              y = \"Posterior density\",\n              fill = \"Party autonomy\")\n\nplot2 <- ggplot(mfx, aes(x = draw)) +\n         stat_halfeye(slab_alpha = .5)  +\n         labs(x = \"Contrast: Party autonomy TRUE - FALSE\",\n              y = \"\",\n              fill = \"Party autonomy\")\n\n## combine plots using the `patchwork` package\nplot1 + plot2\n\n\n\n\n\n8.2.5 Region-specific predictions and contrasts\nPredicted media index by region and level of civil liberties:\n\npred <- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       party_autonomy = FALSE, \n                                       civil_liberties = seq(0, 1, length.out = 100))) |> \n        posterior_draws()\n\nggplot(pred, aes(x = civil_liberties, y = draw)) +\n    stat_lineribbon() +\n    scale_fill_brewer(palette = \"Reds\") +\n    facet_wrap(~ region) +\n    labs(x = \"Civil liberties\",\n         y = \"Media index (predicted)\",\n         fill = \"\")\n\n\n\n\nPredicted media index by region and level of civil liberties:\n\npred <- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       civil_liberties = c(.2, .8),\n                                      party_autonomy = FALSE)) |>\n        posterior_draws()\n\nggplot(pred, aes(x = draw, fill = factor(civil_liberties))) +\n    stat_halfeye(slab_alpha = .5) +\n    facet_wrap(~ region) +\n    labs(x = \"Media index (predicted)\",\n         y = \"Posterior density\",\n         fill = \"Civil liberties\")\n\n\n\n\nPredicted media index by region and party autonomy:\n\npred <- predictions(mod,\n                    newdata = datagrid(region = vdem_2015$region,\n                                       party_autonomy = c(TRUE, FALSE),\n                                       civil_liberties = .5)) |>\n        posterior_draws()\n\nggplot(pred, aes(x = draw, y = region , fill = party_autonomy)) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Media index (predicted)\",\n         y = \"\",\n         fill = \"Party autonomy\")\n\n\n\n\nTRUE/FALSE contrasts (marginal effects) of party autonomy by region:\n\nmfx <- slopes(\n    mod,\n    variables = \"party_autonomy\",\n    newdata = datagrid(\n        region = vdem_2015$region,\n        civil_liberties = .5)) |>\n    posterior_draws()\n\nggplot(mfx, aes(x = draw, y = region , fill = party_autonomy)) +\n    stat_halfeye(slab_alpha = .5) +\n    labs(x = \"Media index (predicted)\",\n         y = \"\",\n         fill = \"Party autonomy\")\n\n\n\n\n\n8.2.6 Hypothetical groups\nWe can also obtain predictions or marginal effects for a hypothetical group instead of one of the observed regions. To achieve this, we create a dataset with NA in the region column. Then we call the marginaleffects or predictions() functions with the allow_new_levels argument. This argument is pushed through via the ellipsis (...) to the posterior_epred function of the brms package:\n\ndat <- data.frame(civil_liberties = .5,\n                  party_autonomy = FALSE,\n                  region = \"New Region\")\n\nmfx <- slopes(\n    mod,\n    variables = \"party_autonomy\",\n    allow_new_levels = TRUE,\n    newdata = dat)\n\ndraws <- posterior_draws(mfx)\n\nggplot(draws, aes(x = draw)) +\n     stat_halfeye() +\n     labs(x = \"Marginal effect of party autonomy in a generic world region\", y = \"\")\n\n\n\n\n\n8.2.7 Averaging, marginalizing, integrating random effects\nConsider a logistic regression model with random effects:\n\ndat <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/plm/EmplUK.csv\")\ndat$x <- as.numeric(dat$output > median(dat$output))\ndat$y <- as.numeric(dat$emp > median(dat$emp))\nmod <- brm(y ~ x + (1 | firm), data = dat, backend = \"cmdstanr\", family = \"bernoulli\")\n\nWe can compute adjusted predictions for a given value of x and for each firm (random effects) as follows:\n\np <- predictions(mod, newdata = datagrid(x = 0, firm = unique))\nhead(p)\n#> \n#>  Estimate    2.5 % 97.5 % x firm\n#>   1.0e+00 9.01e-01 1.0000 0    1\n#>   1.0e+00 8.95e-01 1.0000 0    2\n#>   1.0e+00 9.12e-01 1.0000 0    3\n#>   1.0e+00 7.97e-01 1.0000 0    4\n#>   1.0e+00 9.09e-01 1.0000 0    5\n#>   4.9e-08 8.42e-21 0.0019 0    6\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, y, x, firm\n\nWe can average/marginalize/integrate across random effects with the avg_predictions() function or the by argument:\n\navg_predictions(mod, newdata = datagrid(x = 0, firm = unique))\n#> \n#>  Estimate 2.5 % 97.5 %\n#>     0.454  0.44  0.468\n#> \n#> Columns: estimate, conf.low, conf.high\n\npredictions(mod, newdata = datagrid(x = 0:1, firm = unique), by = \"x\")\n#> \n#>  x Estimate 2.5 % 97.5 %\n#>  0    0.454 0.440  0.468\n#>  1    0.557 0.546  0.570\n#> \n#> Columns: x, estimate, conf.low, conf.high\n\nWe can also draw from the (assumed gaussian) population distribution of random effects, by asking predictions() to make predictions for new “levels” of the random effects. If we then take an average of predictions using avg_predictions() or the by argument, we will have “integrated out the random effects”, as described in the brmsmargins package vignette. In the code below, we make predictions for 100 firm identifiers which were not in the original dataset. We also ask predictions() to push forward the allow_new_levels and sample_new_levels arguments to the brms::posterior_epred function:\n\npredictions(\n    mod,\n    newdata = datagrid(x = 0:1, firm = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\",\n    by = \"x\")\n#> \n#>  x Estimate 2.5 % 97.5 %\n#>  0    0.451 0.339  0.569\n#>  1    0.550 0.440  0.663\n#> \n#> Columns: x, estimate, conf.low, conf.high\n\nWe can “integrate out” random effects in the other slopes() functions too. For instance,\n\navg_comparisons(\n    mod,\n    newdata = datagrid(firm = -1:-100),\n    allow_new_levels = TRUE,\n    sample_new_levels = \"gaussian\")\n#> \n#>  Term Contrast Estimate  2.5 % 97.5 %\n#>     x    1 - 0    0.097 0.0473  0.163\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\nThis is nearly equivalent the brmsmargins command output (with slight variations due to different random seeds):\n\nlibrary(brmsmargins)\nbm <- brmsmargins(\n  k = 100,\n  object = mod,\n  at = data.frame(x = c(0, 1)),\n  CI = .95,\n  CIType = \"ETI\",\n  contrasts = cbind(\"AME x\" = c(-1, 1)),\n  effects = \"integrateoutRE\")\nbm$ContrastSummary |> data.frame()\n#>            M        Mdn        LL       UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#> 1 0.09878948 0.09753526 0.0481556 0.159884          NA         NA 0.95    ETI <NA> <NA> AME x\n\nSee the alternative software vignette for more information on brmsmargins."
  },
  {
    "objectID": "articles/brms.html#multinomial-logit",
    "href": "articles/brms.html#multinomial-logit",
    "title": "\n8  Bayes\n",
    "section": "\n8.3 Multinomial logit",
    "text": "8.3 Multinomial logit\nFit a model with categorical outcome (heating system choice in California houses) and logit link:\n\ndat <- \"https://vincentarelbundock.github.io/Rdatasets/csv/Ecdat/Heating.csv\"\ndat <- read.csv(dat)\nmod <- brm(depvar ~ ic.gc + oc.gc,\n           data = dat,\n           family = categorical(link = \"logit\"))\n\n\n8.3.1 Adjusted predictions\nCompute predicted probabilities for each level of the outcome variable:\n\npred <- predictions(mod)\n\nhead(pred)\n#> \n#>  Group Estimate  2.5 % 97.5 %\n#>     ec   0.0663 0.0447 0.0930\n#>     ec   0.0768 0.0590 0.0974\n#>     ec   0.1030 0.0618 0.1585\n#>     ec   0.0634 0.0459 0.0838\n#>     ec   0.0745 0.0574 0.0947\n#>     ec   0.0709 0.0455 0.1036\n#> \n#> Columns: rowid, group, estimate, conf.low, conf.high, depvar, ic.gc, oc.gc\n\nExtract posterior draws and plot them:\n\ndraws <- posterior_draws(pred)\n\nggplot(draws, aes(x = draw, fill = group)) +\n    geom_density(alpha = .2, color = \"white\") +\n    labs(x = \"Predicted probability\",\n         y = \"Density\",\n         fill = \"Heating system\")\n\n\n\n\nUse the plot_predictions() function to plot conditional adjusted predictions for each level of the outcome variable gear, conditional on the value of the mpg regressor:\n\nplot_predictions(mod, condition = \"oc.gc\") +\n    facet_wrap(~ group) +\n    labs(y = \"Predicted probability\")\n\n\n\n\n\n8.3.2 Marginal effects\n\navg_slopes(mod)\n#> \n#>  Group  Term  Estimate     2.5 %   97.5 %\n#>     ec ic.gc -1.77e-04 -3.96e-04 2.37e-05\n#>     er ic.gc  1.65e-05 -2.26e-04 2.51e-04\n#>     gc ic.gc  1.38e-05 -3.72e-04 4.00e-04\n#>     gr ic.gc  4.24e-05 -2.37e-04 3.30e-04\n#>     hp ic.gc  1.07e-04 -7.73e-05 2.97e-04\n#>     ec oc.gc  4.88e-04 -4.04e-04 1.45e-03\n#>     er oc.gc -1.02e-03 -2.07e-03 2.98e-05\n#>     gc oc.gc  1.04e-03 -7.39e-04 2.78e-03\n#>     gr oc.gc  9.46e-05 -1.19e-03 1.34e-03\n#>     hp oc.gc -5.85e-04 -1.45e-03 2.30e-04\n#> \n#> Columns: group, term, estimate, conf.low, conf.high"
  },
  {
    "objectID": "articles/brms.html#hurdle-models",
    "href": "articles/brms.html#hurdle-models",
    "title": "\n8  Bayes\n",
    "section": "\n8.4 Hurdle models",
    "text": "8.4 Hurdle models\nThis section replicates some analyses from yet another amazing blog post by Andrew Heiss.\nTo begin, we estimate a hurdle model in brms with random effects, using data from the gapminder package: 704G\n\nlibrary(gapminder)\nlibrary(brms)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(ggdist)\nlibrary(cmdstanr)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\nset.seed(1024)\n\nCHAINS <- 4\nITER <- 2000\nWARMUP <- 1000\nBAYES_SEED <- 1234\n\ngapminder <- gapminder::gapminder |> \n  filter(continent != \"Oceania\") |> \n  # Make a bunch of GDP values 0\n  mutate(prob_zero = ifelse(lifeExp < 50, 0.3, 0.02),\n         will_be_zero = rbinom(n(), 1, prob = prob_zero),\n         gdpPercap = ifelse(will_be_zero, 0, gdpPercap)) |> \n  select(-prob_zero, -will_be_zero) |> \n  # Make a logged version of GDP per capita\n  mutate(log_gdpPercap = log1p(gdpPercap)) |> \n  mutate(is_zero = gdpPercap == 0)\n\nmod <- brm(\n  bf(gdpPercap ~ lifeExp + year + (1 + lifeExp + year | continent),\n     hu ~ lifeExp),\n  data = gapminder,\n  backend = \"cmdstanr\",\n  family = hurdle_lognormal(),\n  cores = 2,\n  chains = CHAINS, iter = ITER, warmup = WARMUP, seed = BAYES_SEED,\n  silent = 2)\n\n\n8.4.1 Adjusted predictions\nAdjusted predictions for every observation in the original data:\n\npredictions(mod) |> head()\n#> \n#>  Estimate 2.5 % 97.5 %\n#>       143   103    219\n#>       168   125    256\n#>       202   153    304\n#>       251   197    373\n#>       312   250    454\n#>       398   325    567\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent\n\nAdjusted predictions for the hu parameter:\n\npredictions(mod, dpar = \"hu\") |> head()\n#> \n#>  Estimate 2.5 % 97.5 %\n#>     0.574 0.475  0.652\n#>     0.537 0.442  0.611\n#>     0.496 0.407  0.566\n#>     0.446 0.366  0.511\n#>     0.396 0.325  0.454\n#>     0.341 0.282  0.391\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent\n\nPredictions on a different scale:\n\npredictions(mod, type = \"link\", dpar = \"hu\") |> head()\n#> \n#>  Estimate  2.5 %  97.5 %\n#>    0.2980 -0.101  0.6259\n#>    0.1463 -0.235  0.4527\n#>   -0.0178 -0.377  0.2673\n#>   -0.2189 -0.551  0.0424\n#>   -0.4234 -0.730 -0.1857\n#>   -0.6573 -0.933 -0.4443\n#> \n#> Columns: rowid, estimate, conf.low, conf.high, gdpPercap, lifeExp, year, continent\n\nPlot adjusted predictions as a function of lifeExp:\n\nplot_predictions(\n    mod,\n    condition = \"lifeExp\") +\n    labs(y = \"mu\") +\nplot_predictions(\n    mod,\n    dpar = \"hu\",\n    condition = \"lifeExp\") +\n    labs(y = \"hu\")\n\n\n\n\nPredictions with more than one condition and the re_formula argument from brms:\n\nplot_predictions(\n    mod,\n    re_formula = NULL,\n    condition = c(\"lifeExp\", \"continent\"))\n\n\n\n\n\n8.4.2 Extract draws with posterior_draws()\n\nThe posterior_draws() function extract raw samples from the posterior from objects produced by marginaleffects. This allows us to use richer geoms and summaries, such as those in the ggdist package:\n\npredictions(\n    mod,\n    re_formula = NULL,\n    newdata = datagrid(model = mod,\n                       continent = gapminder$continent,\n                       year = c(1952, 2007),\n                       lifeExp = seq(30, 80, 1))) |>\n    posterior_draws() |>\n    ggplot(aes(lifeExp, draw, fill = continent, color = continent)) +\n    stat_lineribbon(alpha = .25) +\n    facet_grid(year ~ continent)\n\n\n\n\n\n8.4.3 Average Contrasts\nWhat happens to gdpPercap when lifeExp increases by one?\n\navg_comparisons(mod)\n#> \n#>     Term Contrast Estimate 2.5 % 97.5 %\n#>  lifeExp       +1    718.9 515.6  812.0\n#>  year          +1    -63.8 -84.4  -41.1\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\nWhat happens to gdpPercap when lifeExp increases by one standard deviation?\n\navg_comparisons(mod, variables = list(lifeExp = \"sd\"))\n#> \n#>     Term                Contrast Estimate 2.5 % 97.5 %\n#>  lifeExp (x + sd/2) - (x - sd/2)     4050  3718   4741\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\nWhat happens to gdpPercap when lifeExp increases from 50 to 60 and year simultaneously increases its min to its max?\n\navg_comparisons(\n    mod,\n    variables = list(lifeExp = c(50, 60), year = \"minmax\"),\n    cross = TRUE)\n#> \n#>  C: lifeExp   C: year Estimate 2.5 % 97.5 %\n#>     60 - 50 Max - Min      835   523   1404\n#> \n#> Columns: term, contrast_lifeExp, contrast_year, estimate, conf.low, conf.high\n\nPlot draws from the posterior distribution of average contrasts (not the same thing as draws from the posterior distribution of contrasts):\n\navg_comparisons(mod) |>\n    posterior_draws() |>\n    ggplot(aes(estimate, term)) +\n    stat_dotsinterval() +\n    labs(x = \"Posterior distribution of average contrasts\", y = \"\")\n\n\n\n\n\n8.4.4 Marginal effects (slopes)\nAverage Marginal Effect of lifeExp on different scales and for different parameters:\n\navg_slopes(mod)\n#> \n#>     Term Estimate 2.5 % 97.5 %\n#>  lifeExp    718.7 515.6    812\n#>  year       -63.8 -84.4    -41\n#> \n#> Columns: term, estimate, conf.low, conf.high\n\navg_slopes(mod, type = \"link\")\n#> \n#>     Term Estimate   2.5 %   97.5 %\n#>  lifeExp  0.08249  0.0742  0.08856\n#>  year    -0.00937 -0.0120 -0.00632\n#> \n#> Columns: term, estimate, conf.low, conf.high\n\navg_slopes(mod, dpar = \"hu\")\n#> \n#>     Term Estimate    2.5 %   97.5 %\n#>  lifeExp -0.00817 -0.00937 -0.00669\n#>  year     0.00000  0.00000  0.00000\n#> \n#> Columns: term, estimate, conf.low, conf.high\n\navg_slopes(mod, dpar = \"hu\", type = \"link\")\n#> \n#>     Term Estimate  2.5 %  97.5 %\n#>  lifeExp  -0.0993 -0.113 -0.0838\n#>  year      0.0000  0.000  0.0000\n#> \n#> Columns: term, estimate, conf.low, conf.high\n\nPlot Conditional Marginal Effects\n\nplot_slopes(\n    mod,\n    variables = \"lifeExp\",\n    condition = \"lifeExp\") +\n    labs(y = \"mu\") +\n\nplot_slopes(\n    mod,\n    dpar = \"hu\",\n    variables = \"lifeExp\",\n    condition = \"lifeExp\") +\n    labs(y = \"hu\")\n\n\n\n\nOr we can call slopes() or comparisons() with posterior_draws() function to have even more control:\n\ncomparisons(\n    mod,\n    type = \"link\",\n    variables = \"lifeExp\",\n    newdata = datagrid(lifeExp = c(40, 70), continent = gapminder$continent)) |>\n    posterior_draws() |>\n    ggplot(aes(draw, continent, fill = continent)) +\n    stat_dotsinterval() +\n    facet_grid(lifeExp ~ .) +\n    labs(x = \"Effect of a 1 unit change in Life Expectancy\")"
  },
  {
    "objectID": "articles/brms.html#bayesian-estimates-and-credible-intervals",
    "href": "articles/brms.html#bayesian-estimates-and-credible-intervals",
    "title": "\n8  Bayes\n",
    "section": "\n8.5 Bayesian estimates and credible intervals",
    "text": "8.5 Bayesian estimates and credible intervals\nFor bayesian models like those produced by the brms or rstanarm packages, the marginaleffects package functions report the median of the posterior distribution as their main estimates.\nThe default credible intervals are equal-tailed intervals (quantiles), and the default function to identify the center of the distribution is the median. Users can customize the type of intervals reported by setting global options. Note that both the reported estimate and the intervals change slightly:\n\nlibrary(insight)\nlibrary(marginaleffects)\n\nmod <- insight::download_model(\"brms_1\")\n\noptions(marginaleffects_posterior_interval = \"hdi\")\noptions(marginaleffects_posterior_center = mean)\navg_comparisons(mod)\n#> \n#>  Term Contrast Estimate 2.5 % 97.5 %\n#>   cyl       +1    -1.50 -2.38 -0.677\n#>   wt        +1    -3.21 -4.70 -1.570\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\noptions(marginaleffects_posterior_interval = \"eti\")\noptions(marginaleffects_posterior_center = stats::median)\navg_comparisons(mod)\n#> \n#>  Term Contrast Estimate 2.5 % 97.5 %\n#>   cyl       +1    -1.49 -2.36 -0.636\n#>   wt        +1    -3.20 -4.79 -1.645\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high"
  },
  {
    "objectID": "articles/brms.html#random-variables-posterior-and-ggdist",
    "href": "articles/brms.html#random-variables-posterior-and-ggdist",
    "title": "\n8  Bayes\n",
    "section": "\n8.6 Random variables: posterior and ggdist\n",
    "text": "8.6 Random variables: posterior and ggdist\n\nRecent versions of the posterior, brms, and ggdist packages make it easy to draw, summarize and plot random variables. The posterior_draws() can produce objects of class rvar which make it easy to use those features by returning a data frame with a column of type rvar:\n\nlibrary(brms)\nlibrary(ggdist)\nlibrary(ggplot2)\nlibrary(marginaleffects)\nmod <- brm(am ~ mpg + hp, data = mtcars, family = bernoulli)\n\n\navg_comparisons(mod) |>\n  posterior_draws(shape = \"rvar\") |>\n  ggplot(aes(y = term, xdist = rvar)) + \n  stat_slabinterval()"
  },
  {
    "objectID": "articles/brms.html#non-linear-hypothesis-testing",
    "href": "articles/brms.html#non-linear-hypothesis-testing",
    "title": "\n8  Bayes\n",
    "section": "\n8.7 Non-linear hypothesis testing",
    "text": "8.7 Non-linear hypothesis testing\nWe begin by estimating a model:\n\n##| eval = FALSE\nmod <- brm(am ~ mpg + hp, data = mtcars, family = bernoulli(),\n           seed = 1024, silent = 2, chains = 4, iter = 1000)\n#> \n#> SAMPLING FOR MODEL '54927d44a8a7905f9b337838503f25f8' NOW (CHAIN 1).\n#> Chain 1: \n#> Chain 1: Gradient evaluation took 1.3e-05 seconds\n#> Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 0.13 seconds.\n#> Chain 1: Adjust your expectations accordingly!\n#> Chain 1: \n#> Chain 1: \n#> Chain 1: Iteration:   1 / 1000 [  0%]  (Warmup)\n#> Chain 1: Iteration: 100 / 1000 [ 10%]  (Warmup)\n#> Chain 1: Iteration: 200 / 1000 [ 20%]  (Warmup)\n#> Chain 1: Iteration: 300 / 1000 [ 30%]  (Warmup)\n#> Chain 1: Iteration: 400 / 1000 [ 40%]  (Warmup)\n#> Chain 1: Iteration: 500 / 1000 [ 50%]  (Warmup)\n#> Chain 1: Iteration: 501 / 1000 [ 50%]  (Sampling)\n#> Chain 1: Iteration: 600 / 1000 [ 60%]  (Sampling)\n#> Chain 1: Iteration: 700 / 1000 [ 70%]  (Sampling)\n#> Chain 1: Iteration: 800 / 1000 [ 80%]  (Sampling)\n#> Chain 1: Iteration: 900 / 1000 [ 90%]  (Sampling)\n#> Chain 1: Iteration: 1000 / 1000 [100%]  (Sampling)\n#> Chain 1: \n#> Chain 1:  Elapsed Time: 0.041115 seconds (Warm-up)\n#> Chain 1:                0.019328 seconds (Sampling)\n#> Chain 1:                0.060443 seconds (Total)\n#> Chain 1: \n#> \n#> SAMPLING FOR MODEL '54927d44a8a7905f9b337838503f25f8' NOW (CHAIN 2).\n#> Chain 2: \n#> Chain 2: Gradient evaluation took 5e-06 seconds\n#> Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 0.05 seconds.\n#> Chain 2: Adjust your expectations accordingly!\n#> Chain 2: \n#> Chain 2: \n#> Chain 2: Iteration:   1 / 1000 [  0%]  (Warmup)\n#> Chain 2: Iteration: 100 / 1000 [ 10%]  (Warmup)\n#> Chain 2: Iteration: 200 / 1000 [ 20%]  (Warmup)\n#> Chain 2: Iteration: 300 / 1000 [ 30%]  (Warmup)\n#> Chain 2: Iteration: 400 / 1000 [ 40%]  (Warmup)\n#> Chain 2: Iteration: 500 / 1000 [ 50%]  (Warmup)\n#> Chain 2: Iteration: 501 / 1000 [ 50%]  (Sampling)\n#> Chain 2: Iteration: 600 / 1000 [ 60%]  (Sampling)\n#> Chain 2: Iteration: 700 / 1000 [ 70%]  (Sampling)\n#> Chain 2: Iteration: 800 / 1000 [ 80%]  (Sampling)\n#> Chain 2: Iteration: 900 / 1000 [ 90%]  (Sampling)\n#> Chain 2: Iteration: 1000 / 1000 [100%]  (Sampling)\n#> Chain 2: \n#> Chain 2:  Elapsed Time: 0.061582 seconds (Warm-up)\n#> Chain 2:                0.02435 seconds (Sampling)\n#> Chain 2:                0.085932 seconds (Total)\n#> Chain 2: \n#> \n#> SAMPLING FOR MODEL '54927d44a8a7905f9b337838503f25f8' NOW (CHAIN 3).\n#> Chain 3: \n#> Chain 3: Gradient evaluation took 6e-06 seconds\n#> Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\n#> Chain 3: Adjust your expectations accordingly!\n#> Chain 3: \n#> Chain 3: \n#> Chain 3: Iteration:   1 / 1000 [  0%]  (Warmup)\n#> Chain 3: Iteration: 100 / 1000 [ 10%]  (Warmup)\n#> Chain 3: Iteration: 200 / 1000 [ 20%]  (Warmup)\n#> Chain 3: Iteration: 300 / 1000 [ 30%]  (Warmup)\n#> Chain 3: Iteration: 400 / 1000 [ 40%]  (Warmup)\n#> Chain 3: Iteration: 500 / 1000 [ 50%]  (Warmup)\n#> Chain 3: Iteration: 501 / 1000 [ 50%]  (Sampling)\n#> Chain 3: Iteration: 600 / 1000 [ 60%]  (Sampling)\n#> Chain 3: Iteration: 700 / 1000 [ 70%]  (Sampling)\n#> Chain 3: Iteration: 800 / 1000 [ 80%]  (Sampling)\n#> Chain 3: Iteration: 900 / 1000 [ 90%]  (Sampling)\n#> Chain 3: Iteration: 1000 / 1000 [100%]  (Sampling)\n#> Chain 3: \n#> Chain 3:  Elapsed Time: 0.051607 seconds (Warm-up)\n#> Chain 3:                0.017741 seconds (Sampling)\n#> Chain 3:                0.069348 seconds (Total)\n#> Chain 3: \n#> \n#> SAMPLING FOR MODEL '54927d44a8a7905f9b337838503f25f8' NOW (CHAIN 4).\n#> Chain 4: \n#> Chain 4: Gradient evaluation took 6e-06 seconds\n#> Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 0.06 seconds.\n#> Chain 4: Adjust your expectations accordingly!\n#> Chain 4: \n#> Chain 4: \n#> Chain 4: Iteration:   1 / 1000 [  0%]  (Warmup)\n#> Chain 4: Iteration: 100 / 1000 [ 10%]  (Warmup)\n#> Chain 4: Iteration: 200 / 1000 [ 20%]  (Warmup)\n#> Chain 4: Iteration: 300 / 1000 [ 30%]  (Warmup)\n#> Chain 4: Iteration: 400 / 1000 [ 40%]  (Warmup)\n#> Chain 4: Iteration: 500 / 1000 [ 50%]  (Warmup)\n#> Chain 4: Iteration: 501 / 1000 [ 50%]  (Sampling)\n#> Chain 4: Iteration: 600 / 1000 [ 60%]  (Sampling)\n#> Chain 4: Iteration: 700 / 1000 [ 70%]  (Sampling)\n#> Chain 4: Iteration: 800 / 1000 [ 80%]  (Sampling)\n#> Chain 4: Iteration: 900 / 1000 [ 90%]  (Sampling)\n#> Chain 4: Iteration: 1000 / 1000 [100%]  (Sampling)\n#> Chain 4: \n#> Chain 4:  Elapsed Time: 0.043835 seconds (Warm-up)\n#> Chain 4:                0.015924 seconds (Sampling)\n#> Chain 4:                0.059759 seconds (Total)\n#> Chain 4:\n\nNotice that we can compute average contrasts in two different ways, using the avg_comparisons() function or the comparison argument:\n\navg_comparisons(mod)\n#> \n#>  Term Contrast Estimate   2.5 %  97.5 %\n#>   hp        +1  0.00601 0.00306 0.00877\n#>   mpg       +1  0.13500 0.08255 0.17200\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\ncomparisons(mod, comparison = \"differenceavg\")\n#> \n#>  Term Contrast Estimate   2.5 %  97.5 %\n#>   mpg mean(+1)  0.13500 0.08255 0.17200\n#>   hp  mean(+1)  0.00601 0.00306 0.00877\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx\n\nNow, we use the hypothesis argument to compare the first to the second rows of the comparisons() output:\n\ncomparisons(\n    mod,\n    comparison = \"differenceavg\",\n    hypothesis = \"b2 - b1 = 0.2\")\n#> \n#>       Term Estimate  2.5 % 97.5 %\n#>  b2-b1=0.2   -0.329 -0.365 -0.279\n#> \n#> Columns: term, estimate, conf.low, conf.high\n\nThe hypotheses() function of the brms package can also perform non-linear hypothesis testing, and it generates some convenient statistics and summaries. This function accepts a D-by-P matrix of draws from the posterior distribution, where D is the number of draws and N is the number of parameters. We can obtain such a matrix using the posterior_draws(x, shape = \"DxP\"), and we can simply add a couple calls to our chain of operations:\n\navg_comparisons(mod, comparison = \"differenceavg\") |>\n    posterior_draws(shape = \"DxP\") |>\n    brms::hypothesis(\"b2 - b1 > .2\")\n#> Hypothesis Tests for class :\n#>         Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob Star\n#> 1 (b2-b1)-(.2) > 0    -0.33      0.02    -0.36    -0.29          0         0     \n#> ---\n#> 'CI': 90%-CI for one-sided and 95%-CI for two-sided hypotheses.\n#> '*': For one-sided hypotheses, the posterior probability exceeds 95%;\n#> for two-sided hypotheses, the value tested against lies outside the 95%-CI.\n#> Posterior probabilities of point hypotheses assume equal prior probabilities."
  },
  {
    "objectID": "articles/bootstrap.html#delta-method",
    "href": "articles/bootstrap.html#delta-method",
    "title": "\n9  Bootstrap & Simulation\n",
    "section": "\n9.1 Delta method",
    "text": "9.1 Delta method\nThe default strategy to compute standard errors and confidence intervals is the delta method. This is what we obtain by calling:\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\")\n#> \n#>         Term Contrast    Species Estimate Std. Error      z Pr(>|z|)   S  2.5 % 97.5 %\n#>  Petal.Width mean(+1) setosa      -0.1103      0.285 -0.387    0.699 0.5 -0.669  0.449\n#>  Petal.Width mean(+1) versicolor  -0.0201      0.160 -0.125    0.900 0.2 -0.334  0.293\n#>  Petal.Width mean(+1) virginica    0.0216      0.169  0.128    0.898 0.2 -0.309  0.353\n#> \n#> Columns: term, contrast, Species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nSince this is the default method, we obtain the same results if we add the inferences() call in the chain:\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |>\n  inferences(method = \"delta\")\n#> \n#>         Term Contrast    Species Estimate Std. Error      z Pr(>|z|)   S  2.5 % 97.5 %\n#>  Petal.Width mean(+1) setosa      -0.1103      0.285 -0.387    0.699 0.5 -0.669  0.449\n#>  Petal.Width mean(+1) versicolor  -0.0201      0.160 -0.125    0.900 0.2 -0.334  0.293\n#>  Petal.Width mean(+1) virginica    0.0216      0.169  0.128    0.898 0.2 -0.309  0.353\n#> \n#> Columns: term, contrast, Species, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo"
  },
  {
    "objectID": "articles/bootstrap.html#bootstrap",
    "href": "articles/bootstrap.html#bootstrap",
    "title": "\n9  Bootstrap & Simulation\n",
    "section": "\n9.2 Bootstrap",
    "text": "9.2 Bootstrap\nmarginaleffects supports three bootstrap frameworks in R: the well-established boot package, the newer rsample package, and the so-called “bayesian bootstrap” in fwb.\n\n9.2.1 boot\n\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |>\n  inferences(method = \"boot\")\n#> \n#>         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#>  Petal.Width mean(+1) setosa      -0.1103      0.263 -0.640  0.394\n#>  Petal.Width mean(+1) versicolor  -0.0201      0.161 -0.328  0.312\n#>  Petal.Width mean(+1) virginica    0.0216      0.184 -0.336  0.384\n#> \n#> Columns: term, contrast, Species, estimate, predicted, predicted_hi, predicted_lo, std.error, conf.low, conf.high\n\nAll unknown arguments that we feed to inferences() are pushed forward to boot::boot():\n\nest <- avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |>\n  inferences(method = \"boot\", sim = \"balanced\", R = 500, conf_type = \"bca\")\nest\n#> \n#>         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#>  Petal.Width mean(+1) setosa      -0.1103      0.252 -0.601  0.430\n#>  Petal.Width mean(+1) versicolor  -0.0201      0.159 -0.320  0.310\n#>  Petal.Width mean(+1) virginica    0.0216      0.185 -0.332  0.413\n#> \n#> Columns: term, contrast, Species, estimate, predicted, predicted_hi, predicted_lo, std.error, conf.low, conf.high\n\nWe can extract the original boot object from an attribute:\n\nattr(est, \"inferences\")\n#> \n#> BALANCED BOOTSTRAP\n#> \n#> \n#> Call:\n#> bootstrap_boot(model = model, FUN = FUN, newdata = ..1, vcov = ..2, \n#>     variables = ..3, type = ..4, by = ..5, conf_level = ..6, \n#>     comparison = ..7, transform = ..8, wts = ..9, hypothesis = ..10, \n#>     eps = ..11)\n#> \n#> \n#> Bootstrap Statistics :\n#>        original        bias    std. error\n#> t1* -0.11025325  0.0057996460   0.2522810\n#> t2* -0.02006005  0.0002847218   0.1592232\n#> t3*  0.02158742 -0.0022618407   0.1847906\n\nOr we can extract the individual draws with the posterior_draws() function:\n\nposterior_draws(est) |> head()\n#>   drawid        draw        term contrast    Species    estimate predicted predicted_hi predicted_lo std.error   conf.low conf.high\n#> 1      1  0.18010719 Petal.Width mean(+1)     setosa -0.11025325  4.957514     4.901389     5.013640 0.2522810 -0.6007190 0.4297487\n#> 2      1  0.17354742 Petal.Width mean(+1) versicolor -0.02006005  6.327949     6.325011     6.330887 0.1592232 -0.3204596 0.3104354\n#> 3      1  0.17051839 Petal.Width mean(+1)  virginica  0.02158742  7.015513     7.033528     6.997499 0.1847906 -0.3322993 0.4129883\n#> 4      2 -0.05205084 Petal.Width mean(+1)     setosa -0.11025325  4.957514     4.901389     5.013640 0.2522810 -0.6007190 0.4297487\n#> 5      2 -0.08317330 Petal.Width mean(+1) versicolor -0.02006005  6.327949     6.325011     6.330887 0.1592232 -0.3204596 0.3104354\n#> 6      2 -0.09754435 Petal.Width mean(+1)  virginica  0.02158742  7.015513     7.033528     6.997499 0.1847906 -0.3322993 0.4129883\n\nposterior_draws(est, shape = \"DxP\") |> dim()\n#> [1] 500   3\n\n\n9.2.2 rsample\n\nAs before, we can pass arguments to rsample::bootstraps() through inferences(). For example, for stratified resampling:\n\nest <- avg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |>\n  inferences(method = \"rsample\", R = 100, strata = \"Species\")\nest\n#> \n#>         Term Contrast    Species Estimate  2.5 % 97.5 %\n#>  Petal.Width mean(+1) setosa      -0.1103 -0.714  0.415\n#>  Petal.Width mean(+1) versicolor  -0.0201 -0.318  0.277\n#>  Petal.Width mean(+1) virginica    0.0216 -0.350  0.317\n#> \n#> Columns: term, contrast, Species, estimate, predicted, predicted_hi, predicted_lo, conf.low, conf.high\n\nattr(est, \"inferences\")\n#> # Bootstrap sampling using stratification with apparent sample \n#> # A tibble: 101 × 3\n#>    splits           id           estimates       \n#>    <list>           <chr>        <list>          \n#>  1 <split [150/49]> Bootstrap001 <tibble [3 × 7]>\n#>  2 <split [150/57]> Bootstrap002 <tibble [3 × 7]>\n#>  3 <split [150/55]> Bootstrap003 <tibble [3 × 7]>\n#>  4 <split [150/51]> Bootstrap004 <tibble [3 × 7]>\n#>  5 <split [150/49]> Bootstrap005 <tibble [3 × 7]>\n#>  6 <split [150/57]> Bootstrap006 <tibble [3 × 7]>\n#>  7 <split [150/52]> Bootstrap007 <tibble [3 × 7]>\n#>  8 <split [150/58]> Bootstrap008 <tibble [3 × 7]>\n#>  9 <split [150/54]> Bootstrap009 <tibble [3 × 7]>\n#> 10 <split [150/51]> Bootstrap010 <tibble [3 × 7]>\n#> # ℹ 91 more rows\n\nOr we can extract the individual draws with the posterior_draws() function:\n\nposterior_draws(est) |> head()\n#>   drawid        draw        term contrast    Species    estimate predicted predicted_hi predicted_lo   conf.low conf.high\n#> 1      1 -0.39075247 Petal.Width mean(+1)     setosa -0.11025325  4.957514     4.901389     5.013640 -0.7139422 0.4146373\n#> 2      1 -0.10922815 Petal.Width mean(+1) versicolor -0.02006005  6.327949     6.325011     6.330887 -0.3176045 0.2771408\n#> 3      1  0.02076807 Petal.Width mean(+1)  virginica  0.02158742  7.015513     7.033528     6.997499 -0.3495436 0.3173226\n#> 4      2 -0.04324980 Petal.Width mean(+1)     setosa -0.11025325  4.957514     4.901389     5.013640 -0.7139422 0.4146373\n#> 5      2 -0.14061630 Petal.Width mean(+1) versicolor -0.02006005  6.327949     6.325011     6.330887 -0.3176045 0.2771408\n#> 6      2 -0.18557610 Petal.Width mean(+1)  virginica  0.02158742  7.015513     7.033528     6.997499 -0.3495436 0.3173226\n\nposterior_draws(est, shape = \"PxD\") |> dim()\n#> [1]   3 100\n\n\n9.2.3 Fractional Weighted Bootstrap (aka Bayesian Bootstrap)\nThe fwb package implements fractional weighted bootstrap (aka Bayesian bootstrap):\n\n“fwb implements the fractional weighted bootstrap (FWB), also known as the Bayesian bootstrap, following the treatment by Xu et al. (2020). The FWB involves generating sets of weights from a uniform Dirichlet distribution to be used in estimating statistics of interest, which yields a posterior distribution that can be interpreted in the same way the traditional (resampling-based) bootstrap distribution can be.” -Noah Greifer\n\nThe inferences() function makes it easy to apply this inference strategy to marginaleffects objects:\n\navg_comparisons(mod) |> inferences(method = \"fwb\")\n#> \n#>          Term            Contrast Estimate Std. Error  2.5 % 97.5 %\n#>  Petal.Width  +1                   -0.0362     0.1568 -0.341  0.306\n#>  Petal.Length +1                    0.8929     0.0794  0.729  1.047\n#>  Species      versicolor - setosa  -1.4629     0.3306 -2.134 -0.821\n#>  Species      virginica - setosa   -1.9842     0.3930 -2.792 -1.187\n#> \n#> Columns: term, contrast, estimate, std.error, conf.low, conf.high"
  },
  {
    "objectID": "articles/bootstrap.html#simulation-based-inference",
    "href": "articles/bootstrap.html#simulation-based-inference",
    "title": "\n9  Bootstrap & Simulation\n",
    "section": "\n9.3 Simulation-based inference",
    "text": "9.3 Simulation-based inference\nThis simulation-based strategy to compute confidence intervals was described in Krinsky & Robb (1986) and popularized by King, Tomz, Wittenberg (2000). We proceed in 3 steps:\n\nDraw R sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model’s estimated coefficients and variance equal to the model’s variance-covariance matrix (classical, “HC3”, or other).\nUse the R sets of coefficients to compute R sets of estimands: predictions, comparisons, or slopes.\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\nHere are a few examples:\n\nlibrary(ggplot2)\nlibrary(ggdist)\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |>\n  inferences(method = \"simulation\")\n#> \n#>         Term Contrast    Species Estimate Std. Error  2.5 % 97.5 %\n#>  Petal.Width mean(+1) setosa      -0.1124      0.285 -0.663  0.444\n#>  Petal.Width mean(+1) versicolor  -0.0150      0.167 -0.354  0.295\n#>  Petal.Width mean(+1) virginica    0.0186      0.172 -0.323  0.353\n#> \n#> Columns: term, contrast, Species, estimate, std.error, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx\n\nSince simulation based inference generates R estimates of the quantities of interest, we can treat them similarly to draws from the posterior distribution in bayesian models. For example, we can extract draws using the posterior_draws() function, and plot their distributions using packages likeggplot2 and ggdist:\n\navg_comparisons(mod, by = \"Species\", variables = \"Petal.Width\") |>\n  inferences(method = \"simulation\") |>\n  posterior_draws(\"rvar\") |>\n  ggplot(aes(y = Species, xdist = rvar)) +\n  stat_slabinterval()"
  },
  {
    "objectID": "articles/bootstrap.html#multiple-imputation-and-missing-data",
    "href": "articles/bootstrap.html#multiple-imputation-and-missing-data",
    "title": "\n9  Bootstrap & Simulation\n",
    "section": "\n9.4 Multiple imputation and missing data",
    "text": "9.4 Multiple imputation and missing data\nThe same workflow and the same inferences() function can be used to estimate models with multiple imputation for missing data."
  },
  {
    "objectID": "articles/categorical.html#masspolr-function",
    "href": "articles/categorical.html#masspolr-function",
    "title": "\n10  Categorical outcomes\n",
    "section": "\n10.1 MASS::polr function",
    "text": "10.1 MASS::polr function\nConsider a simple ordered logit model in which we predict the number of gears of a car based its miles per gallon and horsepower:\n\nlibrary(MASS)\nmod <- polr(factor(gear) ~ mpg + hp, data = mtcars, Hess = TRUE)\n\nNow, consider a car with 25 miles per gallon and 110 horsepower. The expected predicted probability for each outcome level (gear) for this car is:\n\npredictions(mod, newdata = datagrid(mpg = 25, hp = 110))\n#> \n#>  Group Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 % mpg  hp\n#>      3    0.203     0.0959 2.12   0.0339  4.9 0.0155  0.391  25 110\n#>      4    0.578     0.1229 4.70   <0.001 18.6 0.3373  0.819  25 110\n#>      5    0.218     0.1007 2.17   0.0302  5.1 0.0209  0.416  25 110\n#> \n#> Columns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, hp\n\nSince the gear is categorical, we make one prediction for each level of the outcome.\nNow consider the marginal effects (aka slopes or partial derivatives) for the same car:\n\nslopes(mod, variables = \"mpg\", newdata = datagrid(mpg = 25, hp = 110))\n#> \n#>  Group Term Estimate Std. Error       z Pr(>|z|)    S    2.5 %  97.5 % mpg  hp\n#>      3  mpg -0.06041     0.0169 -3.5809   <0.001 11.5 -0.09347 -0.0273  25 110\n#>      4  mpg -0.00321     0.0335 -0.0958   0.9237  0.1 -0.06896  0.0625  25 110\n#>      5  mpg  0.06362     0.0301  2.1132   0.0346  4.9  0.00461  0.1226  25 110\n#> \n#> Columns: rowid, group, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, gear, mpg, hp\n\nAgain, marginaleffects produces one estimate of the slope for each outcome level. For a small step size \\(\\varepsilon\\), the printed quantities are estimated as:\n\\[\\frac{P(gear=3|mpg=25+\\varepsilon, hp=110)-P(gear=3|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}\\] \\[\\frac{P(gear=4|mpg=25+\\varepsilon, hp=110)-P(gear=4|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}\\] \\[\\frac{P(gear=5|mpg=25+\\varepsilon, hp=110)-P(gear=5|mpg=25-\\varepsilon, hp=110)}{2 \\cdot \\varepsilon}\\]\nWhen we call avg_slopes(), marginaleffects will repeat the same computation for every row of the original dataset, and then report the average slope for each level of the outcome:\n\navg_slopes(mod)\n#> \n#>  Group Term Estimate Std. Error     z Pr(>|z|)    S     2.5 %   97.5 %\n#>      3  mpg -0.07014   0.015480 -4.53  < 0.001 17.4 -0.100479 -0.03980\n#>      3  hp  -0.00377   0.001514 -2.49  0.01283  6.3 -0.006735 -0.00080\n#>      4  mpg  0.03747   0.013857  2.70  0.00685  7.2  0.010311  0.06463\n#>      4  hp   0.00201   0.000957  2.10  0.03545  4.8  0.000137  0.00389\n#>      5  mpg  0.03267   0.009572  3.41  < 0.001 10.6  0.013907  0.05143\n#>      5  hp   0.00175   0.000833  2.11  0.03522  4.8  0.000122  0.00339\n#> \n#> Columns: group, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/categorical.html#nnet-package",
    "href": "articles/categorical.html#nnet-package",
    "title": "\n10  Categorical outcomes\n",
    "section": "\n10.2 nnet package",
    "text": "10.2 nnet package\nThe multinom function of the nnet package allows users to fit log-linear models via neural networks. The data used for this function is a data frame with one observation per row, and the response variable is coded a factor. All the marginaleffects package function work seamlessly with this model. For example, we can estimate a model and compute average marginal effects as follows:\n\nlibrary(nnet)\n\nhead(mtcars)\n#>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n#> Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n\nmod <- multinom(factor(gear) ~ hp + mpg, data = mtcars, trace = FALSE)\n\navg_slopes(mod, type = \"probs\")\n#> \n#>  Group Term  Estimate Std. Error       z Pr(>|z|)    S    2.5 %    97.5 %\n#>      3  hp  -3.44e-05    0.00225 -0.0153  0.98780  0.0 -0.00444  0.004372\n#>      3  mpg -7.13e-02    0.02645 -2.6959  0.00702  7.2 -0.12316 -0.019466\n#>      4  hp  -4.67e-03    0.00221 -2.1126  0.03464  4.9 -0.00900 -0.000337\n#>      4  mpg  1.59e-02    0.02010  0.7916  0.42859  1.2 -0.02348  0.055293\n#>      5  hp   4.70e-03    0.00130  3.6167  < 0.001 11.7  0.00215  0.007250\n#>      5  mpg  5.54e-02    0.01642  3.3735  < 0.001 10.4  0.02322  0.087593\n#> \n#> Columns: group, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNotice that in such models, we get one marginal effect for each term, for each level of the response variable. For this reason, we should use \"group\" in the condition argument (or facet_*() function) when calling one of the plotting functions:\n\nlibrary(ggplot2)\n\nplot_predictions(mod, condition = c(\"mpg\", \"group\"), type = \"probs\")\n\n\n\n\nplot_predictions(mod, condition = \"mpg\", type = \"probs\") + facet_wrap(~group)\n\n\n\n\nplot_comparisons(\n    mod,\n    variables = list(mpg = c(15, 30)),\n    condition = \"group\",\n    type = \"probs\")"
  },
  {
    "objectID": "articles/categorical.html#mlogit-package",
    "href": "articles/categorical.html#mlogit-package",
    "title": "\n10  Categorical outcomes\n",
    "section": "\n10.3 mlogit package",
    "text": "10.3 mlogit package\nThe mlogit package uses data in a slightly different structure, with one row per observation-choice combination. For example, this data on choice of travel mode includes 4 rows per individual, one for each mode of transportation:\n\nlibrary(\"AER\")\nlibrary(\"mlogit\")\nlibrary(\"tidyverse\")\ndata(\"TravelMode\", package = \"AER\")\n\nhead(TravelMode)\n#>   individual  mode choice wait vcost travel gcost income size\n#> 1          1   air     no   69    59    100    70     35    1\n#> 2          1 train     no   34    31    372    71     35    1\n#> 3          1   bus     no   35    25    417    70     35    1\n#> 4          1   car    yes    0    10    180    30     35    1\n#> 5          2   air     no   64    58     68    68     30    2\n#> 6          2 train     no   44    31    354    84     30    2\n\nmod <- mlogit(choice ~ wait + gcost | income + size, TravelMode)\n\navg_slopes(mod, variables = c(\"income\", \"size\"))\n#> \n#>  Group   Term  Estimate Std. Error      z Pr(>|z|)    S     2.5 %   97.5 %\n#>  air   income  0.002786    0.00122  2.289  0.02208  5.5  0.000400  0.00517\n#>  bus   income -0.000372    0.00110 -0.338  0.73547  0.4 -0.002531  0.00179\n#>  car   income  0.003373    0.00137  2.456  0.01403  6.2  0.000682  0.00606\n#>  train income -0.005787    0.00132 -4.390  < 0.001 16.4 -0.008370 -0.00320\n#>  air   size   -0.126465    0.02892 -4.374  < 0.001 16.3 -0.183138 -0.06979\n#>  bus   size    0.011345    0.02587  0.439  0.66096  0.6 -0.039353  0.06204\n#>  car   size    0.045880    0.02476  1.853  0.06385  4.0 -0.002642  0.09440\n#>  train size    0.069240    0.02478  2.794  0.00521  7.6  0.020662  0.11782\n#> \n#> Columns: group, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNote that the slopes() function will always return estimates of zero for regressors before the vertical bar in the formula. This is because marginaleffects increments all rows of the prediction dataset in the same way to compute slopes and contrast. Because mlogit data are in “long” format, this means that alternatives are incremented in the same way, which does not produce alternative-specific changes in the predictors.\nOne strategy to circumvent this problem is to supply a data frame of numeric values to compare, with alternative specific changes. In this example, we test what happens to the probability of selecting each mode of transportation if we only increase the wait time of air travel:\n\naltspec <- data.frame(\n  low = TravelMode$wait,\n  high = ifelse(TravelMode$mode == \"air\", TravelMode$wait + 15, TravelMode$wait)\n)\n\navg_comparisons(mod, variables = list(wait = altspec))\n#> \n#>  Group Term Contrast Estimate Std. Error      z Pr(>|z|)     S   2.5 %  97.5 %\n#>  air   wait   manual  -0.1321    0.01070 -12.35   <0.001 114.0 -0.1531 -0.1111\n#>  bus   wait   manual   0.0251    0.00460   5.45   <0.001  24.2  0.0160  0.0341\n#>  car   wait   manual   0.0701    0.00834   8.41   <0.001  54.5  0.0538  0.0865\n#>  train wait   manual   0.0369    0.00528   6.99   <0.001  38.4  0.0266  0.0473\n#> \n#> Columns: group, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe can compute yet more kinds of marginal effects, we can construct customized data frames and feed them to the newdata argument of the slopes() function.\nIf we want to compute the slope of the response function (marginal effects) when each of the predictors is fixed to its global mean, we can do:\n\nnd <- TravelMode |>\n    summarize(across(c(\"wait\", \"gcost\", \"income\", \"size\"),\n              function(x) rep(mean(x), 4)))\nnd\n#>       wait    gcost   income     size\n#> 1 34.58929 110.8798 34.54762 1.742857\n#> 2 34.58929 110.8798 34.54762 1.742857\n#> 3 34.58929 110.8798 34.54762 1.742857\n#> 4 34.58929 110.8798 34.54762 1.742857\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#> \n#>  Group   Term  Estimate Std. Error     z Pr(>|z|)   S     2.5 %    97.5 %\n#>  air   income  6.66e-03   2.42e-03  2.75  0.00603 7.4  1.91e-03  1.14e-02\n#>  bus   income -1.14e-03   9.43e-04 -1.21  0.22638 2.1 -2.99e-03  7.08e-04\n#>  car   income  6.48e-06   2.02e-05  0.32  0.74893 0.4 -3.32e-05  4.62e-05\n#>  train income -5.52e-03   1.91e-03 -2.89  0.00383 8.0 -9.26e-03 -1.78e-03\n#>  air   size   -1.69e-01   5.88e-02 -2.88  0.00394 8.0 -2.85e-01 -5.43e-02\n#>  bus   size    4.67e-02   2.72e-02  1.72  0.08623 3.5 -6.65e-03  1.00e-01\n#>  car   size    1.36e-03   8.81e-04  1.54  0.12305 3.0 -3.68e-04  3.08e-03\n#>  train size    1.21e-01   4.45e-02  2.73  0.00634 7.3  3.42e-02  2.08e-01\n#> \n#> Columns: group, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nIf we want to compute marginal effects with the gcost and wait fixed at their mean value, conditional on the choice of transportation mode:\n\nnd <- TravelMode |>\n    group_by(mode) |>\n    summarize(across(c(\"wait\", \"gcost\", \"income\", \"size\"), mean))\nnd\n#> # A tibble: 4 × 5\n#>   mode   wait gcost income  size\n#>   <fct> <dbl> <dbl>  <dbl> <dbl>\n#> 1 air    61.0 103.    34.5  1.74\n#> 2 train  35.7 130.    34.5  1.74\n#> 3 bus    41.7 115.    34.5  1.74\n#> 4 car     0    95.4   34.5  1.74\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#> \n#>  Group   Term  Estimate Std. Error      z Pr(>|z|)    S     2.5 %   97.5 %\n#>  air   income  0.006015    0.00233  2.585  0.00975  6.7  0.001454  0.01058\n#>  bus   income -0.000713    0.00146 -0.489  0.62481  0.7 -0.003570  0.00214\n#>  car   income  0.005445    0.00229  2.382  0.01721  5.9  0.000965  0.00993\n#>  train income -0.010747    0.00256 -4.202  < 0.001 15.2 -0.015760 -0.00573\n#>  air   size   -0.232927    0.05659 -4.116  < 0.001 14.7 -0.343846 -0.12201\n#>  bus   size    0.020440    0.03436  0.595  0.55195  0.9 -0.046908  0.08779\n#>  car   size    0.067820    0.04123  1.645  0.09996  3.3 -0.012984  0.14862\n#>  train size    0.144668    0.04774  3.030  0.00244  8.7  0.051101  0.23823\n#> \n#> Columns: group, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nWe can also explore more complex alternatives. Here, for example, only one alternative is affected by cost reduction:\n\nnd <- datagrid(mode = TravelMode$mode, newdata = TravelMode)\nnd <- lapply(1:4, function(i) mutate(nd, gcost = ifelse(1:4 == i, 30, gcost)))\nnd <- bind_rows(nd)\nnd\n#>    individual choice wait vcost travel gcost income size  mode\n#> 1           1     no   35    48    486    30     35    2   air\n#> 2           1     no   35    48    486   111     35    2 train\n#> 3           1     no   35    48    486   111     35    2   bus\n#> 4           1     no   35    48    486   111     35    2   car\n#> 5           1     no   35    48    486   111     35    2   air\n#> 6           1     no   35    48    486    30     35    2 train\n#> 7           1     no   35    48    486   111     35    2   bus\n#> 8           1     no   35    48    486   111     35    2   car\n#> 9           1     no   35    48    486   111     35    2   air\n#> 10          1     no   35    48    486   111     35    2 train\n#> 11          1     no   35    48    486    30     35    2   bus\n#> 12          1     no   35    48    486   111     35    2   car\n#> 13          1     no   35    48    486   111     35    2   air\n#> 14          1     no   35    48    486   111     35    2 train\n#> 15          1     no   35    48    486   111     35    2   bus\n#> 16          1     no   35    48    486    30     35    2   car\n\navg_slopes(mod, newdata = nd, variables = c(\"income\", \"size\"))\n#> \n#>  Group   Term  Estimate Std. Error      z Pr(>|z|)    S     2.5 %    97.5 %\n#>  air   income  8.24e-03   2.46e-03  3.352   <0.001 10.3  3.42e-03  0.013058\n#>  bus   income -1.33e-03   1.30e-03 -1.020    0.308  1.7 -3.88e-03  0.001222\n#>  car   income  2.66e-05   4.31e-05  0.617    0.537  0.9 -5.79e-05  0.000111\n#>  train income -6.94e-03   1.86e-03 -3.735   <0.001 12.4 -1.06e-02 -0.003298\n#>  air   size   -2.12e-01   6.02e-02 -3.526   <0.001 11.2 -3.31e-01 -0.094365\n#>  bus   size    6.06e-02   3.79e-02  1.600    0.110  3.2 -1.37e-02  0.134911\n#>  car   size    2.38e-03   1.57e-03  1.512    0.131  2.9 -7.04e-04  0.005459\n#>  train size    1.49e-01   4.28e-02  3.489   <0.001 11.0  6.55e-02  0.233392\n#> \n#> Columns: group, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nImportant: The newdata argument for mlogit models must be a “balanced” data frame, that is, it must have a number of rows that is a multiple of the number of choices."
  },
  {
    "objectID": "articles/gformula.html#what-is-the-parametric-g-formula",
    "href": "articles/gformula.html#what-is-the-parametric-g-formula",
    "title": "\n11  Causal Inference\n",
    "section": "\n11.1 What is the parametric g-formula?",
    "text": "11.1 What is the parametric g-formula?\nThe parametric g-formula is a method of standardization which can be used to address confounding problems in causal inference with observational data. It relies on the same identification assumptions as Inverse Probability Weighting (IPW), but uses different modeling assumptions. Whereas IPW models the treatment equation, standardization models the mean outcome equation. As Hernán and Robins note:\n\n“Both IP weighting and standardization are estimators of the g-formula, a general method for causal inference first described in 1986. … We say that standardization is a”plug-in g-formula estimator” because it simply replaces the conditional mean outcome in the g-formula by its estimates. When, like in Chapter 13, those estimates come from parametric models, we refer to the method as the parametric g-formula.”"
  },
  {
    "objectID": "articles/gformula.html#how-does-it-work",
    "href": "articles/gformula.html#how-does-it-work",
    "title": "\n11  Causal Inference\n",
    "section": "\n11.2 How does it work?",
    "text": "11.2 How does it work?\nImagine a causal model like this:\n\n##| echo = FALSE,\n##| message = FALSE,\n##| align = \"center\",\n##| fig.width = 4,\n##| fig.height = 3\nlibrary(ggdag)\n\n\nAttaching package: 'ggdag'\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\ncoords <- list(\n  x = c(X = 0, Y = 2, W = 1),\n  y = c(X = 0, Y = 0, W = 1))\nd <- dagify(Y ~ X + W,\n            X ~ W,\n            coords = coords)\nggdag(d) + theme_dag()\n\n\n\n\nWe want to estimate the effect of a binary treatment \\(X\\) on outcome \\(Y\\), but there is a confounding variable \\(W\\). We can use standardization with the parametric g-formula to handle this. Roughly speaking, the procedure is as follows:\n\nUse the observed data to fit a regression model with \\(Y\\) as outcome, \\(X\\) as treatment, and \\(W\\) as control variable (with perhaps some polynomials and/or interactions if there are multiple control variables).\nCreate a new dataset exactly identical to the original data, but where \\(X=1\\) in every row.\nCreate a new dataset exactly identical to the original data, but where \\(X=0\\) in every row.\nUse the model from Step 1 to compute adjusted predictions in the two counterfactual datasets from Steps 2 and 3.\nThe quantity of interest is the difference between the means of adjusted predictions in the two counterfactual datasets.\n\nThis is equivalent to computing an “Average Contrast”, in which the value of \\(X\\) moves from 0 to 1. Thanks to this equivalence, we can apply the parametric g-formula method using a single line of code in marginaleffects, and obtain delta method standard errors automatically."
  },
  {
    "objectID": "articles/gformula.html#example-with-real-world-data",
    "href": "articles/gformula.html#example-with-real-world-data",
    "title": "\n11  Causal Inference\n",
    "section": "\n11.3 Example with real-world data",
    "text": "11.3 Example with real-world data\nLet’s illustrate this method by replicating an example from Chapter 13 of Hernán and Robins. The data come from the National Health and Nutrition Examination Survey Data I Epidemiologic Follow-up Study (NHEFS). The outcome is wt82_71, a measure of weight gain. The treatment is qsmk, a binary measure of smoking cessation. There are many confounders.\nStep 1 is to fit a regression model of the outcome on the treatment and control variables:\n\nlibrary(boot)\nlibrary(marginaleffects)\n\nf <- wt82_71 ~ qsmk + sex + race + age + I(age * age) + factor(education) +\n     smokeintensity + I(smokeintensity * smokeintensity) + smokeyrs +\n     I(smokeyrs * smokeyrs) + factor(exercise) + factor(active) + wt71 +\n     I(wt71 * wt71) + I(qsmk * smokeintensity)\n\nurl <- \"https://raw.githubusercontent.com/vincentarelbundock/modelarchive/main/data-raw/nhefs.csv\"\nnhefs <- read.csv(url)\nnhefs <- na.omit(nhefs[, all.vars(f)])\n\nfit <- glm(f, data = nhefs)\n\nSteps 2 and 3 require us to replicate the full dataset by setting the qsmk treatment to counterfactual values. We can do this automatically by calling comparisons().\n\n11.3.1 TLDR\nThese simple commands do everything we need to apply the parametric g-formula:\n\navg_comparisons(fit, variables = list(qsmk = 0:1))\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   <0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nThe rest of the vignette walks through the process in a bit more detail and compares to replication code from Hernán and Robins.\n\n11.3.2 Adjusted Predictions\nWe can compute average predictions in the original data, and average predictions in the two counterfactual datasets like this:\n\n## average predicted outcome in the original data\np <- predictions(fit)\nmean(p$estimate)\n\n[1] 2.6383\n\n## average predicted outcome in the two counterfactual datasets\np <- predictions(fit, newdata = datagrid(qsmk = 0:1, grid_type = \"counterfactual\"))\naggregate(estimate ~ qsmk, data = p, FUN = mean)\n\n  qsmk estimate\n1    0 1.756213\n2    1 5.273587\n\n\nIn the R code that accompanies their book, Hernán and Robins compute the same quantities manually, as follows:\n\n## create a dataset with 3 copies of each subject\nnhefs$interv <- -1 # 1st copy: equal to original one\n\ninterv0 <- nhefs # 2nd copy: treatment set to 0, outcome to missing\ninterv0$interv <- 0\ninterv0$qsmk <- 0\ninterv0$wt82_71 <- NA\n\ninterv1 <- nhefs # 3rd copy: treatment set to 1, outcome to missing\ninterv1$interv <- 1\ninterv1$qsmk <- 1\ninterv1$wt82_71 <- NA\n\nonesample <- rbind(nhefs, interv0, interv1) # combining datasets\n\n## linear model to estimate mean outcome conditional on treatment and confounders\n## parameters are estimated using original observations only (nhefs)\n## parameter estimates are used to predict mean outcome for observations with \n## treatment set to 0 (interv=0) and to 1 (interv=1)\n\nstd <- glm(f, data = onesample)\nonesample$predicted_meanY <- predict(std, onesample)\n\n## estimate mean outcome in each of the groups interv=0, and interv=1\n## this mean outcome is a weighted average of the mean outcomes in each combination \n## of values of treatment and confounders, that is, the standardized outcome\nmean(onesample[which(onesample$interv == -1), ]$predicted_meanY)\n\n[1] 2.6383\n\nmean(onesample[which(onesample$interv == 0), ]$predicted_meanY)\n\n[1] 1.756213\n\nmean(onesample[which(onesample$interv == 1), ]$predicted_meanY)\n\n[1] 5.273587\n\n\nIt may be useful to note that the datagrid() function provided by marginaleffects can create counterfactual datasets automatically. This is equivalent to the onesample dataset:\n\nnd <- datagrid(\n    model = fit,\n    qsmk = c(0, 1),\n    grid_type = \"counterfactual\")\n\n\n11.3.3 Contrast\nNow we want to compute the treatment effect with the parametric g-formula, which is the difference in average predicted outcomes in the two counterfactual datasets. This is equivalent to taking the average contrast with the comparisons() function. There are three important things to note in the command that follows:\n\nThe variables argument is used to indicate that we want to estimate a “contrast” between adjusted predictions when qsmk is equal to 1 or 0.\n\ncomparisons() automatically produces estimates of uncertainty.\n\n\navg_comparisons(std, variables = list(qsmk = 0:1))\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   <0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n\nUnder the hood, comparisons() did exactly what we described in the g-formula steps above:\nWe can obtain the same result by manually computing the quantities, using the replication code from Hernán and Robins:\n\nmean(onesample[which(onesample$interv == 1), ]$predicted_meanY) -\nmean(onesample[which(onesample$interv == 0), ]$predicted_meanY)\n\n[1] 3.517374\n\n\nAlthough manual computation is simple, it does not provide uncertainty estimates. In contrast, comparisons() has already computed the standard error and confidence interval using the delta method.\nInstead of the delta method, most analysts will rely on bootstrapping. For example, the replication code from Hernán and Robins does this:\n\n## function to calculate difference in means\nstandardization <- function(data, indices) {\n    # create a dataset with 3 copies of each subject\n    d <- data[indices, ] # 1st copy: equal to original one`\n    d$interv <- -1\n    d0 <- d # 2nd copy: treatment set to 0, outcome to missing\n    d0$interv <- 0\n    d0$qsmk <- 0\n    d0$wt82_71 <- NA\n    d1 <- d # 3rd copy: treatment set to 1, outcome to missing\n    d1$interv <- 1\n    d1$qsmk <- 1\n    d1$wt82_71 <- NA\n    d.onesample <- rbind(d, d0, d1) # combining datasets\n\n    # linear model to estimate mean outcome conditional on treatment and confounders\n    # parameters are estimated using original observations only (interv= -1)\n    # parameter estimates are used to predict mean outcome for observations with set\n    # treatment (interv=0 and interv=1)\n    fit <- glm(f, data = d.onesample)\n\n    d.onesample$predicted_meanY <- predict(fit, d.onesample)\n\n    # estimate mean outcome in each of the groups interv=-1, interv=0, and interv=1\n    return(mean(d.onesample$predicted_meanY[d.onesample$interv == 1]) -\n           mean(d.onesample$predicted_meanY[d.onesample$interv == 0]))\n}\n\n## bootstrap\nresults <- boot(data = nhefs, statistic = standardization, R = 1000)\n\n## generating confidence intervals\nse <- sd(results$t[, 1])\nmeant0 <- results$t0\nll <- meant0 - qnorm(0.975) * se\nul <- meant0 + qnorm(0.975) * se\n\nbootstrap <- data.frame(\n    \" \" = \"Treatment - No Treatment\",\n    estimate = meant0,\n    std.error = se,\n    conf.low = ll,\n    conf.high = ul,\n    check.names = FALSE)\nbootstrap\n\n                           estimate std.error conf.low conf.high\n1 Treatment - No Treatment 3.517374 0.4935173 2.550098   4.48465\n\n\nThe results are close to those that we obtained with comparisons(), but the confidence interval differs slightly because of the difference between bootstrapping and the delta method.\n\navg_comparisons(fit, variables = list(qsmk = 0:1))\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n qsmk    1 - 0     3.52       0.44 7.99   <0.001 49.4  2.65   4.38\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/elasticity.html",
    "href": "articles/elasticity.html",
    "title": "\n12  Elasticity\n",
    "section": "",
    "text": "The content of this vignette requires version 0.7.0 of the marginaleffects package.\nIn some contexts, it is useful to interpret the results of a regression model in terms of elasticity or semi-elasticity. One strategy to achieve that is to estimate a log-log or a semilog model, where the left and/or right-hand side variables are logged. Another approach is to note that \\(\\frac{\\partial ln(x)}{\\partial x}=\\frac{1}{x}\\), and to post-process the marginal effects to transform them into elasticities or semi-elasticities.\nFor example, say we estimate a linear model of this form:\n\\[y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\varepsilon\\]\nLet \\(\\hat{y}\\) be the adjusted prediction made by the model for some combination of covariates \\(x_1\\) and \\(x_2\\). The slope with respect to \\(x_1\\) (or “marginal effect”) is:\n\\[\\frac{\\partial \\hat{y}}{\\partial x_1}\\]\nWe can estimate the “eyex”, “eydx”, and “dyex” (semi-)elasticities with respect to \\(x_1\\) as follows:\n\\[\n\\eta_1=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot \\frac{x_1}{\\hat{y}}\\\\\n\\eta_2=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot \\frac{1}{\\hat{y}} \\\\\n\\eta_3=\\frac{\\partial \\hat{y}}{\\partial x_1}\\cdot x_1,\n\\]\nwith interpretations roughly as follows:\n\nA percentage point increase in \\(x_1\\) is associated to a \\(\\eta_1\\) percentage points increase in \\(y\\).\nA unit increase in \\(x_1\\) is associated to a \\(\\eta_2\\) percentage points increase in \\(y\\).\nA percentage point increase in \\(x_1\\) is associated to a \\(\\eta_3\\) units increase in \\(y\\).\n\nFor further intuition, consider the ratio of change in \\(y\\) to change in \\(x\\): \\(\\frac{\\Delta y}{\\Delta x}\\). We can turn this ratio into a ratio between relative changes by dividing both the numerator and the denominator: \\(\\frac{\\frac{\\Delta y}{y}}{\\frac{\\Delta x}{x}}\\). This is of course linked to the expression for the \\(\\eta_1\\) elasticity above.\nWith the marginaleffects package, these quantities are easy to compute:\n\nlibrary(marginaleffects)\nmod <- lm(mpg ~ hp + wt, data = mtcars)\n\navg_slopes(mod)\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %\n#>    hp  -0.0318    0.00903 -3.52   <0.001 11.2 -0.0495 -0.0141\n#>    wt  -3.8778    0.63273 -6.13   <0.001 30.1 -5.1180 -2.6377\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_slopes(mod, slope = \"eyex\")\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>    hp    eY/eX   -0.285     0.0855 -3.34   <0.001 10.2 -0.453 -0.118\n#>    wt    eY/eX   -0.746     0.1418 -5.26   <0.001 22.7 -1.024 -0.468\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_slopes(mod, slope = \"eydx\")\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S    2.5 %    97.5 %\n#>    hp    eY/dX -0.00173   0.000502 -3.46   <0.001 10.8 -0.00272 -0.000751\n#>    wt    eY/dX -0.21165   0.037850 -5.59   <0.001 25.4 -0.28583 -0.137463\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_slopes(mod, slope = \"dyex\")\n#> \n#>  Term Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>    hp    dY/eX    -4.66       1.32 -3.52   <0.001 11.2  -7.26  -2.06\n#>    wt    dY/eX   -12.48       2.04 -6.13   <0.001 30.1 -16.47  -8.49\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/experiments.html#analyzing-2x2-experiments-with-marginaleffects",
    "href": "articles/experiments.html#analyzing-2x2-experiments-with-marginaleffects",
    "title": "\n13  Experiments\n",
    "section": "\n13.1 Analyzing 2x2 Experiments with marginaleffects\n",
    "text": "13.1 Analyzing 2x2 Experiments with marginaleffects\n\nA 2×2 factorial design is a type of experimental design that allows researchers to understand the effects of two independent variables (each with two levels) on a single dependent variable. The design is popular among academic researchers as well as in industry when running A/B tests.\nTo illustrate how to analyze these designs with marginaleffects, we will use the mtcars dataset. We’ll analyze fuel efficiency, mpg (miles per gallon), as a function of am (transmission type) and vs (engine shape).\nvs is an indicator variable for if the car has a straight engine (1 = straight engine, 0 = V-shaped). am is an indicator variable for if the car has manual transmission (1 = manual transmission, 0=automatic transmission). There are then four types of cars (1 type for each of the four combinations of binary indicators).\n\n13.1.1 Fitting a Model\nLet’s start by creating a model for fuel efficiency. For simplicity, we’ll use linear regression and model the interaction between vs and am.\n\nlibrary(tidyverse)\nlibrary(marginaleffects)\n\n## See ?mtcars for variable definitions\nfit <- lm(mpg ~ vs + am + vs:am, data=mtcars) # equivalent to ~ vs*am\n\nWe can plot the predictions from the model using the plot_predictions() function. From the plot below, we can see a few things:\n\nStraight engines (vs=1) are estimated to have better expected fuel efficiency than V-shaped engines (vs=0).\nManual transmissions (am=1) are estimated to have better fuel efficiency for both V-shaped and straight engines.\nFor straight engines, the effect of manual transmissions on fuel efficiency seems to increase.\n\n\nplot_predictions(fit, by = c(\"vs\", \"am\"))\n\n\n\n\n\n13.1.2 Evaluating Effects From The Model Summary\nSince this model is fairly simple the estimated differences between any of the four possible combinations of vs and am can be read from summary(fit) with a little arithmetic. The estimated model is\n\\[ \\mbox{mpg} = 20.743 + 5.693 \\cdot \\mbox{vs} + 4.700 \\cdot \\mbox{am} + 2.929 \\cdot \\mbox{vs} \\cdot \\mbox{am} \\>. \\]\nThe estimated differences in fuel efficiency are:\n\n5.693 mpg between straight engines and V-shaped engines when the car has automatic transmission.\n4.700 mpg between manual transmissions and automatic transmissions when the car has a V-shaped engine.\n7.629 mpg between manual transmissions and automatic transmissions when the car has a straight engine.\n13.322 mpg between manual transmissions with straight engines and automatic transmissions with V-shaped engines.\n\nReading off these differences from the model summary becomes more difficult as more variables are added (not to mention obtaining their estimated standard errors becomes nightmarish). To make the process easier on ourselves, we can leverage the avg_comparisons() function to get the same estimates and their uncertainty.\n\n13.1.3 Using avg_comparisons() To Estimate All Differences\nNote that the dot in the grey rectangle is the estimated fuel efficiency when vs=0 and am=0 (that is, for an automatic transmission car with V-shaped engine).\n\n\n\n\n\nLet’s use avg_comparisons() to get the difference between straight engines and V-shaped engines when the car has automatic transmission. The call to avg_comparisons() is shown below and results in the same estimate we made directly from the model. The contrast corresponding to this estimate is shown in the plot below.\n\navg_comparisons(fit,\n  newdata = datagrid(am = 0),\n  variables = \"vs\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>    vs    1 - 0     5.69       1.65 3.45   <0.001 10.8  2.46   8.93\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\n\n\n\n\nThe next difference is between manual transmissions and automatic transmissions when the car has a V-shaped engine. Again, the call to avg_comparisons() is shown below, and the corresponding contrast is indicated in the plot below using an arrow.\n\navg_comparisons(fit,\n  newdata = datagrid(vs = 0),\n  variables = \"am\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %\n#>    am    1 - 0      4.7       1.74 2.71  0.00678 7.2   1.3    8.1\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\n\n\n\n\nThe third difference we estimated was between manual transmissions and automatic transmissions when the car has a straight engine. The model call and contrast are\n\navg_comparisons(fit,\n  newdata = datagrid(vs = 1),\n  variables = \"am\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>    am    1 - 0     7.63       1.86 4.11   <0.001 14.6  3.99   11.3\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\n\n\n\n\nAnd the last difference and contrast between manual transmissions with straight engines and automatic transmissions with V-shaped engines is\n\navg_comparisons(fit,\n  newdata = datagrid(\"vs\", \"am\"),\n  variables = c(\"am\", \"vs\"),\n  cross = TRUE)\n#> \n#>  C: am C: vs Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>  1 - 0 1 - 0     13.3       1.65 8.07   <0.001 50.3  10.1   16.6\n#> \n#> Columns: rowid, term, contrast_am, contrast_vs, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\n\n\n\n\n\n13.1.4 Conclusion\nThe 2x2 design is a very popular design, and when using a linear model, the estimated differences between groups can be directly read off from the model summary, if not with a little arithmetic. However, when using models with a non-identity link function, or when seeking to obtain the standard errors for estimated differences, things become considerably more difficult. This vignette showed how to use avg_comparisons() to specify contrasts of interests and obtain standard errors for those differences. The approach used applies to all generalized linear models and effects can be further stratified using the by argument (although this is not shown in this vignette.)"
  },
  {
    "objectID": "articles/experiments.html#regression-adjustment-in-experiments",
    "href": "articles/experiments.html#regression-adjustment-in-experiments",
    "title": "\n13  Experiments\n",
    "section": "\n13.2 Regression adjustment in experiments",
    "text": "13.2 Regression adjustment in experiments\nMany analysts who conduct and analyze experiments wish to use regression adjustment with a linear regression model to improve the precision of their estimate of the treatment effect. Unfortunately, regression adjustment can introduce small-sample bias and other undesirable properties (Freedman 2008). Lin (2013) proposes a simple strategy to fix these problems in sufficiently large samples:\n\nCenter all predictors by subtracting each of their means.\nEstimate a linear model in which the treatment is interacted with each of the covariates.\n\nThe estimatr package includes a convenient function to implement this strategy:\n\nlibrary(estimatr)\nlibrary(marginaleffects)\nlalonde <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/MatchIt/lalonde.csv\")\n\nmod <- lm_lin(\n    re78 ~ treat,\n    covariates = ~ age + educ + race,\n    data = lalonde,\n    se_type = \"HC3\")\nsummary(mod)\n#> \n#> Call:\n#> lm_lin(formula = re78 ~ treat, covariates = ~age + educ + race, \n#>     data = lalonde, se_type = \"HC3\")\n#> \n#> Standard error type:  HC3 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value  Pr(>|t|) CI Lower CI Upper  DF\n#> (Intercept)         6488.05     356.71 18.1885 2.809e-59  5787.50   7188.6 604\n#> treat                489.73     878.52  0.5574 5.774e-01 -1235.59   2215.0 604\n#> age_c                 85.88      35.42  2.4248 1.561e-02    16.32    155.4 604\n#> educ_c               464.04     131.51  3.5286 4.495e-04   205.77    722.3 604\n#> racehispan_c        2775.47    1155.40  2.4022 1.660e-02   506.38   5044.6 604\n#> racewhite_c         2291.67     793.30  2.8888 4.006e-03   733.71   3849.6 604\n#> treat:age_c           17.23      76.37  0.2256 8.216e-01  -132.75    167.2 604\n#> treat:educ_c         226.71     308.43  0.7350 4.626e-01  -379.02    832.4 604\n#> treat:racehispan_c -1057.84    2652.42 -0.3988 6.902e-01 -6266.92   4151.2 604\n#> treat:racewhite_c  -1205.68    1805.21 -0.6679 5.045e-01 -4750.92   2339.6 604\n#> \n#> Multiple R-squared:  0.05722 ,   Adjusted R-squared:  0.04317 \n#> F-statistic: 4.238 on 9 and 604 DF,  p-value: 2.424e-05\n\nWe can obtain the same results by fitting a model with the standard lm function and using the comparisons() function:\n\nmod <- lm(re78 ~ treat * (age + educ + race), data = lalonde)\navg_comparisons(\n    mod,\n    variables = \"treat\",\n    vcov = \"HC3\")\n#> \n#>   Term Contrast Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  treat    1 - 0      490        879 0.557    0.577 0.8 -1232   2212\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNotice that the treat coefficient and associate standard error in the lm_lin regression are exactly the same as the estimates produced by the comparisons() function.\n\n13.2.1 References\n\nFreedman, David A. “On Regression Adjustments to Experimental Data.” Advances in Applied Mathematics 40, no. 2 (February 2008): 180–93.\nLin, Winston. “Agnostic Notes on Regression Adjustments to Experimental Data: Reexamining Freedman’s Critique.” Annals of Applied Statistics 7, no. 1 (March 2013): 295–318. https://doi.org/10.1214/12-AOAS583."
  },
  {
    "objectID": "articles/gam.html#estimate-a-generalized-additive-model",
    "href": "articles/gam.html#estimate-a-generalized-additive-model",
    "title": "\n14  GAM\n",
    "section": "\n14.1 Estimate a Generalized Additive Model",
    "text": "14.1 Estimate a Generalized Additive Model\nWe will estimate a GAM model using the mgcv package and the simdat dataset distributed with the itsadug package:\n\nlibrary(marginaleffects)\nlibrary(itsadug)\nlibrary(mgcv)\n\nsimdat$Subject <- as.factor(simdat$Subject)\n\ndim(simdat)\n#> [1] 75600     6\nhead(simdat)\n#>    Group      Time Trial Condition Subject         Y\n#> 1 Adults   0.00000   -10        -1     a01 0.7554469\n#> 2 Adults  20.20202   -10        -1     a01 2.7834759\n#> 3 Adults  40.40404   -10        -1     a01 1.9696963\n#> 4 Adults  60.60606   -10        -1     a01 0.6814298\n#> 5 Adults  80.80808   -10        -1     a01 1.6939195\n#> 6 Adults 101.01010   -10        -1     a01 2.3651969\n\nFit a model with a random effect and group-time smooths:\n\nmodel <- bam(Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\"),\n             data = simdat)\n\nsummary(model)\n#> \n#> Family: gaussian \n#> Link function: identity \n#> \n#> Formula:\n#> Y ~ Group + s(Time, by = Group) + s(Subject, bs = \"re\")\n#> \n#> Parametric coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)   \n#> (Intercept)   2.0574     0.6903   2.980  0.00288 **\n#> GroupAdults   3.1265     0.9763   3.202  0.00136 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Approximate significance of smooth terms:\n#>                         edf Ref.df    F p-value    \n#> s(Time):GroupChildren  8.26  8.850 3649  <2e-16 ***\n#> s(Time):GroupAdults    8.66  8.966 6730  <2e-16 ***\n#> s(Subject)            33.94 34.000  569  <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> R-sq.(adj) =  0.609   Deviance explained =   61%\n#> fREML = 2.3795e+05  Scale est. = 31.601    n = 75600"
  },
  {
    "objectID": "articles/gam.html#adjusted-predictions-predictions-and-plot_predictions",
    "href": "articles/gam.html#adjusted-predictions-predictions-and-plot_predictions",
    "title": "\n14  GAM\n",
    "section": "\n14.2 Adjusted Predictions: predictions() and plot_predictions()\n",
    "text": "14.2 Adjusted Predictions: predictions() and plot_predictions()\n\nCompute adjusted predictions for each observed combination of regressor in the dataset used to fit the model. This gives us a dataset with the same number of rows as the original data, but new columns with predicted values and uncertainty estimates:\n\npred <- predictions(model)\ndim(pred)\n#> [1] 75600    12\nhead(pred)\n#> \n#>  Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %\n#>    -1.874      0.199 -9.41   <0.001 67.4 -2.2643 -1.4834\n#>    -1.346      0.182 -7.41   <0.001 42.8 -1.7025 -0.9901\n#>    -0.819      0.167 -4.90   <0.001 20.0 -1.1467 -0.4916\n#>    -0.293      0.156 -1.88   0.0605  4.0 -0.5988  0.0129\n#>     0.231      0.149  1.55   0.1204  3.1 -0.0606  0.5232\n#>     0.753      0.146  5.17   <0.001 22.0  0.4675  1.0379\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Y, Group, Time, Subject\n\nWe can easily plot adjusted predictions for different values of a regressor using the plot_predictions() function:\n\nplot_predictions(model, condition = \"Time\")"
  },
  {
    "objectID": "articles/gam.html#marginal-effects-slopes-and-plot_slopes",
    "href": "articles/gam.html#marginal-effects-slopes-and-plot_slopes",
    "title": "\n14  GAM\n",
    "section": "\n14.3 Marginal Effects: slopes() and plot_slopes()\n",
    "text": "14.3 Marginal Effects: slopes() and plot_slopes()\n\nMarginal effects are slopes of the prediction equation. They are an observation-level quantity. The slopes() function produces a dataset with the same number of rows as the original data, but with new columns for the slop and uncertainty estimates:\n\nmfx <- slopes(model, variables = \"Time\")\nhead(mfx)\n#> \n#>  Term Estimate Std. Error    z Pr(>|z|)     S  2.5 % 97.5 %\n#>  Time   0.0261    0.00137 19.1   <0.001 267.8 0.0234 0.0288\n#>  Time   0.0261    0.00136 19.2   <0.001 270.3 0.0234 0.0288\n#>  Time   0.0261    0.00133 19.5   <0.001 280.0 0.0235 0.0287\n#>  Time   0.0260    0.00128 20.3   <0.001 301.3 0.0235 0.0285\n#>  Time   0.0259    0.00120 21.6   <0.001 339.8 0.0235 0.0282\n#>  Time   0.0257    0.00109 23.5   <0.001 404.7 0.0236 0.0279\n#> \n#> Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, Y, Group, Time, Subject\n\nWe can plot marginal effects for different values of a regressor using the plot_slopes() function. This next plot shows the slope of the prediction equation, that is, the slope of the previous plot, at every value of the Time variable.\n\nplot_slopes(model, variables = \"Time\", condition = \"Time\")\n\n\n\n\nThe marginal effects in this plot can be interpreted as measuring the change in Y that is associated with a small increase in Time, for different baseline values of Time."
  },
  {
    "objectID": "articles/gam.html#excluding-terms",
    "href": "articles/gam.html#excluding-terms",
    "title": "\n14  GAM\n",
    "section": "\n14.4 Excluding terms",
    "text": "14.4 Excluding terms\nThe predict() method of the mgcv package allows users to “exclude” some smoothing terms, using the exclude argument. You can pass the same argument to any function in the marginaleffects package:\n\npredictions(model, newdata = \"mean\", exclude = \"s(Subject)\")\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  Group Time Subject\n#>      11.7      0.695 16.9   <0.001 210.8  10.4   13.1 Adults 1000     a01\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, Y, Group, Time, Subject\n\nSee the documentation in ?mgcv:::predict.bam for details."
  },
  {
    "objectID": "articles/logit.html#data",
    "href": "articles/logit.html#data",
    "title": "\n15  Logit\n",
    "section": "\n15.1 Data",
    "text": "15.1 Data\nWe focus on subset data from the GUSTO-I study, where patients were randomly assigned to accelerated tissue plasminogen activator (tPA) or streptokinase (SK).\nLoad libraries, data and fit a covariate-adjusted logistic regression model.\n\nlibrary(marginaleffects)\nlibrary(modelsummary)\nlibrary(ggplot2)\nlibrary(rms)\n\nload(url(\n\"https://github.com/vincentarelbundock/modelarchive/raw/main/data-raw/gusto.rda\"\n))\n\ngusto <- subset(gusto, tx %in% c(\"tPA\", \"SK\"))\ngusto$tx <- factor(gusto$tx, levels = c(\"tPA\", \"SK\"))\n\nmod <- glm(\n    day30 ~ tx + rcs(age, 4) + Killip + pmin(sysbp, 120) + lsp(pulse, 50) +\n    pmi + miloc + sex, family = \"binomial\",\n    data = gusto)\n\n\n15.1.1 One-Number Summaries\nAs usual, we can produce a one-number summary of the relationship of interest by exponentiating the coefficients, which yields an Odds Ratio (OR):\n\nmodelsummary(mod, exponentiate = TRUE, coef_omit = \"^(?!txSK)\") \n\n\n\n\n   \n     (1) \n  \n\n\n txSK \n    1.230 \n  \n\n  \n    (0.065) \n  \n\n Num.Obs. \n    30510 \n  \n\n AIC \n    12428.6 \n  \n\n BIC \n    12553.5 \n  \n\n Log.Lik. \n    −6199.317 \n  \n\n F \n    173.216 \n  \n\n RMSE \n    0.24 \n  \n\n\n\n\nUnlike ORs, adjusted risk differences vary from individual to individual based on the values of the control variables. The comparisons() function can compute adjusted risk differences for every individual. Here, we display only the first 6 of them:\n\ncomparisons(\n    mod,\n    variables = \"tx\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)   S    2.5 %  97.5 %\n#>    tx SK - tPA 0.001074   0.000497 2.16  0.03060 5.0 0.000101 0.00205\n#>    tx SK - tPA 0.000857   0.000380 2.26  0.02410 5.4 0.000112 0.00160\n#>    tx SK - tPA 0.001780   0.000779 2.29  0.02229 5.5 0.000253 0.00331\n#>    tx SK - tPA 0.001137   0.000500 2.27  0.02302 5.4 0.000157 0.00212\n#>    tx SK - tPA 0.001366   0.000594 2.30  0.02143 5.5 0.000202 0.00253\n#> --- 30500 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#>    tx SK - tPA 0.002429   0.000808 3.00  0.00266  8.6 0.000844 0.00401\n#>    tx SK - tPA 0.012130   0.003900 3.11  0.00187  9.1 0.004486 0.01977\n#>    tx SK - tPA 0.036812   0.010361 3.55  < 0.001 11.4 0.016505 0.05712\n#>    tx SK - tPA 0.022969   0.006976 3.29  < 0.001 10.0 0.009297 0.03664\n#>    tx SK - tPA 0.049707   0.012843 3.87  < 0.001 13.2 0.024535 0.07488\n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, day30, tx, age, Killip, sysbp, pulse, pmi, miloc, sex\n\nPopulation-averaged (aka “marginal”) adjusted risk difference (see this vignette) can be obtained using the avg_*() functions or using the by argument:\n\navg_comparisons(mod, variables = \"tx\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S   2.5 % 97.5 %\n#>    tx SK - tPA   0.0111    0.00277 4.01   <0.001 14.0 0.00566 0.0165\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nThe comparisons() function above computed the predicted probability of mortality (day30==1) for each observed row of the data in two counterfactual cases: when tx is “SK”, and when tx is “tPA”. Then, it computed the differences between these two sets of predictions. Finally, it computed the population-average of risk differences.\nInstead of risk differences, we could compute population-averaged (marginal) adjusted risk ratios:\n\navg_comparisons(\n    mod,\n    variables = \"tx\",\n    comparison = \"lnratioavg\",\n    transform = exp)\n#> \n#>  Term                 Contrast Estimate Pr(>|z|)    S 2.5 % 97.5 %\n#>    tx ln(mean(SK) / mean(tPA))     1.18   <0.001 13.3  1.08   1.28\n#> \n#> Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nPopulation-averaged (marginal) odds ratios:\n\navg_comparisons(\n    mod,\n    variables = \"tx\",\n    comparison = \"lnoravg\",\n    transform = \"exp\")\n#> \n#>  Term                 Contrast Estimate Pr(>|z|)    S 2.5 % 97.5 %\n#>    tx ln(odds(SK) / odds(tPA))     1.19   <0.001 13.4  1.09    1.3\n#> \n#> Columns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\n15.1.2 Unit-level Summaries\nInstead of estimating one-number summaries, we can focus on unit-level proportion differences using comparisons(). This function applies the fitted logistic regression model to predict outcome probabilities for each patient, i.e., unit-level.\n\ncmp <- comparisons(mod, variables = \"tx\")\ncmp\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)   S    2.5 %  97.5 %\n#>    tx SK - tPA 0.001074   0.000497 2.16  0.03060 5.0 0.000101 0.00205\n#>    tx SK - tPA 0.000857   0.000380 2.26  0.02410 5.4 0.000112 0.00160\n#>    tx SK - tPA 0.001780   0.000779 2.29  0.02229 5.5 0.000253 0.00331\n#>    tx SK - tPA 0.001137   0.000500 2.27  0.02302 5.4 0.000157 0.00212\n#>    tx SK - tPA 0.001366   0.000594 2.30  0.02143 5.5 0.000202 0.00253\n#> --- 30500 rows omitted. See ?avg_comparisons and ?print.marginaleffects --- \n#>    tx SK - tPA 0.002429   0.000808 3.00  0.00266  8.6 0.000844 0.00401\n#>    tx SK - tPA 0.012130   0.003900 3.11  0.00187  9.1 0.004486 0.01977\n#>    tx SK - tPA 0.036812   0.010361 3.55  < 0.001 11.4 0.016505 0.05712\n#>    tx SK - tPA 0.022969   0.006976 3.29  < 0.001 10.0 0.009297 0.03664\n#>    tx SK - tPA 0.049707   0.012843 3.87  < 0.001 13.2 0.024535 0.07488\n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, day30, tx, age, Killip, sysbp, pulse, pmi, miloc, sex\n\nShow the predicted probability for individual patients under both treatment alternatives.\n\nggplot(cmp, aes(predicted_hi, predicted_lo)) +\n  geom_point() +\n  geom_abline(slope = 1, intercept = 0, linetype = 3) +\n  coord_fixed() +\n  labs(x = \"SK\", y = \"tPA\")\n\n\n\n\nWe can present the entire distribution of unit-level proportion differences an a cumulative distribution function:\n\nggplot(cmp, aes(estimate)) + stat_ecdf()\n\n\n\n\nOr the same information as a histogram with the mean and median.\n\nggplot(cmp, aes(estimate)) +\n  geom_histogram(bins = 100) +\n  geom_vline(xintercept = mean(cmp$estimate), color = \"orange\") +\n  geom_vline(xintercept = median(cmp$estimate), color = \"darkgreen\") +\n  labs(x = \"SK - TPA\", title = \"Distribution of unit-level contrasts\")\n\n\n\n\n\n15.1.3 Appendix\ncomparisons() performed the following calculations under the hood:\n\nd  <- gusto\n\nd$tx = \"SK\"\npredicted_hi <- predict(mod, newdata = d, type = \"response\")\n\nd$tx = \"tPA\"\npredicted_lo <- predict(mod, newdata = d, type = \"response\")\n\ncomparison <- predicted_hi - predicted_lo\n\nThe original dataset contains 30510 patients, thus comparisons() generates an output with same amount of rows.\n\nnrow(gusto)\n#> [1] 30510\n\n\nnrow(cmp)\n#> [1] 30510"
  },
  {
    "objectID": "articles/lme4.html",
    "href": "articles/lme4.html",
    "title": "\n16  Mixed Effects\n",
    "section": "",
    "text": "This vignette replicates some of the analyses in this excellent blog post by Solomon Kurz: Use emmeans() to include 95% CIs around your lme4-based fitted lines\nLoad libraries and fit two models of chick weights:\n\nlibrary(lme4)\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(marginaleffects)\n\n## unconditional linear growth model\nfit1 <- lmer(\n  weight ~ 1 + Time + (1 + Time | Chick),\n  data = ChickWeight)\n\n## conditional quadratic growth model\nfit2 <- lmer(\n  weight ~ 1 + Time + I(Time^2) + Diet + Time:Diet + I(Time^2):Diet + (1 + Time + I(Time^2) | Chick),\n  data = ChickWeight)\n\n\n16.0.1 Unit-level predictions\nPredict weight of each chick over time:\n\npred1 <- predictions(fit1,\n                     newdata = datagrid(Chick = ChickWeight$Chick,\n                                        Time = 0:21))\n\np1 <- ggplot(pred1, aes(Time, estimate, level = Chick)) +\n      geom_line() +\n      labs(y = \"Predicted weight\", x = \"Time\", title = \"Linear growth model\")\n\npred2 <- predictions(fit2,\n                     newdata = datagrid(Chick = ChickWeight$Chick,\n                                        Time = 0:21))\n\np2 <- ggplot(pred2, aes(Time, estimate, level = Chick)) +\n      geom_line() +\n      labs(y = \"Predicted weight\", x = \"Time\", title = \"Quadratic growth model\")\n\np1 + p2\n\n\n\n\nPredictions for each chick, in the 4 counterfactual worlds with different values for the Diet variable:\n\npred <- predictions(fit2)\n\nggplot(pred, aes(Time, estimate, level = Chick)) +\n    geom_line() +\n    ylab(\"Predicted Weight\") +\n    facet_wrap(~ Diet, labeller = label_both)\n\n\n\n\n\n16.0.2 Population-level predictions\nTo make population-level predictions, we set the Chick variable to NA, and set re.form=NA. This last argument is offered by the lme4::predict function which is used behind the scenes to compute predictions:\n\npred <- predictions(\n    fit2,\n    newdata = datagrid(Chick = NA,\n                       Diet = 1:4,\n                       Time = 0:21),\n    re.form = NA)\n\nggplot(pred, aes(x = Time, y = estimate, ymin = conf.low, ymax = conf.high)) +\n    geom_ribbon(alpha = .1, fill = \"red\") +\n    geom_line() +\n    facet_wrap(~ Diet, labeller = label_both) +\n    labs(title = \"Population-level trajectories\")"
  },
  {
    "objectID": "articles/multiple_imputation.html#mice",
    "href": "articles/multiple_imputation.html#mice",
    "title": "\n17  Missing Data\n",
    "section": "\n17.1 mice\n",
    "text": "17.1 mice\n\nFirst, we insert missing observations randomly in a dataset and we impute the dataset using the mice package:\n\nlibrary(mice)\nlibrary(marginaleffects)\nset.seed(1024)\n\ndat <- iris\ndat$Sepal.Length[sample(seq_len(nrow(iris)), 40)] <- NA\ndat$Sepal.Width[sample(seq_len(nrow(iris)), 40)] <- NA\ndat$Species[sample(seq_len(nrow(iris)), 40)] <- NA\n\ndat_mice <- mice(dat, m = 20, printFlag = FALSE, .Random.seed = 1024)\n\nThen, we use the standard mice syntax to produce an object of class mira with all the models:\n\nmod_mice <- with(dat_mice, lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species))\n\nFinally, we feed the mira object to a marginaleffects function:\n\nmfx_mice <- avg_slopes(mod_mice, by = \"Species\")\nmfx_mice\n#> \n#>          Term                        Contrast    Species Estimate Std. Error     S    Df      t Pr(>|t|)   2.5 % 97.5 %\n#>  Sepal.Length mean(dY/dX)                     setosa       0.0684     0.0560   2.2 120.0  1.222  0.22413 -0.0424  0.179\n#>  Sepal.Length mean(dY/dX)                     versicolor   0.0540     0.0557   1.6  91.1  0.969  0.33507 -0.0567  0.165\n#>  Sepal.Length mean(dY/dX)                     virginica    0.0582     0.0513   2.0 104.6  1.136  0.25869 -0.0434  0.160\n#>  Sepal.Width  mean(dY/dX)                     setosa       0.1890     0.0836   5.4 400.4  2.260  0.02434  0.0246  0.353\n#>  Sepal.Width  mean(dY/dX)                     versicolor   0.2093     0.0792   6.7  89.2  2.643  0.00971  0.0519  0.367\n#>  Sepal.Width  mean(dY/dX)                     virginica    0.2241     0.1026   4.9  61.2  2.185  0.03272  0.0190  0.429\n#>  Species      mean(versicolor) - mean(setosa) setosa       1.1399     0.0977  68.1 114.8 11.668  < 0.001  0.9464  1.333\n#>  Species      mean(versicolor) - mean(setosa) versicolor   1.1399     0.0977  68.1 114.8 11.668  < 0.001  0.9464  1.333\n#>  Species      mean(versicolor) - mean(setosa) virginica    1.1399     0.0977  68.1 114.8 11.668  < 0.001  0.9464  1.333\n#>  Species      mean(virginica) - mean(setosa)  setosa       1.7408     0.1108 100.7 121.6 15.709  < 0.001  1.5214  1.960\n#>  Species      mean(virginica) - mean(setosa)  versicolor   1.7408     0.1108 100.7 121.6 15.709  < 0.001  1.5214  1.960\n#>  Species      mean(virginica) - mean(setosa)  virginica    1.7408     0.1108 100.7 121.6 15.709  < 0.001  1.5214  1.960\n#> \n#> Columns: term, contrast, Species, estimate, std.error, s.value, predicted, predicted_hi, predicted_lo, df, statistic, p.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/multiple_imputation.html#other-imputation-packages-amelia-missranger-or-lists-of-imputed-data-frames.",
    "href": "articles/multiple_imputation.html#other-imputation-packages-amelia-missranger-or-lists-of-imputed-data-frames.",
    "title": "\n17  Missing Data\n",
    "section": "\n17.2 Other imputation packages: Amelia, missRanger, or lists of imputed data frames.",
    "text": "17.2 Other imputation packages: Amelia, missRanger, or lists of imputed data frames.\nSeveral R packages can impute missing data. Indeed, the Missing Data CRAN View lists at least a dozen alternatives. Since user interface changes a lot from package to package, marginaleffects supports a single workflow which can be used, with some adaptation, to with all imputation packages:\n\nUse an external package to create a list of imputed data frames.\nApply the datalist2mids() function from the miceadds package to convert the list of imputed data frames to a mids object.\nUse the with() function to fit models, as illustrated in the mice section above.\nPass the mids object to a marginaleffects function.\n\nConsider two imputation packages, which can both generate lists of imputed datasets: Amelia and missRanger.\n\nlibrary(Amelia)\nlibrary(miceadds)\nlibrary(missRanger)\n\n## impute data\ndat_amelia <- amelia(dat, noms = \"Species\", p2s = 0)$imputations\nmids_amelia <- datlist2mids(dat_amelia)\n\n## convert lists of imputed datasets to `mids` objects\ndat_missRanger <- replicate(20, missRanger(dat, verbose = 0), simplify = FALSE)\nmids_missRanger <- datlist2mids(dat_missRanger)\n\n## fit models\nmod_amelia <- with(mids_amelia, lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species))\nmod_missRanger <- with(mids_missRanger, lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species))\n\n## `Amelia` slopes\nmfx_amelia <- avg_slopes(mod_amelia, by = \"Species\")\nmfx_amelia\n#> \n#>          Term                        Contrast    Species Estimate Std. Error   S   Df      t Pr(>|t|)   2.5 % 97.5 %\n#>  Sepal.Length mean(dY/dX)                     setosa       0.3456     0.0935 7.3 7.92  3.696  0.00618  0.1296  0.562\n#>  Sepal.Length mean(dY/dX)                     virginica    0.3058     0.1020 5.5 6.52  2.998  0.02177  0.0609  0.551\n#>  Sepal.Length mean(dY/dX)                     versicolor   0.3008     0.0972 5.8 6.75  3.095  0.01829  0.0692  0.532\n#>  Sepal.Width  mean(dY/dX)                     setosa      -0.1690     0.1693 1.5 7.30 -0.998  0.35009 -0.5659  0.228\n#>  Sepal.Width  mean(dY/dX)                     virginica   -0.0621     0.1692 0.5 6.65 -0.367  0.72495 -0.4665  0.342\n#>  Sepal.Width  mean(dY/dX)                     versicolor  -0.0596     0.1510 0.5 7.77 -0.395  0.70348 -0.4097  0.290\n#>  Species      mean(versicolor) - mean(setosa) setosa       0.6696     0.2063 5.8 5.88  3.246  0.01805  0.1624  1.177\n#>  Species      mean(versicolor) - mean(setosa) virginica    0.6696     0.2063 5.8 5.88  3.246  0.01805  0.1624  1.177\n#>  Species      mean(versicolor) - mean(setosa) versicolor   0.6696     0.2063 5.8 5.88  3.246  0.01805  0.1624  1.177\n#>  Species      mean(virginica) - mean(setosa)  setosa       1.1303     0.2169 9.1 6.22  5.211  0.00178  0.6041  1.656\n#>  Species      mean(virginica) - mean(setosa)  virginica    1.1303     0.2169 9.1 6.22  5.211  0.00178  0.6041  1.656\n#>  Species      mean(virginica) - mean(setosa)  versicolor   1.1303     0.2169 9.1 6.22  5.211  0.00178  0.6041  1.656\n#> \n#> Columns: term, contrast, Species, estimate, std.error, s.value, predicted, predicted_hi, predicted_lo, df, statistic, p.value, conf.low, conf.high\n\n## `missRanger` slopes\nmfx_missRanger <- avg_slopes(mod_missRanger, by = \"Species\")\nmfx_missRanger\n#> \n#>          Term                        Contrast    Species Estimate Std. Error     S      Df     t Pr(>|t|)    2.5 % 97.5 %\n#>  Sepal.Length mean(dY/dX)                     setosa       0.0583     0.0414   2.7 3386591  1.41  0.15927 -0.02287  0.139\n#>  Sepal.Length mean(dY/dX)                     versicolor   0.0670     0.0392   3.5  772140  1.71  0.08745 -0.00984  0.144\n#>  Sepal.Length mean(dY/dX)                     virginica    0.0639     0.0368   3.6 1300640  1.74  0.08211 -0.00814  0.136\n#>  Sepal.Width  mean(dY/dX)                     setosa       0.2314     0.0691  10.3 3034568  3.35  < 0.001  0.09600  0.367\n#>  Sepal.Width  mean(dY/dX)                     versicolor   0.2188     0.0551  13.8 1270188  3.97  < 0.001  0.11091  0.327\n#>  Sepal.Width  mean(dY/dX)                     virginica    0.2092     0.0687   8.8  625863  3.05  0.00232  0.07457  0.344\n#>  Species      mean(versicolor) - mean(setosa) setosa       1.1588     0.0704 199.6 4239295 16.45  < 0.001  1.02078  1.297\n#>  Species      mean(versicolor) - mean(setosa) versicolor   1.1588     0.0704 199.6 4239295 16.45  < 0.001  1.02078  1.297\n#>  Species      mean(versicolor) - mean(setosa) virginica    1.1588     0.0704 199.6 4239295 16.45  < 0.001  1.02078  1.297\n#>  Species      mean(virginica) - mean(setosa)  setosa       1.7784     0.0823 341.6 4276524 21.61  < 0.001  1.61710  1.940\n#>  Species      mean(virginica) - mean(setosa)  versicolor   1.7784     0.0823 341.6 4276524 21.61  < 0.001  1.61710  1.940\n#>  Species      mean(virginica) - mean(setosa)  virginica    1.7784     0.0823 341.6 4276524 21.61  < 0.001  1.61710  1.940\n#> \n#> Columns: term, contrast, Species, estimate, std.error, s.value, predicted, predicted_hi, predicted_lo, df, statistic, p.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/multiple_imputation.html#comparing-results-with-different-imputation-software",
    "href": "articles/multiple_imputation.html#comparing-results-with-different-imputation-software",
    "title": "\n17  Missing Data\n",
    "section": "\n17.3 Comparing results with different imputation software",
    "text": "17.3 Comparing results with different imputation software\nWe can use the modelsummary package to compare the results with listwise deletion to the results using different imputations software:\n\nlibrary(modelsummary)\n\n## listwise deletion slopes\nmod_lwd <- lm(Petal.Width ~ Sepal.Length * Sepal.Width + Species, data = dat)\nmfx_lwd <- avg_slopes(mod_lwd, by = \"Species\")\n\n## regression table\nmodels <- list(\n    \"LWD\" = mfx_lwd,\n    \"mice\" = mfx_mice,\n    \"Amelia\" = mfx_amelia,\n    \"missRanger\" = mfx_missRanger)\nmodelsummary(models, shape = term : contrast + Species ~ model)\n\n\n\n\n   \n    Species \n    LWD \n    mice \n    Amelia \n    missRanger \n  \n\n\n Sepal.Length mean(dY/dX) \n    setosa \n    0.033 \n    0.068 \n    0.346 \n    0.058 \n  \n\n  \n    setosa \n    (0.061) \n    (0.056) \n    (0.094) \n    (0.041) \n  \n\n  \n    versicolor \n    0.050 \n    0.054 \n    0.301 \n    0.067 \n  \n\n  \n    versicolor \n    (0.061) \n    (0.056) \n    (0.097) \n    (0.039) \n  \n\n  \n    virginica \n    0.043 \n    0.058 \n    0.306 \n    0.064 \n  \n\n  \n    virginica \n    (0.058) \n    (0.051) \n    (0.102) \n    (0.037) \n  \n\n Sepal.Width mean(dY/dX) \n    setosa \n    0.274 \n    0.189 \n    −0.169 \n    0.231 \n  \n\n  \n    setosa \n    (0.091) \n    (0.084) \n    (0.169) \n    (0.069) \n  \n\n  \n    versicolor \n    0.255 \n    0.209 \n    −0.060 \n    0.219 \n  \n\n  \n    versicolor \n    (0.074) \n    (0.079) \n    (0.151) \n    (0.055) \n  \n\n  \n    virginica \n    0.234 \n    0.224 \n    −0.062 \n    0.209 \n  \n\n  \n    virginica \n    (0.083) \n    (0.103) \n    (0.169) \n    (0.069) \n  \n\n Species mean(versicolor) - mean(setosa) \n    setosa \n    1.157 \n    1.140 \n    0.670 \n    1.159 \n  \n\n  \n    setosa \n    (0.097) \n    (0.098) \n    (0.206) \n    (0.070) \n  \n\n  \n    versicolor \n    1.157 \n    1.140 \n    0.670 \n    1.159 \n  \n\n  \n    versicolor \n    (0.097) \n    (0.098) \n    (0.206) \n    (0.070) \n  \n\n  \n    virginica \n    1.157 \n    1.140 \n    0.670 \n    1.159 \n  \n\n  \n    virginica \n    (0.097) \n    (0.098) \n    (0.206) \n    (0.070) \n  \n\n Species mean(virginica) - mean(setosa) \n    setosa \n    1.839 \n    1.741 \n    1.130 \n    1.778 \n  \n\n  \n    setosa \n    (0.123) \n    (0.111) \n    (0.217) \n    (0.082) \n  \n\n  \n    versicolor \n    1.839 \n    1.741 \n    1.130 \n    1.778 \n  \n\n  \n    versicolor \n    (0.123) \n    (0.111) \n    (0.217) \n    (0.082) \n  \n\n  \n    virginica \n    1.839 \n    1.741 \n    1.130 \n    1.778 \n  \n\n  \n    virginica \n    (0.123) \n    (0.111) \n    (0.217) \n    (0.082) \n  \n\n Num.Obs. \n     \n    60 \n    150 \n    150 \n    150 \n  \n\n Num.Imp. \n     \n     \n    20 \n    5 \n    20 \n  \n\n R2 \n     \n    0.953 \n    0.930 \n    0.873 \n    0.947 \n  \n\n R2 Adj. \n     \n    0.949 \n    0.928 \n    0.869 \n    0.945 \n  \n\n AIC \n     \n    −34.0 \n     \n     \n     \n  \n\n BIC \n     \n    −19.3 \n     \n     \n     \n  \n\n Log.Lik. \n     \n    23.997 \n     \n     \n     \n  \n\n F \n     \n    220.780 \n     \n     \n     \n  \n\n RMSE \n     \n    0.16"
  },
  {
    "objectID": "articles/multiple_imputation.html#passing-new-data-arguments-newdata-wts-by-etc.",
    "href": "articles/multiple_imputation.html#passing-new-data-arguments-newdata-wts-by-etc.",
    "title": "\n17  Missing Data\n",
    "section": "\n17.4 Passing new data arguments: newdata, wts, by, etc.",
    "text": "17.4 Passing new data arguments: newdata, wts, by, etc.\nSometimes we want to pass arguments changing or specifying the data on which we will do our analysis using marginaleffects. This can be for reasons such as wanting to specify the values or weights at which we evaluate e.g. avg_slopes, or due to the underlying models not robustly preserving all the original data columns (such as fixest objects not saving their data in the fit object making it potentially challenging to retrieve, and even if retrievable it will not include the weights used during fitting as a column as wts expects when given a string).\nIf we are not using multiple imputation, or if we want to just pass a single dataset to the several fitted models after multiple imputation, we can pass a single dataset to the newdata argument. However, if we wish to supply each model in our list resulting after multiple imputation with a /different/ dataset on which to calculate results, we cannot use newdata. Instead, in this case it can be useful to revert to a more manual (but still very easy) approach. Here is an example calculating avg_slopes() using a different set of weights for each of the fixest models which we fit after multiple imputation.\n\nset.seed(1024)\nlibrary(mice)\nlibrary(fixest)\nlibrary(marginaleffects)\n\ndat <- mtcars\n\n## insert missing values\ndat$hp[sample(seq_len(nrow(mtcars)), 10)] <- NA\ndat$mpg[sample(seq_len(nrow(mtcars)), 10)] <- NA\ndat$gear[sample(seq_len(nrow(mtcars)), 10)] <- NA\n\n## multiple imputation\ndat <- mice(dat, m = 5, method = \"sample\", printFlag = FALSE)\ndat <- complete(dat, action = \"all\")\n\n## fit models\nmod <- lapply(dat, \\(x) \n    feglm(am ~ mpg * cyl + hp,\n        weight = ~gear,\n        family = binomial,\n        data = x))\n\n## slopes without weights\nlapply(seq_along(mod), \\(i) \n    avg_slopes(mod[[i]], newdata = dat[[i]])) |>\n    mice::pool()\n#> Class: mipo    m = 5 \n#>   term m     estimate         ubar            b            t dfcom       df      riv    lambda       fmi\n#> 1  mpg 5  0.006083006 1.080596e-04 2.722367e-04 4.347437e-04    29 3.458473 3.023185 0.7514407 0.8284122\n#> 2  cyl 5 -0.134279397 7.095734e-04 2.347149e-03 3.526152e-03    29 2.921498 3.969397 0.7987683 0.8667348\n#> 3   hp 5  0.001649797 5.709125e-07 1.375524e-06 2.221542e-06    29 3.556931 2.891212 0.7430107 0.8213978\n\n## slopes with weights\nlapply(seq_along(mod), \\(i) \n    avg_slopes(mod[[i]], newdata = dat[[i]], wts = \"gear\")) |>\n    mice::pool()\n#> Class: mipo    m = 5 \n#>   term m     estimate         ubar            b            t dfcom       df      riv    lambda       fmi\n#> 1  mpg 5  0.006251264 1.056050e-04 2.705362e-04 4.302484e-04    29 3.422442 3.074130 0.7545488 0.8309843\n#> 2  cyl 5 -0.135839559 7.279525e-04 2.480980e-03 3.705129e-03    29 2.868409 4.089794 0.8035284 0.8704875\n#> 3   hp 5  0.001671181 5.697579e-07 1.424665e-06 2.279356e-06    29 3.474808 3.000569 0.7500356 0.8272470"
  },
  {
    "objectID": "articles/python.html#fitting-a-numpyro-model",
    "href": "articles/python.html#fitting-a-numpyro-model",
    "title": "\n18  Python\n",
    "section": "\n18.1 Fitting a NumPyro model",
    "text": "18.1 Fitting a NumPyro model\nTo begin, we load the reticulate package which allows us to interact with the Python interpreter from an R session. Then, we write a NumPyro model and we load it to memory using the source_python() function. The important functions to note in the Python code are:\n\n\nload_df() downloads data on pulmonary fibrosis.\n\nmodel() defines the NumPyro model.\n\nfit_mcmc_model() fits the model using Markov Chain Monte Carlo.\n\npredict_mcmc(): accepts a data frame and returns a matrix of draws from the posterior distribution of adjusted predictions (fitted values).\n\n\nlibrary(reticulate)\nlibrary(marginaleffects)\n\nmodel <- '\n## Model code adapted from the NumPyro documtation under Apache License:\n## https://num.pyro.ai/en/latest/tutorials/bayesian_hierarchical_linear_regression.html\n\nimport pandas as pd\nimport numpy as np\nimport numpyro\nfrom numpyro.infer import SVI, Predictive, MCMC,NUTS, autoguide, TraceMeanField_ELBO\nimport numpyro.distributions as dist\nfrom numpyro.infer.initialization import init_to_median, init_to_uniform,init_to_sample\nfrom jax import random\nfrom sklearn.preprocessing import LabelEncoder\nimport pickle\n\ndef load_df():\n    train = pd.read_csv(\"https://raw.githubusercontent.com/vincentarelbundock/modelarchive/main/data-raw/osic_pulmonary_fibrosis.csv\")\n    return train\n\n\ndef model(data, predict = False):\n    FVC_obs = data[\"FVC\"].values  if predict == False else None\n    patient_encoder = LabelEncoder()\n    Age_obs = data[\"Age\"].values\n    patient_code = patient_encoder.fit_transform(data[\"Patient\"].values)\n    μ_α = numpyro.sample(\"μ_α\", dist.Normal(0.0, 500.0))\n    σ_α = numpyro.sample(\"σ_α\", dist.HalfNormal(100.0))\n\n    age = numpyro.sample(\"age\", dist.Normal(0.0, 500.0))\n\n    n_patients = len(np.unique(patient_code))\n\n    with numpyro.plate(\"plate_i\", n_patients):\n        α = numpyro.sample(\"α\", dist.Normal(μ_α, σ_α))\n\n    σ = numpyro.sample(\"σ\", dist.HalfNormal(100.0))\n    FVC_est = α[patient_code] + age * Age_obs\n\n    with numpyro.plate(\"data\", len(patient_code)):\n        numpyro.sample(\"obs\", dist.Normal(FVC_est, σ), obs=FVC_obs)\n\n\ndef fit_mcmc_model(train_df, samples = 1000):\n    numpyro.set_host_device_count(4)\n    rng_key = random.PRNGKey(0)\n    mcmc = MCMC(\n        NUTS(model),\n        num_samples=samples,\n        num_warmup=1000,\n        progress_bar=True,\n        num_chains = 4\n        )\n    \n    mcmc.run(rng_key, train_df)\n\n    posterior_draws = mcmc.get_samples()\n\n    with open(\"mcmc_posterior_draws.pickle\", \"wb\") as handle:\n        pickle.dump(posterior_draws, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\ndef predict_mcmc(data):\n\n    with open(\"mcmc_posterior_draws.pickle\", \"rb\") as handle:\n        posterior_draws = pickle.load(handle)\n\n    predictive = Predictive(model = model,posterior_samples=posterior_draws)\n    samples = predictive(random.PRNGKey(1), data, predict = True)\n    y_pred = samples[\"obs\"]\n    # transpose so that each column is a draw and each row is an observation\n    y_pred = np.transpose(np.array(y_pred))\n\n    return y_pred \n'\n\n## save python script to temp file\ntmp <- tempfile()\ncat(model, file = tmp)\n\n## load functions\nsource_python(tmp)\n\n## download data\ndf <- load_df()\n\n## fit model\nfit_mcmc_model(df)"
  },
  {
    "objectID": "articles/python.html#analyzing-the-results-in-marginaleffects",
    "href": "articles/python.html#analyzing-the-results-in-marginaleffects",
    "title": "\n18  Python\n",
    "section": "\n18.2 Analyzing the results in marginaleffects\n",
    "text": "18.2 Analyzing the results in marginaleffects\n\nEach of the functions in the marginaleffects package requires that users supply a model object on which the function will operate. When estimating models outside R, we do not have such a model object. We thus begin by creating a “fake” model object: an empty data frame which we define to be of class “custom”. Then, we set a global option to tell marginaleffects that this “custom” class is supported.\n\nmod <- data.frame()\nclass(mod) <- \"custom\"\n\noptions(\"marginaleffects_model_classes\" = \"custom\")\n\nNext, we define a get_predict method for our new custom class. This method must accept three arguments: model, newdata, and .... The get_predict method must return a data frame with one row for each of the rows in newdata, two columns (rowid and estimate), and an attribute called posterior_draws() which hosts a matrix of posterior draws with the same number of rows as newdata.\nThe method below uses reticulate to call the predict_mcmc() function that we defined in the Python script above. The predict_mcmc() function accepts a data frame and returns a matrix with the same number of rows.\n\nget_predict.custom <- function(model, newdata, ...) {\n    pred <- predict_mcmc(newdata)\n    out <- data.frame(\n        rowid = seq_len(nrow(newdata)),\n        predicted = apply(pred, 1, stats::median)\n    )\n    attr(out, \"posterior_draws\") <- pred\n    return(out)\n}\n\nNow we can use most of the marginaleffects package functions to analyze our results. Since we use a “fake” model object, marginaleffects cannot retrieve the original data from the model object, and we always need to supply a newdata argument:\n\n## predictions on the original dataset\npredictions(mod, newdata = df) |> head()\n\n## predictions for user-defined predictor values\npredictions(mod, newdata = datagrid(newdata = df, Age = c(60, 70)))\n\npredictions(mod, newdata = datagrid(newdata = df, Age = range))\n\n## average predictions by group\npredictions(mod, newdata = df, by = \"Sex\")\n\n## contrasts (average)\navg_comparisons(mod, variables = \"Age\", newdata = df)\n\navg_comparisons(mod, variables = list(\"Age\" = \"sd\"), newdata = df)\n\n## slope (elasticity)\navg_slopes(mod, variables = \"Age\", slope = \"eyex\", newdata = df)"
  },
  {
    "objectID": "articles/alternative_software.html#emmeans",
    "href": "articles/alternative_software.html#emmeans",
    "title": "\n19  Alternative Software\n",
    "section": "\n19.1 emmeans\n",
    "text": "19.1 emmeans\n\nThe emmeans package is developed by Russell V. Lenth and colleagues. emmeans is a truly incredible piece of software, and a trailblazer in the R ecosystem. It is an extremely powerful package whose functionality overlaps marginaleffects to a significant degree: marginal means, contrasts, and slopes. Even if the two packages can compute many of the same quantities, emmeans and marginaleffects have pretty different philosophies with respect to user interface and computation.\nAn emmeans analysis typically starts by computing “marginal means” by holding all numeric covariates at their means, and by averaging across a balanced grid of categorical predictors. Then, users can use the contrast() function to estimate the difference between marginal means.\nThe marginaleffects package supplies a marginal_means() function which can replicate most emmeans analyses by computing marginal means. However, the typical analysis is more squarely centered on predicted/fitted values. This is a useful starting point because, in many cases, analysts will find it easy and intuitive to express their scientific queries in terms of changes in predicted values. For example,\n\nHow does the average predicted probability of survival differ between treatment and control group?\nWhat is the difference between the predicted wage of college and high school graduates?\n\nLet’s say we estimate a linear regression model with two continuous regressors and a multiplicative interaction:\n\\[y = \\beta_0 + \\beta_1 x + \\beta_2 z + \\beta_3 x \\cdot z + \\varepsilon\\]\nIn this model, the effect of \\(x\\) on \\(y\\) will depend on the value of covariate \\(z\\). Let’s say the user wants to estimate what happens to the predicted value of \\(y\\) when \\(x\\) increases by 1 unit, when \\(z \\in \\{-1, 0, 1\\}\\). To do this, we use the comparisons() function. The variables argument determines the scientific query of interest, and the newdata argument determines the grid of covariate values on which we want to evaluate the query:\n\nmodel <- lm(y ~ x * z, data)\n\ncomparisons(\n  model,\n  variables = list(x = 1), # what is the effect of 1-unit change in x?\n  newdata = datagrid(z = -1:1) # when z is held at values -1, 0, or 1\n)\n\nAs the vignettes show, marginaleffects can also compute contrasts on marginal means. It can also compute various quantities of interest like raw fitted values, slopes (partial derivatives), and contrasts between marginal means. It also offers a flexible mechanism to run (non-)linear hypothesis tests using the delta method, and it offers fully customizable strategy to compute quantities like odds ratios (or completely arbitrary functions of predicted outcome).\nThus, in my (Vincent’s) biased opinion, the main benefits of marginaleffects over emmeans are:\n\nSupport more model types.\nSimpler, more intuitive, and highly consistent user interface.\nEasier to compute average slopes or unit-level contrasts for whole datasets.\nEasier to compute slopes (aka marginal effects, trends, or partial derivatives) for custom grids and continuous regressors.\nEasier to implement causal inference strategies like the parametric g-formula and regression adjustment in experiments (see vignettes).\nAllows the computation of arbitrary quantities of interest via user-supplied functions and automatic delta method inference.\nCommon plots are easy with the plot_predictions(), plot_comparisons(), and plot_slopes() functions.\n\nTo be fair, many of the marginaleffects advantages listed above come down to subjective preferences over user interface. Readers are thus encouraged to try both packages to see which interface they prefer.\nAFAICT, the main advantages of emmeans over marginaleffects are:\n\nOmnibus tests.\nMultiplicity adjustments.\nMarginal means (but not slopes or contrasts) are often faster to compute.\n\nPlease let me know if you find other features in emmeans so I can add them to this list.\nThe Marginal Means Vignette includes side-by-side comparisons of emmeans and marginaleffects to compute marginal means. The rest of this section compares the syntax for contrasts and marginaleffects.\n\n19.1.1 Contrasts\nAs far as I can tell, emmeans does not provide an easy way to compute unit-level contrasts for every row of the dataset used to fit our model. Therefore, the side-by-side syntax shown below will always include newdata=datagrid() to specify that we want to compute only one contrast: at the mean values of the regressors. In day-to-day practice with slopes(), however, this extra argument would not be necessary.\nFit a model:\n\nlibrary(emmeans)\nlibrary(marginaleffects)\n\nmod <- glm(vs ~ hp + factor(cyl), data = mtcars, family = binomial)\n\nLink scale, pairwise contrasts:\n\nemm <- emmeans(mod, specs = \"cyl\")\ncontrast(emm, method = \"revpairwise\", adjust = \"none\", df = Inf)\n#>  contrast    estimate      SE  df z.ratio p.value\n#>  cyl6 - cyl4   -0.905    1.63 Inf  -0.555  0.5789\n#>  cyl8 - cyl4  -19.542 4367.17 Inf  -0.004  0.9964\n#>  cyl8 - cyl6  -18.637 4367.16 Inf  -0.004  0.9966\n#> \n#> Degrees-of-freedom method: user-specified \n#> Results are given on the log odds ratio (not the response) scale.\n\ncomparisons(mod,\n            type = \"link\",\n            newdata = \"mean\",\n            variables = list(cyl = \"pairwise\"))\n#> \n#>  Term Contrast Estimate Std. Error        z Pr(>|z|)   S   2.5 %  97.5 %\n#>   cyl    6 - 4   -0.905       1.63 -0.55506    0.579 0.8    -4.1    2.29\n#>   cyl    8 - 4  -19.542    4367.17 -0.00447    0.996 0.0 -8579.0 8539.95\n#>   cyl    8 - 6  -18.637    4367.17 -0.00427    0.997 0.0 -8578.1 8540.85\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, vs, hp, cyl\n\nResponse scale, reference groups:\n\nemm <- emmeans(mod, specs = \"cyl\", regrid = \"response\")\ncontrast(emm, method = \"trt.vs.ctrl1\", adjust = \"none\", df = Inf, ratios = FALSE)\n#>  contrast    estimate    SE  df z.ratio p.value\n#>  cyl6 - cyl4   -0.222 0.394 Inf  -0.564  0.5727\n#>  cyl8 - cyl4   -0.595 0.511 Inf  -1.163  0.2447\n#> \n#> Degrees-of-freedom method: user-specified\n\ncomparisons(mod, newdata = \"mean\")\n#> \n#>  Term Contrast  Estimate Std. Error         z Pr(>|z|)   S     2.5 %   97.5 %\n#>   hp     +1    -1.56e-10   6.80e-07 -0.000229    1.000 0.0 -1.33e-06 1.33e-06\n#>   cyl    6 - 4 -2.22e-01   3.94e-01 -0.564207    0.573 0.8 -9.94e-01 5.50e-01\n#>   cyl    8 - 4 -5.95e-01   5.11e-01 -1.163438    0.245 2.0 -1.60e+00 4.07e-01\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, vs, hp, cyl\n\n\n19.1.2 Contrasts by group\nHere is a slightly more complicated example with contrasts estimated by subgroup in a lme4 mixed effects model. First we estimate a model and compute pairwise contrasts by subgroup using emmeans:\n\nlibrary(dplyr)\nlibrary(lme4)\nlibrary(emmeans)\n\ndat <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/lme4/VerbAgg.csv\")\ndat$woman <- as.numeric(dat$Gender == \"F\")\n\nmod <- glmer(\n    woman ~ btype * resp + situ + (1 + Anger | item),\n    family = binomial,\n    data = dat)\n\nemmeans(mod, specs = \"btype\", by = \"resp\") |>\n    contrast(method = \"revpairwise\", adjust = \"none\")\n#> resp = no:\n#>  contrast      estimate     SE  df z.ratio p.value\n#>  scold - curse  -0.0152 0.1097 Inf  -0.139  0.8898\n#>  shout - curse  -0.2533 0.1022 Inf  -2.479  0.0132\n#>  shout - scold  -0.2381 0.0886 Inf  -2.686  0.0072\n#> \n#> resp = perhaps:\n#>  contrast      estimate     SE  df z.ratio p.value\n#>  scold - curse  -0.2393 0.1178 Inf  -2.031  0.0422\n#>  shout - curse  -0.0834 0.1330 Inf  -0.627  0.5309\n#>  shout - scold   0.1559 0.1358 Inf   1.148  0.2510\n#> \n#> resp = yes:\n#>  contrast      estimate     SE  df z.ratio p.value\n#>  scold - curse   0.0391 0.1292 Inf   0.302  0.7624\n#>  shout - curse   0.5802 0.1784 Inf   3.252  0.0011\n#>  shout - scold   0.5411 0.1888 Inf   2.866  0.0042\n#> \n#> Results are averaged over the levels of: situ \n#> Results are given on the log odds ratio (not the response) scale.\n\nWhat did emmeans do to obtain these results? Roughly speaking:\n\nCreate a prediction grid with one cell for each combination of categorical predictors in the model, and all numeric variables held at their means.\nMake adjusted predictions in each cell of the prediction grid.\nTake the average of those predictions (marginal means) for each combination of btype (focal variable) and resp (group by variable).\nCompute pairwise differences (contrasts) in marginal means across different levels of the focal variable btype.\n\nIn short, emmeans computes pairwise contrasts between marginal means, which are themselves averages of adjusted predictions. This is different from the default types of contrasts produced by comparisons(), which reports contrasts between adjusted predictions, without averaging across a pre-specified grid of predictors. What does comparisons() do instead?\nLet newdata be a data frame supplied by the user (or the original data frame used to fit the model), then:\n\nCreate a new data frame called newdata2, which is identical to newdata except that the focal variable is incremented by one level.\nCompute contrasts as the difference between adjusted predictions made on the two datasets:\n\npredict(model, newdata = newdata2) - predict(model, newdata = newdata)\n\n\n\nAlthough it is not idiomatic, we can use still use comparisons() to emulate the emmeans results. First, we create a prediction grid with one cell for each combination of categorical predictor in the model:\n\nnd <- datagrid(\n    model = mod,\n    resp = dat$resp,\n    situ = dat$situ,\n    btype = dat$btype)\nnrow(nd)\n#> [1] 18\n\nThis grid has 18 rows, one for each combination of levels for the resp (3), situ (2), and btype (3) variables (3 * 2 * 3 = 18).\nThen we compute pairwise contrasts over this grid:\n\ncmp <- comparisons(mod,\n    variables = list(\"btype\" = \"pairwise\"),\n    newdata = nd,\n    type = \"link\")\nnrow(cmp)\n#> [1] 54\n\nThere are 3 pairwise contrasts, corresponding to the 3 pairwise comparisons possible between the 3 levels of the focal variable btype: scold-curse, shout-scold, shout-curse. The comparisons() function estimates those 3 contrasts for each row of newdata, so we get \\(18 \\times 3 = 54\\) rows.\nFinally, if we wanted contrasts averaged over each subgroup of the resp variable, we can use the avg_comparisons() function with the by argument:\n\navg_comparisons(mod,\n    by = \"resp\",\n    variables = list(\"btype\" = \"pairwise\"),\n    newdata = nd,\n    type = \"link\")\n#> \n#>   Term                  Contrast    resp Estimate Std. Error      z Pr(>|z|)   S  2.5 %   97.5 %\n#>  btype mean(scold) - mean(curse) no       -0.0152     0.1097 -0.139  0.88976 0.2 -0.230  0.19972\n#>  btype mean(scold) - mean(curse) perhaps  -0.2393     0.1178 -2.031  0.04221 4.6 -0.470 -0.00842\n#>  btype mean(scold) - mean(curse) yes       0.0391     0.1292  0.302  0.76239 0.4 -0.214  0.29234\n#>  btype mean(shout) - mean(curse) no       -0.2533     0.1022 -2.479  0.01319 6.2 -0.454 -0.05300\n#>  btype mean(shout) - mean(curse) perhaps  -0.0834     0.1330 -0.627  0.53090 0.9 -0.344  0.17737\n#>  btype mean(shout) - mean(curse) yes       0.5802     0.1784  3.252  0.00115 9.8  0.230  0.92987\n#>  btype mean(shout) - mean(scold) no       -0.2381     0.0886 -2.686  0.00723 7.1 -0.412 -0.06436\n#>  btype mean(shout) - mean(scold) perhaps   0.1559     0.1358  1.148  0.25103 2.0 -0.110  0.42215\n#>  btype mean(shout) - mean(scold) yes       0.5411     0.1888  2.866  0.00416 7.9  0.171  0.91116\n#> \n#> Columns: term, contrast, resp, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nThese results are identical to those produced by emmeans (except for \\(t\\) vs. \\(z\\)).\n\n19.1.3 Marginal Effects\nAs far as I can tell, emmeans::emtrends makes it easier to compute marginal effects for a few user-specified values than for large grids or for the full original dataset.\nResponse scale, user-specified values:\n\nmod <- glm(vs ~ hp + factor(cyl), data = mtcars, family = binomial)\n\nemtrends(mod, ~hp, \"hp\", regrid = \"response\", at = list(cyl = 4))\n#>   hp hp.trend    SE  df asymp.LCL asymp.UCL\n#>  147 -0.00786 0.011 Inf   -0.0294    0.0137\n#> \n#> Confidence level used: 0.95\n\nslopes(mod, newdata = datagrid(cyl = 4))\n#> \n#>  Term Contrast Estimate Std. Error      z Pr(>|z|)   S   2.5 % 97.5 %  hp cyl\n#>   hp     dY/dX -0.00785      0.011 -0.713    0.476 1.1 -0.0294 0.0137 147   4\n#>   cyl    6 - 4 -0.22219      0.394 -0.564    0.573 0.8 -0.9940 0.5496 147   4\n#>   cyl    8 - 4 -0.59469      0.511 -1.163    0.245 2.0 -1.5965 0.4071 147   4\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, vs, hp, cyl\n\nLink scale, user-specified values:\n\nemtrends(mod, ~hp, \"hp\", at = list(cyl = 4))\n#>   hp hp.trend     SE  df asymp.LCL asymp.UCL\n#>  147  -0.0326 0.0339 Inf    -0.099    0.0338\n#> \n#> Confidence level used: 0.95\n\nslopes(mod, type = \"link\", newdata = datagrid(cyl = 4))\n#> \n#>  Term Contrast Estimate Std. Error        z Pr(>|z|)   S     2.5 %   97.5 %  hp cyl\n#>   hp     dY/dX  -0.0326   3.39e-02 -0.96144    0.336 1.6    -0.099 3.38e-02 147   4\n#>   cyl    6 - 4  -0.9049   1.63e+00 -0.55506    0.579 0.8    -4.100 2.29e+00 147   4\n#>   cyl    8 - 4 -19.5418   4.37e+03 -0.00447    0.996 0.0 -8579.030 8.54e+03 147   4\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, vs, hp, cyl\n\n\n19.1.4 More examples\nHere are a few more emmeans vs. marginaleffects comparisons:\n\n## Example of examining a continuous x categorical interaction using emmeans and marginaleffects\n## Authors: Cameron Patrick and Vincent Arel-Bundock\n\nlibrary(tidyverse)\nlibrary(emmeans)\nlibrary(marginaleffects)\n\n## use the mtcars data, set up am as a factor\ndata(mtcars)\nmc <- mtcars |> mutate(am = factor(am))\n\n## fit a linear model to mpg with wt x am interaction\nm <- lm(mpg ~ wt*am, data = mc)\nsummary(m)\n\n## 1. means for each level of am at mean wt.\nemmeans(m, \"am\")\nmarginal_means(m, variables = \"am\")\npredictions(m, newdata = datagrid(am = 0:1))\n\n## 2. means for each level of am at wt = 2.5, 3, 3.5.\nemmeans(m, c(\"am\", \"wt\"), at = list(wt = c(2.5, 3, 3.5)))\npredictions(m, newdata = datagrid(am = 0:1, wt = c(2.5, 3, 3.5))\n\n## 3. means for wt = 2.5, 3, 3.5, averaged over levels of am (implicitly!).\nemmeans(m, \"wt\", at = list(wt = c(2.5, 3, 3.5)))\n\n## same thing, but the averaging is more explicit, using the `by` argument\npredictions(\n  m,\n  newdata = datagrid(am = 0:1, wt = c(2.5, 3, 3.5)),\n  by = \"wt\")\n\n## 4. graphical version of 2.\nemmip(m, am ~ wt, at = list(wt = c(2.5, 3, 3.5)), CIs = TRUE)\nplot_predictions(m, condition = c(\"wt\", \"am\"))\n\n## 5. compare levels of am at specific values of wt.\n## this is a bit ugly because the emmeans defaults for pairs() are silly.\n## infer = TRUE: enable confidence intervals.\n## adjust = \"none\": begone, Tukey.\n## reverse = TRUE: contrasts as (later level) - (earlier level)\npairs(emmeans(m, \"am\", by = \"wt\", at = list(wt = c(2.5, 3, 3.5))),\n      infer = TRUE, adjust = \"none\", reverse = TRUE)\n\ncomparisons(\n  m,\n  variables = \"am\",\n  newdata = datagrid(wt = c(2.5, 3, 3.5)))\n\n## 6. plot of pairswise comparisons\nplot(pairs(emmeans(m, \"am\", by = \"wt\", at = list(wt = c(2.5, 3, 3.5))),\n      infer = TRUE, adjust = \"none\", reverse = TRUE))\n\n## Since `wt` is numeric, the default is to plot it as a continuous variable on\n## the x-axis.  But not that this is the **exact same info** as in the emmeans plot.\nplot_comparisons(m, variables = \"am\", condition = \"wt\")\n\n## You of course customize everything, set draw=FALSE, and feed the raw data to feed to ggplot2\np <- plot_comparisons(\n  m,\n  variables = \"am\",\n  condition = list(wt = c(2.5, 3, 3.5)),\n  draw = FALSE)\n\nggplot(p, aes(y = wt, x = comparison, xmin = conf.low, xmax = conf.high)) +\n  geom_pointrange()\n\n## 7. slope of wt for each level of am\nemtrends(m, \"am\", \"wt\")\nslopes(m, newdata = datagrid(am = 0:1))"
  },
  {
    "objectID": "articles/alternative_software.html#margins-and-prediction",
    "href": "articles/alternative_software.html#margins-and-prediction",
    "title": "\n19  Alternative Software\n",
    "section": "\n19.2 margins and prediction\n",
    "text": "19.2 margins and prediction\n\nThe margins and prediction packages for R were designed by Thomas Leeper to emulate the behavior of the margins command from Stata. These packages are trailblazers and strongly influenced the development of marginaleffects. The main benefits of marginaleffects over these packages are:\n\nSupport more model types\nFaster\nMemory efficient\nPlots using ggplot2 instead of Base R\nMore extensive test suite\nActive development\n\nThe syntax of the two packages is very similar.\n\n19.2.1 Average Marginal Effects\n\nlibrary(margins)\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ cyl + hp + wt, data = mtcars)\n\nmar <- margins(mod)\nsummary(mar)\n#>  factor     AME     SE       z      p   lower   upper\n#>     cyl -0.9416 0.5509 -1.7092 0.0874 -2.0214  0.1382\n#>      hp -0.0180 0.0119 -1.5188 0.1288 -0.0413  0.0052\n#>      wt -3.1670 0.7406 -4.2764 0.0000 -4.6185 -1.7155\n\nmfx <- slopes(mod)\n\n\n19.2.2 Individual-Level Marginal Effects\nMarginal effects in a user-specified data frame:\n\nhead(data.frame(mar))\n#>    mpg cyl disp  hp drat    wt  qsec vs am gear carb   fitted se.fitted   dydx_cyl    dydx_hp   dydx_wt Var_dydx_cyl  Var_dydx_hp Var_dydx_wt X_weights X_at_number\n#> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 22.82043 0.6876212 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 22.01285 0.6056817 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 25.96040 0.7349593 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 20.93608 0.5800910 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 17.16780 0.8322986 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n#> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 20.25036 0.6638322 -0.9416168 -0.0180381 -3.166973    0.3035104 0.0001410451   0.5484521        NA           1\n\nhead(mfx)\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>   cyl   -0.942      0.551 -1.71   0.0874 3.5 -2.02  0.138\n#>   cyl   -0.942      0.551 -1.71   0.0874 3.5 -2.02  0.138\n#>   cyl   -0.942      0.551 -1.71   0.0874 3.5 -2.02  0.138\n#>   cyl   -0.942      0.551 -1.71   0.0874 3.5 -2.02  0.138\n#>   cyl   -0.942      0.551 -1.71   0.0874 3.5 -2.02  0.138\n#>   cyl   -0.942      0.551 -1.71   0.0874 3.5 -2.02  0.138\n#> \n#> Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, cyl, hp, wt\nnd <- data.frame(cyl = 4, hp = 110, wt = 3)\n\n\n19.2.3 Marginal Effects at the Mean\n\nmar <- margins(mod, data = data.frame(prediction::mean_or_mode(mtcars)), unit_ses = TRUE)\ndata.frame(mar)\n#>        mpg    cyl     disp       hp     drat      wt     qsec     vs      am   gear   carb   fitted se.fitted   dydx_cyl    dydx_hp   dydx_wt Var_dydx_cyl  Var_dydx_hp Var_dydx_wt SE_dydx_cyl SE_dydx_hp SE_dydx_wt X_weights X_at_number\n#> 1 20.09062 6.1875 230.7219 146.6875 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 20.09062 0.4439832 -0.9416168 -0.0180381 -3.166973    0.3035013 0.0001410453     0.54846   0.5509096 0.01187625  0.7405808        NA           1\n\nslopes(mod, newdata = \"mean\")\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S   2.5 %   97.5 %\n#>   cyl   -0.942     0.5509 -1.71   0.0874  3.5 -2.0214  0.13816\n#>   hp    -0.018     0.0119 -1.52   0.1288  3.0 -0.0413  0.00524\n#>   wt    -3.167     0.7406 -4.28   <0.001 15.7 -4.6185 -1.71547\n#> \n#> Columns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, cyl, hp, wt\n\n\n19.2.4 Counterfactual Average Marginal Effects\nThe at argument of the margins package emulates Stata by fixing the values of some variables at user-specified values, and by replicating the full dataset several times for each combination of the supplied values (see the Stata section below). For example, if the dataset includes 32 rows and the user calls at=list(cyl=c(4, 6)), margins will compute 64 unit-level marginal effects estimates:\n\ndat <- mtcars\ndat$cyl <- factor(dat$cyl)\nmod <- lm(mpg ~ cyl * hp + wt, data = mtcars)\n\nmar <- margins(mod, at = list(cyl = c(4, 6, 8)))\nsummary(mar)\n#>  factor    cyl     AME     SE       z      p   lower   upper\n#>     cyl 4.0000  0.0381 0.6000  0.0636 0.9493 -1.1378  1.2141\n#>     cyl 6.0000  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>     cyl 8.0000  0.0381 0.5999  0.0636 0.9493 -1.1376  1.2139\n#>      hp 4.0000 -0.0878 0.0267 -3.2937 0.0010 -0.1400 -0.0355\n#>      hp 6.0000 -0.0499 0.0154 -3.2397 0.0012 -0.0800 -0.0197\n#>      hp 8.0000 -0.0120 0.0108 -1.1065 0.2685 -0.0332  0.0092\n#>      wt 4.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n#>      wt 6.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n#>      wt 8.0000 -3.1198 0.6613 -4.7175 0.0000 -4.4160 -1.8236\n\navg_slopes(\n    mod,\n    by = \"cyl\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\n#> \n#>  Term    Contrast cyl Estimate Std. Error       z Pr(>|z|)    S   2.5 %   97.5 %\n#>   cyl mean(dY/dX)   4   0.0381     0.5999  0.0636   0.9493  0.1 -1.1376  1.21390\n#>   cyl mean(dY/dX)   6   0.0381     0.5999  0.0636   0.9493  0.1 -1.1376  1.21390\n#>   cyl mean(dY/dX)   8   0.0381     0.5999  0.0636   0.9493  0.1 -1.1376  1.21390\n#>   hp  mean(dY/dX)   4  -0.0878     0.0267 -3.2937   <0.001 10.0 -0.1400 -0.03554\n#>   hp  mean(dY/dX)   6  -0.0499     0.0154 -3.2397   0.0012  9.7 -0.0800 -0.01970\n#>   hp  mean(dY/dX)   8  -0.0120     0.0108 -1.1065   0.2685  1.9 -0.0332  0.00923\n#>   wt  mean(dY/dX)   4  -3.1198     0.6613 -4.7175   <0.001 18.7 -4.4160 -1.82365\n#>   wt  mean(dY/dX)   6  -3.1198     0.6613 -4.7175   <0.001 18.7 -4.4160 -1.82365\n#>   wt  mean(dY/dX)   8  -3.1198     0.6613 -4.7175   <0.001 18.7 -4.4160 -1.82365\n#> \n#> Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\n\n19.2.5 Adjusted Predictions\nThe syntax to compute adjusted predictions using the predictions() package or marginaleffects is very similar:\n\nprediction::prediction(mod) |> head()\n#>    mpg cyl disp  hp drat    wt  qsec vs am gear carb   fitted se.fitted\n#> 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4 21.90488 0.6927034\n#> 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4 21.10933 0.6266557\n#> 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1 25.64753 0.6652076\n#> 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1 20.04859 0.6041400\n#> 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2 17.25445 0.7436172\n#> 6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1 19.53360 0.6436862\n\nmarginaleffects::predictions(mod) |> head()\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      21.9      0.693 31.6   <0.001 726.6  20.5   23.3\n#>      21.1      0.627 33.7   <0.001 823.9  19.9   22.3\n#>      25.6      0.665 38.6   <0.001   Inf  24.3   27.0\n#>      20.0      0.604 33.2   <0.001 799.8  18.9   21.2\n#>      17.3      0.744 23.2   <0.001 393.2  15.8   18.7\n#>      19.5      0.644 30.3   <0.001 669.5  18.3   20.8\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, hp, wt"
  },
  {
    "objectID": "articles/alternative_software.html#stata",
    "href": "articles/alternative_software.html#stata",
    "title": "\n19  Alternative Software\n",
    "section": "\n19.3 Stata\n",
    "text": "19.3 Stata\n\nStata is a good but expensive software package for statistical analysis. It is published by StataCorp LLC. This section compares Stata’s margins command to marginaleffects.\nThe results produced by marginaleffects are extensively tested against Stata. See the test suite for a list of the dozens of models where we compared estimates and standard errors.\n\n19.3.1 Average Marginal Effect (AMEs)\nMarginal effects are unit-level quantities. To compute “average marginal effects”, we first calculate marginal effects for each observation in a dataset. Then, we take the mean of those unit-level marginal effects.\n\n19.3.1.1 Stata\nBoth Stata’s margins command and the slopes() function can calculate average marginal effects (AMEs). Here is an example showing how to estimate AMEs in Stata:\nquietly reg mpg cyl hp wt\nmargins, dydx(*)\n\nAverage marginal effects                        Number of obs     =         32\nModel VCE    : OLS\n \nExpression   : Linear prediction, predict()\ndy/dx w.r.t. : cyl hp wt\n \n------------------------------------------------------------------------------\n    |            Delta-method\n    |      dy/dx   Std. Err.      t    P>|t|     [95% Conf. Interval]\n------------------------------------------------------------------------------\ncyl |  -.9416168   .5509164    -1.71   0.098    -2.070118    .1868842\n hp |  -.0180381   .0118762    -1.52   0.140    -.0423655    .0062893\n wt |  -3.166973   .7405759    -4.28   0.000    -4.683974   -1.649972\n------------------------------------------------------------------------------\n\n19.3.1.2 marginaleffects\nThe same results can be obtained with slopes() and summary() like this:\n\nlibrary(\"marginaleffects\")\nmod <- lm(mpg ~ cyl + hp + wt, data = mtcars)\navg_slopes(mod)\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S   2.5 %   97.5 %\n#>   cyl   -0.942     0.5509 -1.71   0.0874  3.5 -2.0214  0.13817\n#>   hp    -0.018     0.0119 -1.52   0.1288  3.0 -0.0413  0.00524\n#>   wt    -3.167     0.7406 -4.28   <0.001 15.7 -4.6185 -1.71547\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNote that Stata reports t statistics while marginaleffects reports Z. This produces slightly different p-values because this model has low degrees of freedom: mtcars only has 32 rows\n\n19.3.2 Counterfactual Marginal Effects\nA “counterfactual marginal effect” is a special quantity obtained by replicating a dataset while fixing some regressor to user-defined values.\nConcretely, Stata computes counterfactual marginal effects in 3 steps:\n\nDuplicate the whole dataset 3 times and sets the values of cyl to the three specified values in each of those subsets.\nCalculate marginal effects for each observation in that large grid.\nTake the average of marginal effects for each value of the variable of interest.\n\n\n19.3.2.1 Stata\nWith the at argument, Stata’s margins command estimates average counterfactual marginal effects. Here is an example:\nquietly reg mpg i.cyl##c.hp wt\nmargins, dydx(hp) at(cyl = (4 6 8))\n\nAverage marginal effects                        Number of obs     =         32\nModel VCE    : OLS\n\nExpression   : Linear prediction, predict()\ndy/dx w.r.t. : hp\n\n1._at        : cyl             =           4\n\n2._at        : cyl             =           6\n\n3._at        : cyl             =           8\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |      dy/dx   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\nhp           |\n         _at |\n          1  |   -.099466   .0348665    -2.85   0.009    -.1712749   -.0276571\n          2  |  -.0213768    .038822    -0.55   0.587    -.1013323    .0585787\n          3  |   -.013441   .0125138    -1.07   0.293    -.0392137    .0123317\n------------------------------------------------------------------------------\n\n\n19.3.2.2 marginaleffects\nYou can estimate average counterfactual marginal effects with slopes() by using the datagridcf() to create a counterfactual dataset in which the full original dataset is replicated for each potential value of the cyl variable. Then, we tell the by argument to average within groups:\n\nmod <- lm(mpg ~ as.factor(cyl) * hp + wt, data = mtcars)\n\navg_slopes(\n    mod,\n    variables = \"hp\",\n    by = \"cyl\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\n#> \n#>  Term    Contrast cyl Estimate Std. Error      z Pr(>|z|)   S   2.5 %  97.5 %\n#>    hp mean(dY/dX)   4  -0.0995     0.0349 -2.853  0.00433 7.9 -0.1678 -0.0311\n#>    hp mean(dY/dX)   6  -0.0214     0.0388 -0.551  0.58188 0.8 -0.0975  0.0547\n#>    hp mean(dY/dX)   8  -0.0134     0.0125 -1.074  0.28278 1.8 -0.0380  0.0111\n#> \n#> Columns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo\n\nThis is equivalent to taking the group-wise mean of observation-level marginal effects (without the by argument):\n\nmfx <- slopes(\n    mod,\n    variables = \"hp\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\naggregate(estimate ~ term + cyl, data = mfx, FUN = mean)\n#>   term cyl    estimate\n#> 1   hp   4 -0.09946598\n#> 2   hp   6 -0.02137679\n#> 3   hp   8 -0.01344103\n\n\nNote that following Stata, the standard errors for group-averaged marginal effects are computed by taking the “Jacobian at the mean:”\n\nJ <- attr(mfx, \"jacobian\")\nJ_mean <- aggregate(J, by = list(mfx$cyl), FUN = mean)\nJ_mean <- as.matrix(J_mean[, 2:ncol(J_mean)])\nsqrt(diag(J_mean %*% vcov(mod) %*% t(J_mean)))\n#> [1] 0.03486651 0.03882204 0.01251383\n\n\n19.3.3 Average Counterfactual Adjusted Predictions\n\n19.3.3.1 Stata\nJust like Stata’s margins command computes average counterfactual marginal effects, it can also estimate average counterfactual adjusted predictions.\nHere is an example:\nquietly reg mpg i.cyl##c.hp wt\nmargins, at(cyl = (4 6 8))\n\nPredictive margins                              Number of obs     =         32\nModel VCE    : OLS\n\nExpression   : Linear prediction, predict()\n\n1._at        : cyl             =           4\n\n2._at        : cyl             =           6\n\n3._at        : cyl             =           8\n\n------------------------------------------------------------------------------\n             |            Delta-method\n             |     Margin   Std. Err.      t    P>|t|     [95% Conf. Interval]\n-------------+----------------------------------------------------------------\n         _at |\n          1  |   17.44233   2.372914     7.35   0.000     12.55522    22.32944\n          2  |    18.9149   1.291483    14.65   0.000     16.25505    21.57476\n          3  |   18.33318   1.123874    16.31   0.000     16.01852    20.64785\n------------------------------------------------------------------------------\nAgain, this is what Stata does in the background:\n\nIt duplicates the whole dataset 3 times and sets the values of cyl to the three specified values in each of those subsets.\nIt calculates predictions for that large grid.\nIt takes the average prediction for each value of cyl.\n\nIn other words, average counterfactual adjusted predictions as implemented by Stata are a hybrid between predictions at the observed values (the default in marginaleffects::predictions) and predictions at representative values.\n\n19.3.3.2 marginaleffects\nYou can estimate average counterfactual adjusted predictions with predictions() by, first, setting the grid_type argument of datagrid() to \"counterfactual\" and, second, by averaging the predictions using the by argument of summary(), or a manual function like dplyr::summarise().\n\nmod <- lm(mpg ~ as.factor(cyl) * hp + wt, data = mtcars)\n\npredictions(\n    mod,\n    by = \"cyl\",\n    newdata = datagridcf(cyl = c(4, 6, 8)))\n#> \n#>  cyl Estimate Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n#>    4     17.4       2.37  7.35   <0.001  42.2  12.8   22.1\n#>    6     18.9       1.29 14.65   <0.001 158.9  16.4   21.4\n#>    8     18.3       1.12 16.31   <0.001 196.3  16.1   20.5\n#> \n#> Columns: cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\npredictions(\n    mod,\n    newdata = datagridcf(cyl = c(4, 6, 8))) |>\n    group_by(cyl) |>\n    summarize(AAP = mean(estimate))\n#> # A tibble: 3 × 2\n#>   cyl     AAP\n#>   <fct> <dbl>\n#> 1 4      17.4\n#> 2 6      18.9\n#> 3 8      18.3"
  },
  {
    "objectID": "articles/alternative_software.html#brmsmargins",
    "href": "articles/alternative_software.html#brmsmargins",
    "title": "\n19  Alternative Software\n",
    "section": "\n19.4 brmsmargins\n",
    "text": "19.4 brmsmargins\n\nThe brmsmargins package is developed by Joshua Wiley:\n\nThis package has functions to calculate marginal effects from brms models ( http://paul-buerkner.github.io/brms/ ). A central motivator is to calculate average marginal effects (AMEs) for continuous and discrete predictors in fixed effects only and mixed effects regression models including location scale models.\n\nThe main advantage of brmsmargins over marginaleffects is its ability to compute “Marginal Coefficients” following the method described in Hedeker et al (2012).\nThe main advantages of marginaleffects over brmsmargins are:\n\nSupport for 60+ model types, rather than just the brms package.\nSimpler user interface (subjective).\nAt the time of writing (2022-05-25) brmsmargins did not support certain brms models such as those with multivariate or multinomial outcomes. It also did not support custom outcome transformations.\n\nThe rest of this section presents side-by-side replications of some of the analyses from the brmsmargins vignettes in order to show highlight parallels and differences in syntax.\n\n19.4.1 Marginal Effects for Fixed Effects Models\n\n19.4.1.1 AMEs for Logistic Regression\nEstimate a logistic regression model with brms:\n\nlibrary(brms)\nlibrary(brmsmargins)\nlibrary(marginaleffects)\nlibrary(data.table)\nlibrary(withr)\nsetDTthreads(5)\nh <- 1e-4\n\nvoid <- capture.output(\n    bayes.logistic <- brm(\n      vs ~ am + mpg, data = mtcars,\n      family = \"bernoulli\", seed = 1234,\n      silent = 2, refresh = 0,\n      backend = \"cmdstanr\",\n      chains = 4L, cores = 4L)\n)\n\nCompute AMEs manually:\n\nd1 <- d2 <- mtcars\nd2$mpg <- d2$mpg + h\np1 <- posterior_epred(bayes.logistic, newdata = d1)\np2 <- posterior_epred(bayes.logistic, newdata = d2)\nm <- (p2 - p1) / h\nquantile(rowMeans(m), c(.5, .025, .975))\n#>        50%       2.5%      97.5% \n#> 0.07010427 0.05418413 0.09092451\n\nCompute AMEs with brmsmargins:\n\nbm <- brmsmargins(\n  bayes.logistic,\n  add = data.frame(mpg = c(0, 0 + h)),\n  contrasts = cbind(\"AME MPG\" = c(-1 / h, 1 / h)),\n  CI = 0.95,\n  CIType = \"ETI\")\ndata.frame(bm$ContrastSummary)\n#>            M        Mdn         LL         UL PercentROPE PercentMID   CI CIType ROPE  MID   Label\n#> 1 0.07105468 0.07010427 0.05418413 0.09092451          NA         NA 0.95    ETI <NA> <NA> AME MPG\n\nCompute AMEs using marginaleffects:\n\navg_slopes(bayes.logistic) \n#> \n#>  Term Contrast Estimate   2.5 %  97.5 %\n#>   am     1 - 0  -0.2665 -0.4242 -0.0703\n#>   mpg    dY/dX   0.0701  0.0542  0.0909\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\nThe mpg element of the Effect column from marginaleffects matches the the M column of the output from brmsmargins.\n\n19.4.2 Marginal Effects for Mixed Effects Models\nEstimate a mixed effects logistic regression model with brms:\n\nd <- withr::with_seed(\n  seed = 12345, code = {\n    nGroups <- 100\n    nObs <- 20\n    theta.location <- matrix(rnorm(nGroups * 2), nrow = nGroups, ncol = 2)\n    theta.location[, 1] <- theta.location[, 1] - mean(theta.location[, 1])\n    theta.location[, 2] <- theta.location[, 2] - mean(theta.location[, 2])\n    theta.location[, 1] <- theta.location[, 1] / sd(theta.location[, 1])\n    theta.location[, 2] <- theta.location[, 2] / sd(theta.location[, 2])\n    theta.location <- theta.location %*% chol(matrix(c(1.5, -.25, -.25, .5^2), 2))\n    theta.location[, 1] <- theta.location[, 1] - 2.5\n    theta.location[, 2] <- theta.location[, 2] + 1\n    d <- data.table(\n      x = rep(rep(0:1, each = nObs / 2), times = nGroups))\n    d[, ID := rep(seq_len(nGroups), each = nObs)]\n\n    for (i in seq_len(nGroups)) {\n      d[ID == i, y := rbinom(\n        n = nObs,\n        size = 1,\n        prob = plogis(theta.location[i, 1] + theta.location[i, 2] * x))\n        ]\n    }\n    copy(d)\n  })\n\nvoid <- capture.output(\n    mlogit <- brms::brm(\n      y ~ 1 + x + (1 + x | ID), family = \"bernoulli\",\n      data = d, seed = 1234,\n      backend = \"cmdstanr\",\n      silent = 2, refresh = 0,\n      chains = 4L, cores = 4L)\n)\n\n\n19.4.2.1 AME: Including Random Effects\n\nbm <- brmsmargins(\n  mlogit,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  effects = \"includeRE\",\n  CI = .95,\n  CIType = \"ETI\")\ndata.frame(bm$ContrastSummary)\n#>          M       Mdn         LL        UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#> 1 0.111492 0.1115944 0.08095807 0.1420166          NA         NA 0.95    ETI <NA> <NA> AME x\n\navg_slopes(mlogit)\n#> \n#>  Term Contrast Estimate  2.5 % 97.5 %\n#>     x    1 - 0    0.111 0.0806   0.14\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\n\n19.4.2.2 AME: Fixed Effects Only (Grand Mean)\n\nbm <- brmsmargins(\n  mlogit,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  effects = \"fixedonly\",\n  CI = .95,\n  CIType = \"ETI\")\ndata.frame(bm$ContrastSummary)\n#>           M       Mdn         LL        UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#> 1 0.1039555 0.1034452 0.06319565 0.1491665          NA         NA 0.95    ETI <NA> <NA> AME x\n\navg_slopes(mlogit, re_formula = NA)\n#> \n#>  Term Contrast Estimate  2.5 % 97.5 %\n#>     x    1 - 0    0.101 0.0623  0.143\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\n\n19.4.3 Marginal Effects for Location Scale Models\n\n19.4.3.1 AMEs for Fixed Effects Location Scale Models\nEstimate a fixed effects location scale model with brms:\n\nd <- withr::with_seed(\n  seed = 12345, code = {\n    nObs <- 1000L\n    d <- data.table(\n      grp = rep(0:1, each = nObs / 2L),\n      x = rnorm(nObs, mean = 0, sd = 0.25))\n    d[, y := rnorm(nObs,\n                   mean = x + grp,\n                   sd = exp(1 + x + grp))]\n    copy(d)\n  })\n\nvoid <- capture.output(\n    ls.fe <- brm(bf(\n      y ~ 1 + x + grp,\n      sigma ~ 1 + x + grp),\n      family = \"gaussian\",\n      data = d, seed = 1234,\n      silent = 2, refresh = 0,\n      backend = \"cmdstanr\",\n      chains = 4L, cores = 4L)\n)\n\n\n19.4.3.2 Fixed effects only\n\nbm <- brmsmargins(\n  ls.fe,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  CI = 0.95, CIType = \"ETI\",\n  effects = \"fixedonly\")\ndata.frame(bm$ContrastSummary)\n#>          M     Mdn        LL      UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#> 1 1.626186 1.63215 0.7349262 2.46998          NA         NA 0.95    ETI <NA> <NA> AME x\n\navg_slopes(ls.fe, re_formula = NA)\n#> \n#>  Term Contrast Estimate 2.5 % 97.5 %\n#>   grp    1 - 0     1.02 0.355   1.70\n#>   x      dY/dX     1.63 0.735   2.47\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\n\n19.4.3.3 Discrete change and distributional parameter (dpar)\nCompute the contrast between adjusted predictions on the sigma parameter, when grp=0 and grp=1:\n\nbm <- brmsmargins(\n  ls.fe,\n  at = data.frame(grp = c(0, 1)),\n  contrasts = cbind(\"AME grp\" = c(-1, 1)),\n  CI = 0.95, CIType = \"ETI\", dpar = \"sigma\",\n  effects = \"fixedonly\")\ndata.frame(bm$ContrastSummary)\n#>          M     Mdn       LL       UL PercentROPE PercentMID   CI CIType ROPE  MID   Label\n#> 1 4.899239 4.89621 4.423663 5.422412          NA         NA 0.95    ETI <NA> <NA> AME grp\n\nIn marginaleffects we use the comparisons() function and the variables argument:\n\navg_comparisons(\n  ls.fe,\n  variables = list(grp = 0:1),\n  dpar = \"sigma\")\n#> \n#>  Term Contrast Estimate 2.5 % 97.5 %\n#>   grp    1 - 0      4.9  4.42   5.42\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high\n\n\n19.4.3.4 Marginal effect (continuous) on sigma\n\nbm <- brmsmargins(\n  ls.fe,\n  add = data.frame(x = c(0, h)),\n  contrasts = cbind(\"AME x\" = c(-1 / h, 1 / h)),\n  CI = 0.95, CIType = \"ETI\", dpar = \"sigma\",\n  effects = \"fixedonly\")\ndata.frame(bm$ContrastSummary)\n#>          M     Mdn       LL       UL PercentROPE PercentMID   CI CIType ROPE  MID Label\n#> 1 4.458758 4.46162 3.498163 5.443716          NA         NA 0.95    ETI <NA> <NA> AME x\n\navg_slopes(ls.fe, dpar = \"sigma\", re_formula = NA)\n#> \n#>  Term Contrast Estimate 2.5 % 97.5 %\n#>   grp    1 - 0     4.90  4.42   5.42\n#>   x      dY/dX     4.46  3.50   5.44\n#> \n#> Columns: term, contrast, estimate, conf.low, conf.high"
  },
  {
    "objectID": "articles/alternative_software.html#effects",
    "href": "articles/alternative_software.html#effects",
    "title": "\n19  Alternative Software\n",
    "section": "\n19.5 effects\n",
    "text": "19.5 effects\n\nThe effects package was created by John Fox and colleagues.\n\n\nmarginaleffects supports 30+ more model types than effects.\n\neffects focuses on the computation of “adjusted predictions.” The plots it produces are roughly equivalent to the ones produced by the plot_predictions() and predictions() functions in marginaleffects.\n\neffects does not appear support marginal effects (slopes), marginal means, or contrasts\n\neffects uses Base graphics whereas marginaleffects uses ggplot2\n\n\neffects includes a lot of very powerful options to customize plots. In contrast, marginaleffects produces objects which can be customized by chaining ggplot2 functions. Users can also call plot_predictions(model, draw=FALSE) to create a prediction grid, and then work the raw data directly to create the plot they need\n\neffects offers several options which are not currently available in marginaleffects, including:\n\nPartial residuals plots\nMany types of ways to plot adjusted predictions: package vignette"
  },
  {
    "objectID": "articles/alternative_software.html#modelbased",
    "href": "articles/alternative_software.html#modelbased",
    "title": "\n19  Alternative Software\n",
    "section": "\n19.6 modelbased\n",
    "text": "19.6 modelbased\n\nThe modelbased package is developed by the easystats team.\nThis section is incomplete; contributions are welcome.\n\nWrapper around emmeans to compute marginal means and marginal effects.\nPowerful functions to create beautiful plots."
  },
  {
    "objectID": "articles/alternative_software.html#ggeffects",
    "href": "articles/alternative_software.html#ggeffects",
    "title": "\n19  Alternative Software\n",
    "section": "\n19.7 ggeffects\n",
    "text": "19.7 ggeffects\n\nThe ggeffects package is developed by Daniel Lüdecke.\nThis section is incomplete; contributions are welcome.\n\nWrapper around emmeans to compute marginal means."
  },
  {
    "objectID": "articles/extensions.html#support-a-new-model-type",
    "href": "articles/extensions.html#support-a-new-model-type",
    "title": "\n20  Extensions\n",
    "section": "\n20.1 Support a new model type",
    "text": "20.1 Support a new model type\nIt is very easy to add support for new models in marginaleffects. All we need is to set a global option and define 4 very simple functions.\nIf you add support for a class of models produced by a CRAN package, please consider submitting your code for inclusion in the package: https://github.com/vincentarelbundock/marginaleffects\nIf you add support for a class of models produced by a package hosted elsewhere than CRAN, you can submit it for inclusion in the unsupported user-submitted library of extensions: Currently\n\n\ncountreg package. Thanks to Olivier Beaumais.\n\ncensreg package. Thanks to Oleg Komashko.\n\nThe rest of this section illustrates how to add support for a very simple lm_manual model.\n\n20.1.1 Fit function\nTo begin, we define a function which fits a model. Normally, this function will be supplied by a modeling package published on CRAN. Here, we create a function called lm_manual(), which estimates a linear regression model using simple linear algebra operates:\n\nlm_manual <- function(f, data, ...) {\n    # design matrix\n    X <- model.matrix(f, data = data)\n    # response matrix\n    Y <- data[[as.character(f[2])]]\n    # coefficients\n    b <- solve(crossprod(X)) %*% crossprod(X, Y)\n    Yhat <- X %*% b\n    # variance-covariance matrix\n    e <- Y - Yhat\n    df <- nrow(X) - ncol(X)\n    s2 <- sum(e^2) / df\n    V <- s2 * solve(crossprod(X))\n    # model object\n    out <- list(\n        d = data,\n        f = f,\n        X = X,\n        Y = Y,\n        V = V,\n        b = b)\n    # class name: lm_manual\n    class(out) <- c(\"lm_manual\", \"list\")\n    return(out)\n}\n\nImportant: The custom fit function must assign a new class name to the object it returns. In the example above, the model is assigned to be of class lm_manual (see the penultimate line of code in the function).\nOur new function replicates the results of lm():\n\nmodel <- lm_manual(mpg ~ hp + drat, data = mtcars)\nmodel$b\n#>                    [,1]\n#> (Intercept) 10.78986122\n#> hp          -0.05178665\n#> drat         4.69815776\n\nmodel_lm <- lm(mpg ~ hp + drat, data = mtcars)\ncoef(model_lm)\n#> (Intercept)          hp        drat \n#> 10.78986122 -0.05178665  4.69815776\n\n\n20.1.2 marginaleffects extension\nTo extend support in marginaleffects, the first step is to tell the package that our new class is supported. We do this by defining a global option:\n\nlibrary(marginaleffects)\n\noptions(\"marginaleffects_model_classes\" = \"lm_manual\")\n\nThen, we define 4 methods:\n\n\nget_coef()\n\nMandatory arguments: model, ...\n\nReturns: named vector of parameters (coefficients).\n\n\n\nset_coef()\n\nMandatory arguments: model, coefs (named vector of coefficients), ...\n\nReturns: A new model object in which the original coefficients were replaced by the new vector.\nExample\n\n\n\nget_vcov()\n\nMandatory arguments: model, ....\nOptional arguments: vcov\n\nReturns: A named square variance-covariance matrix.\n\n\n\nget_predict()\n\nMandatory arguments: model, newdata (data frame), ...\n\nOption arguments: type and other model-specific arguments.\nReturns: A data frame with two columns: a unique rowid and a column of estimate values.\n\n\n\nNote that each of these methods will be named with the suffix .lm_manual to indicate that they should be used whenever marginaleffects needs to process an object of class lm_manual.\n\nget_coef.lm_manual <- function(model, ...) {\n    b <- model$b\n    b <- setNames(as.vector(b), row.names(b))\n    return(b)\n}\n\nset_coef.lm_manual <- function(model, coefs, ...) {\n    out <- model\n    out$b <- coefs\n    return(out)\n}\n\nget_vcov.lm_manual <- function(model, ...) {\n    return(model$V)\n}\n\nget_predict.lm_manual <- function(model, newdata, ...) {\n    newX <- model.matrix(model$f, data = newdata)\n    Yhat <- newX %*% model$b\n    out <- data.frame(\n        rowid = seq_len(nrow(Yhat)),\n        estimate = as.vector(Yhat))\n    return(out)\n}\n\nThe methods we just defined work as expected:\n\nget_coef(model)\n#> (Intercept)          hp        drat \n#> 10.78986122 -0.05178665  4.69815776\n\nget_vcov(model)\n#>             (Intercept)            hp         drat\n#> (Intercept) 25.78356135 -3.054007e-02 -5.836030687\n#> hp          -0.03054007  8.635615e-05  0.004969385\n#> drat        -5.83603069  4.969385e-03  1.419990359\n\nget_predict(model, newdata = head(mtcars))\n#>   rowid estimate\n#> 1     1 23.41614\n#> 2     2 23.41614\n#> 3     3 24.06161\n#> 4     4 19.56366\n#> 5     5 16.52639\n#> 6     6 18.31918\n\nNow we can use the avg_slopes() function:\n\navg_slopes(model, newdata = mtcars, variables = c(\"hp\", \"drat\"))\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S 2.5 %  97.5 %\n#>  hp    -0.0518    0.00929 -5.57   <0.001 25.2 -0.07 -0.0336\n#>  drat   4.6982    1.19163  3.94   <0.001 13.6  2.36  7.0337\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\npredictions(model, newdata = mtcars) |> head()\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  mpg cyl disp  hp drat   wt qsec vs am gear carb\n#>      23.4      0.671 34.9   <0.001 883.6  22.1   24.7 21.0   6  160 110 3.90 2.62 16.5  0  1    4    4\n#>      23.4      0.671 34.9   <0.001 883.6  22.1   24.7 21.0   6  160 110 3.90 2.88 17.0  0  1    4    4\n#>      24.1      0.720 33.4   <0.001 810.2  22.6   25.5 22.8   4  108  93 3.85 2.32 18.6  1  1    4    1\n#>      19.6      0.999 19.6   <0.001 281.4  17.6   21.5 21.4   6  258 110 3.08 3.21 19.4  1  0    3    1\n#>      16.5      0.735 22.5   <0.001 369.1  15.1   18.0 18.7   8  360 175 3.15 3.44 17.0  0  0    3    2\n#>      18.3      1.343 13.6   <0.001 138.3  15.7   21.0 18.1   6  225 105 2.76 3.46 20.2  1  0    3    1\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, cyl, disp, hp, drat, wt, qsec, vs, am, gear, carb\n\nNote that, for custom model, we typically have to supply values for the newdata and variables arguments explicitly."
  },
  {
    "objectID": "articles/extensions.html#modify-or-extend-supported-models",
    "href": "articles/extensions.html#modify-or-extend-supported-models",
    "title": "\n20  Extensions\n",
    "section": "\n20.2 Modify or extend supported models",
    "text": "20.2 Modify or extend supported models\nLet’s say you want to estimate a model using the mclogit::mblogit function. That package is already supported by marginaleffects, but you want to use a type (scale) of predictions that is not currently supported: a “centered link scale.”\nTo achieve this, we would need to override the get_predict.mblogit() method. However, it can be unsafe to reassign methods supplied by a package that we loaded with library. To be safe, we assign a new model class to our object (“customclass”) which will inherit from mblogit. Then, we define a get_predict.customclass method to make our new kinds of predictions.\nLoad libraries, estimate a model:\n\nlibrary(mclogit)\nlibrary(data.table)\n\nmodel <- mblogit(\n    factor(gear) ~ am + mpg,\n    data = mtcars,\n    trace = FALSE)\n\nTell marginaleffects that we are adding support for a new class model models, and assign a new inherited class name to a duplicate of the model object:\n\noptions(\"marginaleffects_model_classes\" = \"customclass\")\n\nmodel_custom <- model\n\nclass(model_custom) <- c(\"customclass\", class(model))\n\nDefine a new get_predict.customclass method. We use the default predict() function to obtain predictions. Since this is a multinomial model, predict() returns a matrix of predictions with one column per level of the response variable.\nOur new get_predict.customclass method takes this matrix of predictions, modifies it, and reshapes it to return a data frame with three columns: rowid, group, and estimate:\n\nget_predict.customclass <- function(model, newdata, ...) {\n    out <- predict(model, newdata = newdata, type = \"link\")\n    out <- cbind(0, out)\n    colnames(out)[1] <- dimnames(model$D)[[1]][[1]]\n    out <- out - rowMeans(out)\n    out <- as.data.frame(out)\n    out$rowid <- seq_len(nrow(out))\n    out <- data.table(out)\n    out <- melt(\n        out,\n        id.vars = \"rowid\",\n        value.name = \"estimate\",\n        variable.name = \"group\")\n}\n\nFinally, we can call any slopes() function and obtain results. Notice that our object of class customclass now produces different results than the default mblogit object:\n\navg_predictions(model)\n#> \n#>  Group Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n#>      3    0.469     0.0444 10.56  < 0.001 84.2 0.3817  0.556\n#>      4    0.375     0.0671  5.59  < 0.001 25.4 0.2436  0.506\n#>      5    0.156     0.0503  3.11  0.00188  9.1 0.0577  0.255\n#> \n#> Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\navg_predictions(model_custom)\n#> \n#>  Group Estimate Std. Error         z Pr(>|z|)   S 2.5 % 97.5 %\n#>      3    -1.42       2525 -0.000561    1.000 0.0 -4950   4947\n#>      4     6.36       1779  0.003578    0.997 0.0 -3480   3493\n#>      5    -4.95       3074 -0.001609    0.999 0.0 -6030   6020\n#> \n#> Columns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/links.html",
    "href": "articles/links.html",
    "title": "21  Links",
    "section": "",
    "text": "Causal inference with potential outcomes bootcamp by Solomon Kurz\nCausal inference with potential outcomes bootcamp by Solomon Kurz\nMarginal and conditional effects for GLMMs with marginaleffects by Andrew Heiss\nMarginalia: A guide to figuring out what the heck marginal effects, marginal slopes, average marginal effects, marginal effects at the mean, and all these other marginal things are by Andrew Heiss\nMatching by Noah Greifer\nDouble propensity score adjustment using g-computation by Noah Greifer\nSubgroup Analysis After Propensity Score Matching Using R by Noah Greifer\nBayesian Model Averaged Marginal Effects by A. Jordan Nafa"
  },
  {
    "objectID": "articles/performance.html#what-to-do-when-marginaleffects-is-slow",
    "href": "articles/performance.html#what-to-do-when-marginaleffects-is-slow",
    "title": "\n22  Performance\n",
    "section": "\n22.1 What to do when marginaleffects is slow?",
    "text": "22.1 What to do when marginaleffects is slow?\nSome options:\n\nCompute marginal effects and contrasts at the mean (or other representative value) instead of all observed rows of the original dataset: Use the newdata argument and the datagrid() function.\nCompute marginal effects for a subset of variables, paying special attention to exclude factor variables which can be particularly costly to process: Use the variables argument.\nDo not compute standard errors: Use the vcov = FALSE argument.\n\nThis simulation illustrates how computation time varies for a model with 25 regressors and 100,000 observations:\n\nlibrary(marginaleffects)\n\n## simulate data and fit a large model\nN <- 1e5\ndat <- data.frame(matrix(rnorm(N * 26), ncol = 26))\nmod <- lm(X1 ~ ., dat)\n\nresults <- bench::mark(\n    # marginal effects at the mean; no standard error\n    slopes(mod, vcov = FALSE, newdata = \"mean\"),\n    # marginal effects at the mean\n    slopes(mod, newdata = \"mean\"),\n    # 1 variable; no standard error\n    slopes(mod, vcov = FALSE, variables = \"X3\"),\n    # 1 variable\n    slopes(mod, variables = \"X3\"),\n    # 26 variables; no standard error\n    slopes(mod, vcov = FALSE),\n    # 26 variables\n    slopes(mod),\n    iterations = 1, check = FALSE)\n\nresults[, c(1, 3, 5)]\n## <bch:expr>                                  <bch:tm> <bch:byt>\n## slopes(mod, vcov = FALSE, newdata = \"mean\") 230.09ms    1.24GB\n## slopes(mod, newdata = \"mean\")               329.14ms    1.25GB\n## slopes(mod, vcov = FALSE, variables = \"X3\")  198.7ms  496.24MB\n## slopes(mod, variables = \"X3\")                  1.27s    3.29GB\n## slopes(mod, vcov = FALSE)                      5.73s   11.05GB\n## slopes(mod)                                   21.68s   78.02GB\n\nThe benchmarks above were conducted using the development version of marginaleffects on 2023-02-03."
  },
  {
    "objectID": "articles/performance.html#speed-comparison",
    "href": "articles/performance.html#speed-comparison",
    "title": "\n22  Performance\n",
    "section": "\n22.2 Speed comparison",
    "text": "22.2 Speed comparison\nThe slopes() function is relatively fast. This simulation was conducted using the development version of the package on 2022-04-04:\n\nlibrary(margins)\n\nN <- 1e3\ndat <- data.frame(\n    y = sample(0:1, N, replace = TRUE),\n    x1 = rnorm(N),\n    x2 = rnorm(N),\n    x3 = rnorm(N),\n    x4 = factor(sample(letters[1:5], N, replace = TRUE)))\nmod <- glm(y ~ x1 + x2 + x3 + x4, data = dat, family = binomial)\n\nmarginaleffects is about the same speed as margins when unit-level standard errors are not computed:\n\nresults <- bench::mark(\n    slopes(mod, vcov = FALSE),\n    margins(mod, unit_ses = FALSE),\n    check = FALSE, relative = TRUE)\nresults[, c(1, 3, 5)]\n\n##   expression                        median mem_alloc\n##   <bch:expr>                          <dbl>     <dbl>\n## 1 slopes(mod, vcov = FALSE)   1         1\n## 2 margins(mod, unit_ses = FALSE)       6.15      4.17\n\nmarginaleffects can be 100x times faster than margins when unit-level standard errors are computed:\n\nresults <- bench::mark(\n    slopes(mod, vcov = TRUE),\n    margins(mod, unit_ses = TRUE),\n    check = FALSE, relative = TRUE, iterations = 1)\nresults[, c(1, 3, 5)]\n\n## <bch:expr>                     <dbl>     <dbl>\n## slopes(mod, vcov = TRUE)          1        1  \n## margins(mod, unit_ses = TRUE)   128.      18.6\n\nModels estimated on larger datasets (> 1000 observations) can be difficult to process using the margins package, because of memory and time constraints. In contrast, marginaleffects can work well on much larger datasets.\nIn some cases, marginaleffects will be considerably slower than packages like emmeans or modmarg. This is because these packages make extensive use of hard-coded analytical derivatives, or reimplement their own fast prediction functions."
  },
  {
    "objectID": "articles/uncertainty.html#delta-method",
    "href": "articles/uncertainty.html#delta-method",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.1 Delta Method",
    "text": "23.1 Delta Method\nAll the standard errors generated by the slopes(), comparisons(), and hypotheses() functions of this package package are estimated using the delta method. Mathematical treatments of this method can be found in most statistics textbooks and on Wikipedia. Roughly speaking, the delta method allows us to approximate the distribution of a smooth function of an asymptotically normal estimator.\nConcretely, this allows us to generate standard errors around functions of a model’s coefficient estimates. Predictions, contrasts, marginal effects, and marginal means are functions of the coefficients, so we can use the delta method to estimate standard errors around all of those quantities. Since there are a lot of mathematical treatments available elsewhere, this vignette focuses on the “implementation” in marginaleffects.\nConsider the case of the marginal_means() function. When a user calls this function, they obtain a vector of marginal means. To estimate standard errors around this vector:\n\nTake the numerical derivative of the marginal means vector with respect to the first coefficient in the model:\n\nCompute marginal means with the original model: \\(f(\\beta)\\)\n\nIncrement the first (and only the first) coefficient held inside the model object by a small amount, and compute marginal means again: \\(f(\\beta+\\varepsilon)\\)\n\nCalculate: \\(\\frac{f(\\beta+\\varepsilon) - f(\\beta)}{\\varepsilon}\\)\n\n\n\nRepeat step 1 for every coefficient in the model to construct a \\(J\\) matrix.\nExtract the variance-covariance matrix of the coefficient estimates: \\(V\\)\n\nStandard errors are the square root of the diagonal of \\(JVJ'\\)\n\n\nScroll down this page to the Numerical Derivatives section to see a detailed explanation, along with code for manual computation."
  },
  {
    "objectID": "articles/uncertainty.html#standard-errors-and-intervals-for-slopes-and-comparisons",
    "href": "articles/uncertainty.html#standard-errors-and-intervals-for-slopes-and-comparisons",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.2 Standard errors and intervals for slopes() and comparisons()\n",
    "text": "23.2 Standard errors and intervals for slopes() and comparisons()\n\nAll standard errors for the slopes() and comparisons() functions are computed using the delta method, as described above."
  },
  {
    "objectID": "articles/uncertainty.html#standard-errors-and-intervals-for-marginal_means-and-predictions",
    "href": "articles/uncertainty.html#standard-errors-and-intervals-for-marginal_means-and-predictions",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.3 Standard errors and intervals for marginal_means() and predictions()\n",
    "text": "23.3 Standard errors and intervals for marginal_means() and predictions()\n\nThe marginal_means() and predictions() function can compute the confidence intervals in two ways. If the following conditions hold:\n\nThe user sets: type = \"response\"\n\nThe model class is glm\n\nThe transform argument is NULL\n\n\nthen marginal_means() and predictions() will first compute estimates on the link scale, and then back transform them using the inverse link function supplied by insight::link_inverse(model) function.\nIn all other cases, standard errors are computed using the delta method as described above."
  },
  {
    "objectID": "articles/uncertainty.html#robust-standard-errors",
    "href": "articles/uncertainty.html#robust-standard-errors",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.4 Robust standard errors",
    "text": "23.4 Robust standard errors\nAll the functions in the marginaleffects package can compute robust standard errors on the fly for any model type supported by the sandwich package. The vcov argument supports string shortcuts like \"HC3\", a one-sided formula to request clustered standard errors, variance-covariance matrices, or functions which return such matrices. Here are a few examples.\nAdjusted predictions with classical or heteroskedasticity-robust standard errors:\n\nlibrary(marginaleffects)\nlibrary(patchwork)\nmod <- lm(mpg ~ hp, data = mtcars)\n\np <- predictions(mod)\nhead(p, 2)\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      22.6      0.777 29.1   <0.001 614.7  21.1   24.1\n#>      22.6      0.777 29.1   <0.001 614.7  21.1   24.1\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp\n\np <- predictions(mod, vcov = \"HC3\")\nhead(p, 2)\n#> \n#>  Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n#>      22.6      0.863 26.2   <0.001 499.5  20.9   24.3\n#>      22.6      0.863 26.2   <0.001 499.5  20.9   24.3\n#> \n#> Columns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp\n\nMarginal effects with cluster-robust standard errors:\n\navg_slopes(mod, vcov = ~cyl)\n#> \n#>  Term Estimate Std. Error     z Pr(>|z|)    S  2.5 %  97.5 %\n#>    hp  -0.0682     0.0187 -3.65   <0.001 11.9 -0.105 -0.0316\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nComparing adjusted predictions with classical and robust standard errors:\n\np1 <- plot_predictions(mod, condition = \"hp\")\np2 <- plot_predictions(mod, condition = \"hp\", vcov = \"HC3\")\np1 + p2"
  },
  {
    "objectID": "articles/uncertainty.html#simulation-based-inference",
    "href": "articles/uncertainty.html#simulation-based-inference",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.5 Simulation-based inference",
    "text": "23.5 Simulation-based inference\nmarginaleffects offers an experimental inferences() function to conduct simulation-based inference following the strategy proposed by Krinsky & Robb (1986):\n\nDraw iter sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model’s estimated coefficients and variance equal to the model’s variance-covariance matrix (classical, “HC3”, or other).\nUse the iter sets of coefficients to compute iter sets of estimands: predictions, comparisons, or slopes.\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\nHere are a few examples:\n\nlibrary(marginaleffects)\nlibrary(ggplot2)\nlibrary(ggdist)\n\nmod <- glm(vs ~ hp * wt + factor(gear), data = mtcars, family = binomial)\n\nmod |> predictions() |> inferences(method = \"simulation\")\n#> \n#>  Estimate Std. Error    2.5 % 97.5 %\n#>  7.84e-01      0.193 2.67e-01  0.972\n#>  7.84e-01      0.164 3.47e-01  0.962\n#>  8.98e-01      0.134 4.83e-01  0.990\n#>  8.74e-01      0.230 1.84e-01  0.997\n#>  1.31e-02      0.187 4.70e-05  0.767\n#> --- 22 rows omitted. See ?avg_predictions and ?print.marginaleffects --- \n#>  3.83e-01      0.295 2.03e-02  0.958\n#>  1.21e-06      0.140 2.36e-12  0.481\n#>  6.89e-03      0.160 3.10e-05  0.661\n#>  8.07e-11      0.159 2.22e-16  0.698\n#>  7.95e-01      0.166 3.55e-01  0.967\n#> Columns: rowid, estimate, std.error, conf.low, conf.high, vs, hp, wt, gear\n\nmod |> avg_slopes(vcov = ~gear) |> inferences(method = \"simulation\")\n#> \n#>  Term Contrast Estimate Std. Error   2.5 %  97.5 %\n#>  gear    4 - 3 -0.04159     0.0537 -0.0897 0.12365\n#>  gear    5 - 3 -0.20145     0.2728 -0.4877 0.32987\n#>  hp      dY/dX -0.00465     0.0046 -0.0118 0.00504\n#>  wt      dY/dX  0.01096     0.3226 -0.6647 0.65808\n#> \n#> Columns: term, contrast, estimate, std.error, conf.low, conf.high\n\nSince simulation based inference generates iter estimates of the quantities of interest, we can treat them similarly to draws from the posterior distribution in bayesian models. For example, we can extract draws using the posterior_draws() function, and plot their distributions using packages likeggplot2 and ggdist:\n\nmod |>\n  avg_comparisons(variables = \"gear\") |>\n  inferences(method = \"simulation\") |>\n  posterior_draws(\"rvar\") |>\n  ggplot(aes(y = contrast, xdist = rvar)) +\n  stat_slabinterval()"
  },
  {
    "objectID": "articles/uncertainty.html#bootstrap",
    "href": "articles/uncertainty.html#bootstrap",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.6 Bootstrap",
    "text": "23.6 Bootstrap\nIt is easy to use the bootstrap as an alternative strategy to compute standard errors and confidence intervals. Several R packages can help us achieve this, including the long-established boot package:\n\nlibrary(boot)\nset.seed(123)\n\nbootfun <- function(data, indices, ...) {\n    d <- data[indices, ]\n    mod <- lm(mpg ~ am + hp + factor(cyl), data = d)\n    cmp <- comparisons(mod, newdata = d, vcov = FALSE, variables = \"am\")\n    tidy(cmp)$estimate\n}\n\nb <- boot(data = mtcars, statistic = bootfun, R = 1000)\n\nb\n#> \n#> ORDINARY NONPARAMETRIC BOOTSTRAP\n#> \n#> \n#> Call:\n#> boot(data = mtcars, statistic = bootfun, R = 1000)\n#> \n#> \n#> Bootstrap Statistics :\n#>     original     bias    std. error\n#> t1* 4.157856 0.01543426    1.003461\nboot.ci(b, type = \"perc\")\n#> BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\n#> Based on 1000 bootstrap replicates\n#> \n#> CALL : \n#> boot.ci(boot.out = b, type = \"perc\")\n#> \n#> Intervals : \n#> Level     Percentile     \n#> 95%   ( 2.240,  6.277 )  \n#> Calculations and Intervals on Original Scale\n\nNote that, in the code above, we set vcov=FALSE to avoid computation of delta method standard errors and speed things up.\nCompare to the delta method standard errors:\n\nmod <- lm(mpg ~ am + hp + factor(cyl), data = mtcars)\navg_comparisons(mod, variables = \"am\")\n#> \n#>  Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n#>    am    1 - 0     4.16       1.26 3.31   <0.001 10.1   1.7   6.62\n#> \n#> Columns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "articles/uncertainty.html#mixed-effects-models-satterthwaite-and-kenward-roger-corrections",
    "href": "articles/uncertainty.html#mixed-effects-models-satterthwaite-and-kenward-roger-corrections",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.7 Mixed effects models: Satterthwaite and Kenward-Roger corrections",
    "text": "23.7 Mixed effects models: Satterthwaite and Kenward-Roger corrections\nFor linear mixed effects models we can apply the Satterthwaite and Kenward-Roger corrections in the same way as above:\n\nlibrary(marginaleffects)\nlibrary(patchwork)\nlibrary(lme4)\n\ndat <- mtcars\ndat$cyl <- factor(dat$cyl)\ndat$am <- as.logical(dat$am)\nmod <- lmer(mpg ~ hp + am + (1 | cyl), data = dat)\n\nMarginal effects at the mean with classical standard errors and z-statistic:\n\nslopes(mod, newdata = \"mean\")\n#> \n#>  Term     Contrast Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %\n#>    hp dY/dX         -0.0518     0.0115 -4.52   <0.001 17.3 -0.0743 -0.0294\n#>    am TRUE - FALSE   4.6661     1.1343  4.11   <0.001 14.6  2.4430  6.8892\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, am, cyl\n\nMarginal effects at the mean with Kenward-Roger adjusted variance-covariance and degrees of freedom:\n\nslopes(mod,\n                newdata = \"mean\",\n                vcov = \"kenward-roger\")\n#> \n#>  Term     Contrast Estimate Std. Error     t Pr(>|t|)   S  2.5 %  97.5 %   Df\n#>    hp dY/dX         -0.0518     0.0152 -3.41   0.0964 3.4 -0.131  0.0269 1.68\n#>    am TRUE - FALSE   4.6661     1.2824  3.64   0.0874 3.5 -1.980 11.3121 1.68\n#> \n#> Columns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, df, predicted, predicted_hi, predicted_lo, mpg, hp, am, cyl\n\nWe can use the same option in any of the package’s core functions, including:\n\nplot_predictions(mod, condition = \"hp\", vcov = \"satterthwaite\")"
  },
  {
    "objectID": "articles/uncertainty.html#numerical-derivatives-sensitivity-to-step-size",
    "href": "articles/uncertainty.html#numerical-derivatives-sensitivity-to-step-size",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.8 Numerical derivatives: Sensitivity to step size",
    "text": "23.8 Numerical derivatives: Sensitivity to step size\n\ndat <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/palmerpenguins/penguins.csv\")\ndat$large_penguin <- ifelse(dat$body_mass_g > median(dat$body_mass_g, na.rm = TRUE), 1, 0)\nmod <- glm(large_penguin ~ bill_length_mm * flipper_length_mm + species, data = dat, family = binomial)\n\nmarginaleffects uses numerical derivatives in two contexts:\n\nEstimate the partial derivatives reported by slopes() function.\n\nCentered finite difference\n\n\\(\\frac{f(x + \\varepsilon_1 / 2) - f(x - \\varepsilon_1 / 2)}{\\varepsilon_1}\\), where we take the derivative with respect to a predictor of interest, and \\(f\\) is the predict() function.\n\n\nEstimate standard errors using the delta method.\n\nForward finite difference\n\n\\(\\frac{g(\\hat{\\beta}) - g(\\hat{\\beta} + \\varepsilon_2)}{\\varepsilon_2}\\), where we take the derivative with respect to a model’s coefficients, and \\(g\\) is a marginaleffects function which returns some quantity of interest (e.g., slope, marginal means, predictions, etc.)\n\n\n\nNote that the step sizes used in those two contexts can differ. If the variables and coefficients have very different scales, it may make sense to use different values for \\(\\varepsilon_1\\) and \\(\\varepsilon_2\\).\nBy default, \\(\\varepsilon_1\\) is set to 1e-4 times the range of the variable with respect to which we are taking the derivative. By default, \\(\\varepsilon_2\\) is set to the maximum value of 1e-8, or 1e-4 times the smallest absolute coefficient estimate. (These choices are arbitrary, but I have found that in practice, smaller values can produce unstable results.)\n\\(\\varepsilon_1\\) can be controlled by the eps argument of the slopes() function. \\(\\varepsilon_2\\) can be controlled by setting a global option which tells marginaleffects to compute the jacobian using the numDeriv package instead of its own internal functions. This allows more control over the step size, and also gives access to other differentiation methods, such as Richardson’s. To use numDeriv, we define a list of arguments which will be pushed forward to numDeriv::jacobian:\n\navg_slopes(mod, variables = \"bill_length_mm\")\n#> \n#>            Term Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>  bill_length_mm   0.0279    0.00595 4.68   <0.001 18.4 0.0162 0.0395\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\noptions(marginaleffects_numDeriv = list(method = \"Richardson\"))\navg_slopes(mod, variables = \"bill_length_mm\")\n#> \n#>            Term Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>  bill_length_mm   0.0279    0.00595 4.68   <0.001 18.4 0.0162 0.0395\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-3)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#> \n#>            Term Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n#>  bill_length_mm   0.0279      0.568 0.049    0.961 0.1 -1.09   1.14\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-5)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#> \n#>            Term Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>  bill_length_mm   0.0279    0.00601 4.64   <0.001 18.1 0.0161 0.0396\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-7)))\navg_slopes(mod, variables = \"bill_length_mm\")\n#> \n#>            Term Estimate Std. Error    z Pr(>|z|)    S  2.5 % 97.5 %\n#>  bill_length_mm   0.0279    0.00595 4.68   <0.001 18.4 0.0162 0.0395\n#> \n#> Columns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high\n\nNotice that the standard errors can vary considerably when using different step sizes. It is good practice for analysts to consider the sensitivity of their results to this setting.\nNow, we illustrate the full process of standard error computation, using raw R code. First, we choose two step sizes:\n\neps1 <- 1e-5 # slope\neps2 <- 1e-7 # delta method\n\ns <- slopes(mod, newdata = head(dat, 3), variables = \"bill_length_mm\", eps = eps1)\nprint(s[, 1:5], digits = 6)\n#> \n#>            Term  Estimate Std. Error       z\n#>  bill_length_mm 0.0179765  0.0086771 2.07172\n#>  bill_length_mm 0.0359630  0.0126120 2.85150\n#>  bill_length_mm 0.0849071  0.0213175 3.98298\n#> \n#> Columns: rowid, term, estimate, std.error, statistic\n\nWe can get the same estimates manually with these steps:\n\nlinkinv <- mod$family$linkinv\n\n## increment the variable of interest by h\ndat_hi <- transform(dat, bill_length_mm = bill_length_mm + eps1)\n\n## model matrices: first 3 rows\nmm_lo <- insight::get_modelmatrix(mod, data = dat)[1:3,]\nmm_hi <- insight::get_modelmatrix(mod, data = dat_hi)[1:3,]\n\n## predictions\np_lo <- linkinv(mm_lo %*% coef(mod))\np_hi <- linkinv(mm_hi %*% coef(mod))\n\n## slopes\n(p_hi - p_lo) / eps1\n#>         [,1]\n#> 1 0.01797653\n#> 2 0.03596304\n#> 3 0.08490712\n\nTo get standard errors, we build a jacobian matrix where each column holds derivatives of the vector valued slope function, with respect to each of the coefficients. Using the same example:\n\nb_lo <- b_hi <- coef(mod)\nb_hi[1] <- b_hi[1] + eps2\n\ndydx_lo <- (linkinv(mm_hi %*% b_lo) - linkinv(mm_lo %*% b_lo)) / eps1\ndydx_hi <- (linkinv(mm_hi %*% b_hi) - linkinv(mm_lo %*% b_hi)) / eps1\n(dydx_hi - dydx_lo) / eps2\n#>         [,1]\n#> 1 0.01600109\n#> 2 0.02771394\n#> 3 0.02275957\n\nThis gives us the first column of \\(J\\), which we can recover in full from the marginaleffects object attribute:\n\nJ <- attr(s, \"jacobian\")\nJ\n#>      (Intercept) bill_length_mm flipper_length_mm speciesChinstrap speciesGentoo bill_length_mm:flipper_length_mm\n#> [1,]  0.01600109      0.6775622          2.897231                0             0                         122.6916\n#> [2,]  0.02771394      1.1957935          5.153128                0             0                         222.4993\n#> [3,]  0.02275957      1.1500800          4.440004                0             0                         224.0828\n\nTo build the full matrix, we would simply iterate through the coefficients, incrementing them one after the other. Finally, we get standard errors via:\n\nsqrt(diag(J %*% vcov(mod) %*% t(J)))\n#> [1] 0.008677096 0.012611983 0.021317511\n\nWhich corresponds to our original standard errors:\n\nprint(s[, 1:5], digits = 7)\n#> \n#>            Term   Estimate  Std. Error        z\n#>  bill_length_mm 0.01797653 0.008677096 2.071722\n#>  bill_length_mm 0.03596304 0.012611983 2.851498\n#>  bill_length_mm 0.08490712 0.021317511 3.982975\n#> \n#> Columns: rowid, term, estimate, std.error, statistic\n\nReverting to default settings:\n\noptions(marginaleffects_numDeriv = NULL)\n\nNote that our default results for this model are very similar – but not exactly identical – to those generated by the margins. As should be expected, the results in margins are also very sensitive to the value of eps for this model:\n\nlibrary(margins)\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), unit_ses = TRUE)$SE_dydx_bill_length_mm\n#> [1] 0.008727977 0.012567079 0.021293275\n\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), eps = 1e-4, unit_ses = TRUE)$SE_dydx_bill_length_mm\n#> [1] 0.2269512 0.2255849 0.6636208\n\nmargins(mod, variables = \"bill_length_mm\", data = head(dat, 3), eps = 1e-5, unit_ses = TRUE)$SE_dydx_bill_length_mm\n#> [1] 0.02317078 0.02928267 0.05480282"
  },
  {
    "objectID": "articles/uncertainty.html#bayesian-estimates-and-credible-intervals",
    "href": "articles/uncertainty.html#bayesian-estimates-and-credible-intervals",
    "title": "\n23  Standard Errors\n",
    "section": "\n23.9 Bayesian estimates and credible intervals",
    "text": "23.9 Bayesian estimates and credible intervals\nSee the brms vignette for a discussion of bayesian estimates and credible intervals."
  },
  {
    "objectID": "articles/supported_models.html",
    "href": "articles/supported_models.html",
    "title": "\n24  Supported Models\n",
    "section": "",
    "text": "This table shows the list of 80 supported model types. There are three main alternative software packages to compute such slopes (1) Stata’s margins command, (2) R’s margins::margins() function, and (3) R’s emmeans::emtrends() function. The test suite hosted on Github compares the numerical equivalence of results produced by marginaleffects::slopes() to those produced by all 3 alternative software packages:\n\n✓: a green check means that the results of at least one model are equal to a reasonable tolerance.\n✖: a red cross means that the results are not identical; extra caution is warranted.\nU: a grey U means that computing slopes for a model type is unsupported by alternative packages, but supported by marginaleffects.\nAn empty cell means means that no comparison has been made yet.\n\nI am eager to add support for new models. Feel free to file a request or submit code on Github.\n\n\n\n\n\n\n\nNumerical equivalence\n\n\nSupported by marginaleffects\nStata\nmargins\nemtrends\n\n\n Package \n    Function \n    dY/dX  \n    SE  \n    dY/dX   \n    SE   \n    dY/dX    \n    SE    \n  \n\n\n\n stats \n    lm \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n  \n    glm \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n  \n    loess \n     \n     \n    ✓ \n     \n    U \n    U \n  \n\n AER \n    ivreg \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    U \n    U \n  \n\n  \n    tobit \n    ✓ \n    ✓ \n    U \n    U \n    ✓ \n    ✓ \n  \n\n afex \n    afex_aov \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n aod \n    betabin \n     \n     \n    U \n    U \n    U \n    U \n  \n\n betareg \n    betareg \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n bife \n    bife \n     \n     \n    U \n    U \n    U \n    U \n  \n\n biglm \n    biglm \n     \n     \n    U \n    U \n    U \n    U \n  \n\n  \n    bigglm \n     \n     \n    U \n    U \n    U \n    U \n  \n\n blme \n    blmer \n     \n     \n     \n     \n     \n     \n  \n\n  \n    bglmer \n     \n     \n     \n     \n     \n     \n  \n\n brglm2 \n    bracl \n     \n     \n    U \n    U \n    U \n    U \n  \n\n  \n    brglmFit \n     \n     \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n  \n    brnb \n     \n     \n    ✓ \n    ✓ \n    U \n    U \n  \n\n  \n    brmultinom \n     \n     \n    U \n    U \n    U \n    U \n  \n\n brms \n    brm \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n crch \n    crch \n     \n     \n    U \n    U \n    U \n    U \n  \n\n  \n    hxlr \n     \n     \n    U \n    U \n    U \n    U \n  \n\n estimatr \n    lm_lin \n     \n     \n     \n     \n     \n     \n  \n\n  \n    lm_robust \n    ✓ \n    ✓ \n    ✓ \n    U \n    ✓ \n    ✓ \n  \n\n  \n    iv_robust \n    ✓ \n    ✓ \n    U \n    U \n    U \n    U \n  \n\n fixest \n    feols \n    ✓ \n    ✓ \n    U \n    U \n    U \n    U \n  \n\n  \n    feglm \n     \n     \n    U \n    U \n    U \n    U \n  \n\n  \n    fenegbin \n     \n     \n    U \n    U \n    U \n    U \n  \n\n  \n    fepois \n    ✓ \n    ✓ \n    U \n    U \n    U \n    U \n  \n\n gam \n    gam \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n gamlss \n    gamlss \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n geepack \n    geeglm \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n glmmTMB \n    glmmTMB \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n glmx \n    glmx \n     \n     \n    ✓ \n    U \n    U \n    U \n  \n\n ivreg \n    ivreg \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    U \n    U \n  \n\n lme4 \n    lmer \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n  \n    glmer \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n  \n    glmer.nb \n     \n     \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n lmerTest \n    lmer \n     \n     \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n MASS \n    glmmPQL \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n  \n    glm.nb \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n  \n    polr \n    ✓ \n    ✓ \n    ✖ \n    ✖ \n    ✓ \n    ✓ \n  \n\n  \n    rlm \n     \n     \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n mclogit \n    mblogit \n     \n     \n    U \n    U \n    U \n    U \n  \n\n  \n    mclogit \n     \n     \n    U \n    U \n    U \n    U \n  \n\n MCMCglmm \n    MCMCglmm \n    U \n    U \n    U \n    U \n    U \n    U \n  \n\n mgcv \n    gam \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n  \n    bam \n     \n     \n    U \n    U \n    ✓ \n    ✖ \n  \n\n mhurdle \n    mhurdle \n     \n     \n    ✓ \n    ✓ \n    U \n    U \n  \n\n mlogit \n    mlogit \n     \n     \n    U \n    U \n    U \n    U \n  \n\n nlme \n    gls \n     \n     \n    U \n    U \n    ✓ \n    ✓ \n  \n\n  \n    lme \n     \n     \n     \n     \n     \n     \n  \n\n nnet \n    multinom \n    ✓ \n    ✓ \n    U \n    U \n    U \n    U \n  \n\n ordbetareg \n    ordbetareg \n     \n     \n    U \n    U \n     \n     \n  \n\n ordinal \n    clm \n    ✓ \n    ✓ \n    U \n    U \n    U \n    U \n  \n\n plm \n    plm \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    U \n    U \n  \n\n phylolm \n    phylolm \n     \n     \n     \n     \n     \n     \n  \n\n  \n    phyloglm \n     \n     \n     \n     \n     \n     \n  \n\n pscl \n    hurdle \n     \n     \n    ✓ \n    U \n    ✓ \n    ✖ \n  \n\n  \n    hurdle \n     \n     \n    ✓ \n    U \n    ✓ \n    ✖ \n  \n\n  \n    zeroinfl \n    ✓ \n    ✓ \n    ✓ \n    U \n    ✓ \n    ✓ \n  \n\n quantreg \n    rq \n    ✓ \n    ✓ \n    U \n    U \n    ✓ \n    ✓ \n  \n\n Rchoice \n    hetprob \n     \n     \n     \n     \n     \n     \n  \n\n  \n    ivpml \n     \n     \n     \n     \n     \n     \n  \n\n rms \n    ols \n     \n     \n     \n     \n     \n     \n  \n\n  \n    lrm \n     \n     \n     \n     \n     \n     \n  \n\n  \n    orm \n     \n     \n     \n     \n     \n     \n  \n\n robust \n    lmRob \n     \n     \n    U \n    U \n    U \n    U \n  \n\n robustbase \n    glmrob \n     \n     \n    ✓ \n    ✓ \n    U \n    U \n  \n\n  \n    lmrob \n     \n     \n    ✓ \n    ✓ \n    U \n    U \n  \n\n robustlmm \n    rlmer \n     \n     \n    U \n    U \n     \n     \n  \n\n rstanarm \n    stan_glm \n     \n     \n    ✖ \n    U \n    ✓ \n    ✓ \n  \n\n sampleSelection \n    selection \n     \n     \n    U \n    U \n    U \n    U \n  \n\n  \n    heckit \n     \n     \n    U \n    U \n    U \n    U \n  \n\n scam \n    scam \n     \n     \n    U \n    U \n    U \n    U \n  \n\n speedglm \n    speedglm \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    U \n    U \n  \n\n  \n    speedlm \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    U \n    U \n  \n\n survey \n    svyglm \n     \n     \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n  \n\n survival \n    clogit \n     \n     \n     \n     \n     \n     \n  \n\n  \n    coxph \n    ✓ \n    ✓ \n    U \n    U \n    ✓ \n    ✓ \n  \n\n tobit1 \n    tobit1 \n     \n     \n    ✓ \n    ✓ \n    U \n    U \n  \n\n truncreg \n    truncreg \n    ✓ \n    ✓ \n    ✓ \n    ✓ \n    U \n    U"
  },
  {
    "objectID": "articles/tables.html#marginal-effects",
    "href": "articles/tables.html#marginal-effects",
    "title": "\n25  Tables\n",
    "section": "\n25.1 Marginal effects",
    "text": "25.1 Marginal effects\nWe can summarize the results of the comparisons() or slopes() functions using the modelsummary package.\n\nlibrary(modelsummary)\nlibrary(marginaleffects)\n\nmod <- glm(am ~ wt + drat, family = binomial, data = mtcars)\nmfx <- slopes(mod)\n\nmodelsummary(mfx)\n\n\n\n\n   \n     (1) \n  \n\n\n wt \n    −0.217 \n  \n\n  \n    (0.080) \n  \n\n drat \n    0.278 \n  \n\n  \n    (0.168) \n  \n\n Num.Obs. \n    32 \n  \n\n AIC \n    22.0 \n  \n\n BIC \n    26.4 \n  \n\n Log.Lik. \n    −8.011 \n  \n\n F \n    3.430 \n  \n\n RMSE \n    0.28 \n  \n\n\n\n\nThe same results can be visualized with modelplot():\n\nmodelplot(mfx)"
  },
  {
    "objectID": "articles/tables.html#contrasts",
    "href": "articles/tables.html#contrasts",
    "title": "\n25  Tables\n",
    "section": "\n25.2 Contrasts",
    "text": "25.2 Contrasts\nWhen using the comparisons() function (or the slopes() function with categorical variables), the output will include two columns to uniquely identify the quantities of interest: term and contrast.\n\ndat <- mtcars\ndat$gear <- as.factor(dat$gear)\nmod <- glm(vs ~ gear + mpg, data = dat, family = binomial)\n\ncmp <- comparisons(mod)\nget_estimates(cmp)\n#> # A tibble: 3 × 8\n#>   term  contrast          estimate std.error statistic    p.value conf.low conf.high\n#>   <chr> <chr>                <dbl>     <dbl>     <dbl>      <dbl>    <dbl>     <dbl>\n#> 1 gear  mean(4) - mean(3)   0.0372    0.137      0.272 0.785       -0.230     0.305 \n#> 2 gear  mean(5) - mean(3)  -0.340     0.0988    -3.44  0.000588    -0.533    -0.146 \n#> 3 mpg   mean(+1)            0.0608    0.0128     4.74  0.00000218   0.0356    0.0860\n\nWe can use the shape argument of the modelsummary function to structure the table properly:\n\nmodelsummary(cmp, shape = term + contrast ~ model)\n\n\n\n\n   \n       \n     (1) \n  \n\n\n gear \n    mean(4) - mean(3) \n    0.037 \n  \n\n  \n    mean(4) - mean(3) \n    (0.137) \n  \n\n  \n    mean(5) - mean(3) \n    −0.340 \n  \n\n  \n    mean(5) - mean(3) \n    (0.099) \n  \n\n mpg \n    mean(+1) \n    0.061 \n  \n\n  \n    mean(+1) \n    (0.013) \n  \n\n Num.Obs. \n     \n    32 \n  \n\n AIC \n     \n    26.2 \n  \n\n BIC \n     \n    32.1 \n  \n\n Log.Lik. \n     \n    −9.101 \n  \n\n F \n     \n    2.389 \n  \n\n RMSE \n     \n    0.31 \n  \n\n\n\n\nCross-contrasts can be a bit trickier, since there are multiple simultaneous groups. Consider this example:\n\nmod <- lm(mpg ~ factor(cyl) + factor(gear), data = mtcars)\ncmp <- comparisons(\n  mod,\n  variables = c(\"gear\", \"cyl\"),\n  cross = TRUE)\nget_estimates(cmp)\n#> # A tibble: 4 × 9\n#>   term  contrast_gear     contrast_cyl      estimate std.error statistic p.value conf.low conf.high\n#>   <chr> <chr>             <chr>                <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n#> 1 cross mean(4) - mean(3) mean(6) - mean(4)    -5.33      2.77     -1.93 0.0542     -10.8  0.0953  \n#> 2 cross mean(4) - mean(3) mean(8) - mean(4)    -9.22      3.62     -2.55 0.0108     -16.3 -2.13    \n#> 3 cross mean(5) - mean(3) mean(6) - mean(4)    -5.16      2.63     -1.96 0.0500     -10.3  0.000166\n#> 4 cross mean(5) - mean(3) mean(8) - mean(4)    -9.04      3.19     -2.84 0.00453    -15.3 -2.80\n\nAs we can see above, there are two relevant grouping columns: contrast_gear and contrast_cyl. We can simply plug those names in the shape argument:\n\nmodelsummary(\n  cmp,\n  shape = contrast_gear + contrast_cyl ~ model)\n\n\n\n\n gear \n    cyl \n     (1) \n  \n\n\n mean(4) - mean(3) \n    mean(6) - mean(4) \n    −5.332 \n  \n\n mean(4) - mean(3) \n    mean(6) - mean(4) \n    (2.769) \n  \n\n mean(4) - mean(3) \n    mean(8) - mean(4) \n    −9.218 \n  \n\n mean(4) - mean(3) \n    mean(8) - mean(4) \n    (3.618) \n  \n\n mean(5) - mean(3) \n    mean(6) - mean(4) \n    −5.156 \n  \n\n mean(5) - mean(3) \n    mean(6) - mean(4) \n    (2.631) \n  \n\n mean(5) - mean(3) \n    mean(8) - mean(4) \n    −9.042 \n  \n\n mean(5) - mean(3) \n    mean(8) - mean(4) \n    (3.185) \n  \n\n Num.Obs. \n     \n    32 \n  \n\n R2 \n     \n    0.740 \n  \n\n R2 Adj. \n     \n    0.701 \n  \n\n AIC \n     \n    173.7 \n  \n\n BIC \n     \n    182.5 \n  \n\n Log.Lik. \n     \n    −80.838 \n  \n\n F \n     \n    19.190 \n  \n\n RMSE \n     \n    3.03"
  },
  {
    "objectID": "articles/tables.html#marginal-means",
    "href": "articles/tables.html#marginal-means",
    "title": "\n25  Tables\n",
    "section": "\n25.3 Marginal means",
    "text": "25.3 Marginal means\n\nlibrary(\"marginaleffects\")\nlibrary(\"modelsummary\")\n\ndat <- mtcars\ndat$cyl <- as.factor(dat$cyl)\ndat$am <- as.logical(dat$am)\nmod <- lm(mpg ~ hp + cyl + am, data = dat)\nmm <- marginal_means(mod)\n\nmodelsummary(mm,\n             title = \"Estimated Marginal Means\",\n             estimate = \"{estimate} ({std.error}){stars}\",\n             statistic = NULL,\n             group = term + value ~ model)\n\n\n\nEstimated Marginal Means\n \n   \n       \n     (1) \n  \n\n\n cyl \n    6 \n    18.960 (1.073)*** \n  \n\n  \n    4 \n    22.885 (1.357)*** \n  \n\n  \n    8 \n    19.351 (1.377)*** \n  \n\n am \n    TRUE \n    22.478 (0.834)*** \n  \n\n  \n    FALSE \n    18.320 (0.785)*** \n  \n\n Num.Obs. \n     \n    32 \n  \n\n R2 \n     \n    0.825 \n  \n\n R2 Adj. \n     \n    0.799 \n  \n\n AIC \n     \n    161.0 \n  \n\n BIC \n     \n    169.8 \n  \n\n Log.Lik. \n     \n    −74.502 \n  \n\n F \n     \n    31.794 \n  \n\n RMSE \n     \n    2.48"
  },
  {
    "objectID": "articles/faq.html#stack-overflow-questions",
    "href": "articles/faq.html#stack-overflow-questions",
    "title": "\n26  FAQ\n",
    "section": "\n26.1 Stack Overflow questions",
    "text": "26.1 Stack Overflow questions\n\nplot_predictions() over a range of unobserved values\nPlot the marginal effects from a plm package model\nModels with demeaned, polynomials, or transformed variables"
  },
  {
    "objectID": "articles/faq.html#calling-marginaleffects-in-functions-loops-environments-or-after-re-assigning-variables",
    "href": "articles/faq.html#calling-marginaleffects-in-functions-loops-environments-or-after-re-assigning-variables",
    "title": "\n26  FAQ\n",
    "section": "\n26.2 Calling marginaleffects in functions, loops, environments, or after re-assigning variables",
    "text": "26.2 Calling marginaleffects in functions, loops, environments, or after re-assigning variables\nFunctions from the marginaleffects package can sometimes fail when they are called inside a function, loop, or other environments. To see why, it is important to know that marginaleffects often needs to operate on the original data that was used to fit the model. To extract this original data, we use the get_data() function from the insight package.\nIn most cases, get_data() can extract the data which is stored inside the model object created by the modeling package. However, some modeling packages do not save the original data in the model object (in order to save memory). In those cases, get_data() will parse the call to find the name of the data object, and will search for that data object in the global environment. When users fit models in a different environment (e.g., function calls), get_data() may not be able to retrieve the original data.\nA related problem can arise if users fit a model, but then assign a new value to the variable that used to store the dataset.\nRecommendations:\n\nSupply your dataset explicitly to the newdata argument of slopes() functions.\nAvoid assigning a new value to a variable that you use to store a dataset for model fitting."
  },
  {
    "objectID": "articles/NEWS.html#development-version",
    "href": "articles/NEWS.html#development-version",
    "title": "News",
    "section": "(development version)",
    "text": "(development version)\nNew:\n\ns.value column in all output: Shannon transforms for p values. See Greenland (2019).\nmarginal_means supports mira (mice objects).\ncomparisons(): the variables arguments now accepts “revpairwise”, “revsequential”, “revreference” for factor and character variables.\n\nPerformance:\n\nComputing elasticities for linear models is now up to 30% faster (#787, @etiennebacher)."
  },
  {
    "objectID": "articles/NEWS.html#section",
    "href": "articles/NEWS.html#section",
    "title": "News",
    "section": "0.12.0",
    "text": "0.12.0\nBreaking change:\n\nRow order of output has changed for many calls, especially those using the by argument. This may break hypothesis tests conducted by indexing b1, b2, etc. This was necessary to fix Issue #776. Thanks to @marcora for the report.\n\nNew:\n\nhypotheses(): Joint hypothesis tests (F and Chi-square) with the joint and joint_test arguments.\nvcov.hypotheses method.\nwts is now available in plot_predictions(), plot_comparisons(), and plot_slopes().\n\nBug:\n\nWrong order of rows in bayesian models with by argument. Thanks to @shirdekel for report #782."
  },
  {
    "objectID": "articles/NEWS.html#section-1",
    "href": "articles/NEWS.html#section-1",
    "title": "News",
    "section": "0.11.2",
    "text": "0.11.2\n\nvcov() and coef() methods for marginaleffects objects.\nStrings in wts are accepted with the by argument.\npredictions() and avg_predictions() no longer use an automatic backtransformation for GLM models unless hypothesis is NULL.\nvcov() can be used to retrieve a full variance-covariance matrix from objects produced by comparisons(), slopes(), predictions(), or marginal_means() objects.\nWhen processing objects obtained using mice multiple imputation, the pooled model using mice::pool is attached to the model attribute of the output. This means that functions like modelsummary::modelsummary() will not erroneously report goodness-of-fit statistics from just a single model and will instead appropriately report the statistics for the pooled model. Thanks to @Tristan-Siegfried for PR #740.\nMore informative error messages on some prediction problems. Thanks to @andymilne for Report #751.\n\nPerformance:\n\ninferences() is now up to 17x faster and much more memory-efficient when method is \"boot\" or \"rsample\" (#770, #771, @etiennebacher).\n\nBugs:\n\nbrms models with nl=TRUE and a single predictor generated an error. Thanks to @Tristan-Siegried for Report #759.\navg_predictions(): Incorrect group-wise averaging when all predictors are categorical, the variables variable is used, and we are averaging with avg_ or the by argument. Thanks to BorgeJorge for report #766.\nBug when datagrid() when called inside a user-written function. Thanks to @NickCH-K for report #769 and to @capnrefsmmat for the diagnostics."
  },
  {
    "objectID": "articles/NEWS.html#section-2",
    "href": "articles/NEWS.html#section-2",
    "title": "News",
    "section": "0.11.1",
    "text": "0.11.1\nBreaking change:\n\nRow orders are now more consistent, but may have changed from previous version. This could affect results from hypothesis with b1, b2, … indexing.\n\nSupport new models:\n\nnlme::lme()\nphylolm::phylolm()\nphylolm::phyloglm()\n\nNew:\n\nVignette on 2x2 experimental designs. Thanks to Demetri Pananos.\ncomparisons() accepts data frames with two numeric columns (“low” and “high”) to specify fully customizable contrasts.\ndatagrid() gets a new by argument to create apply grid-making functions within groups.\nplot_*() gain a newdata argument for use with by.\n\nBug:\n\ncomparisons(comparison = \"lnratioavg\") ignored wts argument. Thanks to Demetri Pananos for report #737.\nordinal::clm(): incorrect standard errors when location and scale parameters are the same. Thanks to MrJerryTAO for report #718.\nIncorrect label for “2sd” comparisons. Thanks to Andy Milne for report #720.\nInvalid factor levels in datagrid() means newdata argument gets ignored. Thanks to Josh Errickson for report #721.\nError in models with only categorical predictors and the by argument. Thanks to Sam Brilleman for report #723.\nElasticities are now supported for ordinal::clm() models. Thanks to MrJerryTAO for report #729.\nglmmTMB models with zero-inflated components are supported. Thanks to @Helsinki-Ronan and @strengejacke for report #734."
  },
  {
    "objectID": "articles/NEWS.html#section-3",
    "href": "articles/NEWS.html#section-3",
    "title": "News",
    "section": "0.11.0",
    "text": "0.11.0\nBreaking changes:\n\ntype column is replaced by type attribute.\npredictions() only works with officially supported model types (same list as comparisons() and slopes()).\n\nRenamed arguments (backward compatibility is preserved):\n\ntransform_pre -> comparison\ntransform_post -> transform\n\nNew:\n\np_adjust argument: Adjust p-values for multiple comparisons.\nequivalence argument available everywhere.\n\nPerformance:\n\nMuch faster results in avg_*() functions for models with only categorical predictors and many rows of data, using deduplication and weights instead of unit-level estimates.\nFaster predictions in lm() and glm() models using RcppEigen.\nBayesian models with many rows. Thanks to Etienne Bacher. #694\nFaster predictions, especially with standard errors and large datasets.\n\nBugs:\n\nMultiple imputation with mira objects was not pooling all datasets. Thanks to @Generalized for report #711.\nSupport for more models with offsets. Thanks to @mariofiorini for report #705.\nError on predictions() with by and wts. Thanks to Noah Greifer for report #695.\nafex: some models generated errors. Thanks to Daniel Lüdecke for report #696.\ngroup column name is always forbidden. Thanks to Daniel Lüdecke for report #697.\nBlank graphs in plot_comparisons() with a list in variables.\ntype=\"link\" produced an error with some categorical brms models. Thanks to @shirdekel for report #703.\nError on predictions(variables = ...) for glmmTMB models. Thanks to Daniel Lüdecke for report #707.\nby with user-specified function in comparison and factor predictor did not aggregate correctly. Thanks to @joaotedde for report #715.\nordinal::clm: Support cum.prob and linear.predictor prediction types. Thanks to @MrJerryTAO for report #717."
  },
  {
    "objectID": "articles/NEWS.html#section-4",
    "href": "articles/NEWS.html#section-4",
    "title": "News",
    "section": "0.10.0",
    "text": "0.10.0\nPerformance:\n\n2-4x faster execution for many calls. Thanks to Etienne Bacher.\n\nNew models supported:\n\nMCMCglmm::MCMCglmm\nRchoice::hetprob\nRchoice::ivpml\nMultiple imputation using mice and any package which can return a list of imputed data frames (e.g., Amelia, missRanger, etc.)\n\nPlot improvements:\n\nNew by argument to display marginal estimates by subgroup.\nNew rug argument to display tick marks in the margins.\nNew points argument in plot_predictions() to display a scatter plot.\nNew gray argument to plot in grayscale using line types and shapes instead of color.\nThe effect argument is renamed to variables in plot_slopes() and plot_comparisons(). This improves consistency with the analogous slopes() and comparisons() functions.\nThe plotting vignette was re-written.\n\nOther:\n\nSupport multiple imputation with mice mira objects. The multiple imputation vignette was rewritten.\nThe variables_grid argument in marginal_means() is renamed newdata. Backward compatibility is maintained.\navg_*() returns an informative error when vcov is “satterthwaite” or “kenward-roger”\n“satterthwaite” and “kenward-roger” are now supported when newdata is not NULL\nInformative error when hypothesis includes a b# larger than the available number of estimates.\navg_predictions(model, variables = \"x\") computes average counterfactual predictions by subgroups of x\ndatagrid() and plot_*() functions are faster in datasets with many extraneous columns.\nIn predictions(type = NULL) with glm() and Gam() we first make predictions on the link scale and then backtransform them. Setting type=\"response\" explicitly makes predictions directly on the response scale without backtransformation.\nStandard errors now supported for more glmmTMB models.\nUse the numDeriv package for numeric differentiation in the calculation of delta method standard error. A global option can now be passed to numDeriv::jacobian:\n\noptions(marginaleffects_numDeriv = list(method = \"simple\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = list(method = \"Richardson\", method.args = list(eps = 1e-6)))\noptions(marginaleffects_numDeriv = NULL)\n\nPrint:\n\nPrint fewer significant digits.\nprint.marginaleffects now prints all columns supplied to newdata\nLess redundant labels when using hypothesis\n\nMany improvements to documentation.\n\nBugfixes:\n\nStandard errors could be inaccurate in models with non-linear components (and interactions) when some of the coefficients were very small. This was related to the step size used for numerical differentiation for the delta method. Issue #684.\navg_predictions(by =) did not work when the dataset included a column named term. Issue #683.\nbrms models with multivariate outcome collapsed categories in comparisons(). Issue #639.\nhypotheses() now works on lists and in calls to lapply(), purrr::map(), etc. Issue #660."
  },
  {
    "objectID": "articles/NEWS.html#section-5",
    "href": "articles/NEWS.html#section-5",
    "title": "News",
    "section": "0.9.0",
    "text": "0.9.0\nBreaking changes:\n\nAll functions return an estimate column instead of the function-specific predicted, comparisons, dydx, etc. This change only affects unit-level estimates, and not average estimates, which already used the estimate column name.\nThe transform_avg argument in tidy() deprecated. Use transform_post instead.\nplot_*(draw=FALSE) now return the actual variable names supplied to the condition argument, rather than the opaque “condition1”, “condition2”, etc.\n\nNew models supported:\n\nblme package.\n\nNew features:\n\nNew functions: avg_predictions(), avg_comparisons(), avg_slopes()\nEquivalence, non-inferiority, and non-superiority tests with the hypotheses() function and equivalence argument.\nNew experimental inferences() function: simulation-based inferences and bootstrap using the boot, rsample, and fwb package.\nNew df argument to set degrees of freedom manually for p and CI.\nPretty print() for all objects.\nby argument\n\nTRUE returns average (marginal) predictions, comparisons, or slopes.\nSupports bayesian models.\n\nhypothesis argument\n\nNumeric value sets the null used in calculating Z and p.\nExample: comparisons(mod, transform_pre = \"ratio\", hypothesis = 1)\n\nAll arguments from the main functions are now available through tidy(), and summary(): conf_level, transform_post, etc.\nBayesian posterior distribution summaries (median, mean, HDI, quantiles) can be customized using global options. See ?comparisons\n\nRenamed functions (backward-compatibility is maintained by keeping the old function names as aliases):\n\nmarginaleffects() -> slopes()\nposteriordraws() -> posterior_draws()\nmarginalmeans() -> marginal_means()\nplot_cap() -> plot_predictions()\nplot_cme() -> plot_slopes()\nplot_cco() -> plot_comparisons()\n\nBug fixes:\n\nIncorrect results: In 0.8.1, plot_*() the threenum and minmax labels did not correspond to the correct numeric values.\nFix corner case for slopes when the dataset includes infinite values.\nmlogit error with factors.\nThe vcov argument now accepts functions for most models.\n\nOther:\n\nRemoved major performance bottleneck for slopes()"
  },
  {
    "objectID": "articles/NEWS.html#section-6",
    "href": "articles/NEWS.html#section-6",
    "title": "News",
    "section": "0.8.1",
    "text": "0.8.1\n\ndeltamethod() can run hypothesis tests on objects produced by the comparisons(), marginaleffects(), predictions(), and marginalmeans() functions. This feature relies on match.call(), which means it may not always work when used programmatically, inside functions and nested environments. It is generally safer and more efficient to use the hypothesis argument.\nplot_cme() and plot_cco() accept lists with user-specified values for the regressors, and can display nice labels for shortcut string-functions like “threenum” or “quartile”.\nposterior_draws: new shape argument to return MCMC draws in various formats, including the new rvar structure from the posterior package.\ntransform_avg function gets printed in summary() output.\ntransform_post and transform_avg support string shortcuts: “exp” and “ln”\nAdded support for mlm models from lm(). Thanks to Noah Greifer.\n\nBug fixes:\n\nhypothesis argument with bayesian models and tidy() used to raise an error.\nMissing values for some regressors in the comparisons() output for brms models."
  },
  {
    "objectID": "articles/NEWS.html#section-7",
    "href": "articles/NEWS.html#section-7",
    "title": "News",
    "section": "0.8.0",
    "text": "0.8.0\nBreaking change:\n\nThe interaction argument is deprecated and replaced by the cross argument. This is to reduce ambiguity with respect to the interaction argument in emmeans, which does something completely different, akin to the difference-in-differences illustrated in the Interactions vignette.\n\n71 classes of models supported, including the new:\n\nrms::ols\nrms::lrm\nrms::orm\n\nNew features:\n\nPlots: plot_cme(), plot_cap(), and plot_cco() are now much more flexible in specifying the comparisons to display. The condition argument accepts lists, functions, and shortcuts for common reference values, such as “minmax”, “threenum”, etc.\nvariables argument of the comparisons() function is more flexible:\n\nAccepts functions to specify custom differences in numeric variables (e.g., forward and backward differencing).\nCan specify pairs of factors to compare in the variables argument of the comparisons function.\n\nvariables argument of the predictions() function is more flexible:\n\nAccepts shortcut strings, functions, and vectors of arbitrary length.\n\nIntegrate out random effects in bayesian brms models (see Bayesian analysis vignette)\n\nNew vignettes:\n\nExperiments\nExtending marginal effects\nIntegrating out random effects in bayesian models\n\nBug fixes and minor improvements:\n\nThe default value of conf_level in summary() and tidy() is now NULL, which inherits the conf_level value in the original comparisons/marginaleffects/predictions calls.\nFix typo in function names for missing “lnratioavgwts”\nInteractions with fixest::i() are parsed properly as categorical variables\nFor betareg objects, inference can now be done on all coefficients using deltamethod(). previously only the location coefficients were available.\nFor objects from crch package, a number of bugs have been fixed; standard errors should now be correct for deltamethod(), marginaleffects(), etc.\nFixed a bug in the tidy() function for glmmTMB models without random effects, which caused all t statistics to be identical."
  },
  {
    "objectID": "articles/NEWS.html#section-8",
    "href": "articles/NEWS.html#section-8",
    "title": "News",
    "section": "0.7.1",
    "text": "0.7.1\n\nNew supported model class: gamlss. Thanks to Marcio Augusto Diniz.\nmarginalmeans() accepts a wts argument with values: “equal”, “proportional”, “cells”.\nby argument\n\naccepts data frames for complex groupings.\nin marginalmeans only accepts data frames.\naccepts “group” to group by response level.\nworks with bayesian models.\n\nbyfun argument for the predictions() function to aggregate using different functions.\nhypothesis argument\n\nThe matrix column names are used as labels for hypothesis tests.\nBetter labels with “sequential”, “reference”, “pairwise”.\nnew shortcuts “revpairwise”, “revsequential”, “revreference”\n\nwts argument is respected in by argument and with *avg shortcuts in the transform_pre argument.\ntidy.predictions() and tidy.marginalmeans() get a new transform_avg argument.\nNew vignettes:\n\nUnit-level contrasts in logistic regressions. Thanks to @arthur-albuquerque.\nPython Numpy models in marginaleffects. Thanks to @timpipeseek.\nBootstrap example in standard errors vignette."
  },
  {
    "objectID": "articles/NEWS.html#section-9",
    "href": "articles/NEWS.html#section-9",
    "title": "News",
    "section": "0.7.0",
    "text": "0.7.0\nBreaking changes:\n\nby is deprecated in summary() and tidy(). Use the same by argument in the main functions instead: comparisons(), marginaleffects(), predictions()\nCharacter vectors are no longer supported in the variables argument of the predictions() function. Use newdata=\"fivenum\" or “grid”, “mean”, or “median” instead.\n\nCritical bug fix:\n\nContrasts with interactions were incorrect in version 0.6.0. The error should have been obvious to most analysts in most cases (weird-looking alignment). Thanks to @vmikk.\n\nNew supported packages and models:\n\nsurvival::clogit\nbiglm: The main quantities can be computed, but not the delta method standard errors. See https://github.com/vincentarelbundock/marginaleffects/issues/387\n\nNew vignette:\n\nElasticity\nFrequently Asked Questions\n\nNew features:\n\nElasticity and semi-elasticity using the new slope argument in marginaleffects(): eyex, dyex, eydx\ndatagrid() accepts functions: datagrid(newdata = mtcars, hp = range, mpg = fivenum, wt = sd)\nNew datagridcf() function to create counterfactual datasets. This is a shortcut to the datagrid() function with default to grid_type = \"counterfactual\"\nNew by arguments in predictions(), comparisons(), marginaleffects()\nNew newdata shortcuts: “tukey”, “grid”\nNew string shortcuts for transform_pre in comparisons()\nmarginalmeans() now back transforms confidence intervals when possible.\nvcov argument string shortcuts are now case-insensitive\nThe default contrast in comparisons() for binary predictors is now a difference between 1 and 0, rather than +1 relative to baseline.\ndocumentation improvements"
  },
  {
    "objectID": "articles/NEWS.html#section-10",
    "href": "articles/NEWS.html#section-10",
    "title": "News",
    "section": "0.6.0",
    "text": "0.6.0\nNew supported packages and models:\n\ntidymodels objects of class tidy_model are supported if the fit engine is supported by marginaleffects.\n\nNew function:\n\ndeltamethod(): Hypothesis tests on functions of parameters\nplot_cco(): Plot conditional contrasts\n\nNew arguments:\n\nhypothesis for hypothesis tests and custom contrasts\ntransform_post in predictions()\nwts argument in predictions() only affects average predictions in tidy() or summary().\n\nNew or improved vignettes:\n\nHypothesis Tests and Custom Contrasts using the Delta Method: https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html\nMultiple Imputation: https://vincentarelbundock.github.io/marginaleffects/articles/multiple_imputation.html\nCausal Inference with the g-Formula: https://vincentarelbundock.github.io/marginaleffects/articles/gformula.html (Thanks to Rohan Kapre for the idea)\n\nDeprecated or renamed arguments:\n\ncontrast_factor and contrast_numeric arguments are deprecated in comparisons(). Use a named list in the variables argument instead. Backward compatibility is maintained.\nThe transform_post argument in tidy() and summary() is renamed to transform_avg to disambiguate against the argument of the same name in comparisons(). Backward compatibility is preserved.\n\nMisc:\n\ntidy.predictions() computes standard errors using the delta method for average predictions\nSupport gam models with matrix columns.\neps in marginaleffects() is now “adaptive” by default: it equals 0.0001 multiplied the range of the predictor variable\ncomparisons() now supports “log of marginal odds ratio” in the transform_pre argument. Thanks to Noah Greifer.\nNew transform_pre shortcuts: dydx, expdydx\ntidy.predictions() computes standard errors and confidence intervals for linear models or GLM on the link scale."
  },
  {
    "objectID": "articles/NEWS.html#section-11",
    "href": "articles/NEWS.html#section-11",
    "title": "News",
    "section": "0.5.0",
    "text": "0.5.0\nBreaking changes:\n\ntype no longer accepts a character vector. Must be a single string.\nconf.int argument deprecated. Use vcov = FALSE instead.\n\nNew supported packages and models:\n\nmlogit\nmhurdle\ntobit1\nglmmTMB\n\nNew features:\n\ninteraction argument in comparisons() to compute interactions between contrasts (cross-contrasts).\nby argument in tidy() and summary() computes group-average marginal effects and comparisons.\ntransform_pre argument can define custom contrasts between adjusted predictions (e.g., log adjusted risk ratios). Available in comparisons().\ntransform_post argument allows back transformation before returning the final results. Available in comparisons(), marginalmeans(), summary(), tidy().\nThe variables argument of the comparisons() function accepts a named list to specify variable-specific contrast types.\nRobust standard errors with the vcov argument. This requires version 0.17.1 of the insight package.\n\nsandwich package shortcuts: vcov = \"HC3\", \"HC2\", \"NeweyWest\", and more.\nMixed effects models: vcov = \"satterthwaite\" or \"kenward-roger\"\nOne-sided formula to clusters: vcov = ~cluster_variable\nVariance-covariance matrix\nFunction which returns a named squared matrix\n\nmarginalmeans() allows interactions\nBayesian Model Averaging for brms models using type = \"average\". See vignette on the marginaleffects website.\neps argument for step size of numerical derivative\nmarginaleffects and comparisons now report confidence intervals by default.\nNew dependency on the data.table package yields substantial performance improvements.\nMore informative error messages and warnings\nBug fixes and performance improvements\n\nNew pages on the marginaleffects website: https://vincentarelbundock.github.io/marginaleffects/\n\nAlternative software packages\nRobust standard errors (and more)\nPerformance tips\nTables and plots\nMultinomial Logit and Discrete Choice Models\nGeneralized Additive Models\nMixed effects models (Bayesian and Frequentist)\nTransformations and Custom Contrasts: Adjusted Risk Ratio Example\n\nArgument name changes (backward compatibility is preserved:\n\nEverywhere:\n\nconf.level -> conf_level\n\ndatagrid():\n\nFUN.factor -> FUN_factor (same for related arguments)\ngrid.type -> grid_type"
  },
  {
    "objectID": "articles/NEWS.html#section-12",
    "href": "articles/NEWS.html#section-12",
    "title": "News",
    "section": "0.4.1",
    "text": "0.4.1\nNew supported packages and models:\n\nstats::loess\nsampleSelection::selection\nsampleSelection::heckit\n\nMisc:\n\nmgcv::bam models allow exclude argument.\nGam models allow include_smooth argument.\nNew tests\nBug fixes"
  },
  {
    "objectID": "articles/NEWS.html#section-13",
    "href": "articles/NEWS.html#section-13",
    "title": "News",
    "section": "0.4.0",
    "text": "0.4.0\nNew function:\n\ncomparisons() computes contrasts\n\nMisc:\n\nSpeed optimizations\npredictions() and plot_cap() include confidence intervals for linear models\nMore robust handling of in-formula functions: factor(), strata(), mo()\nDo not overwrite user’s ggplot2::theme_set() call"
  },
  {
    "objectID": "articles/NEWS.html#section-14",
    "href": "articles/NEWS.html#section-14",
    "title": "News",
    "section": "0.3.4",
    "text": "0.3.4\n\nBug fixes"
  },
  {
    "objectID": "articles/NEWS.html#section-15",
    "href": "articles/NEWS.html#section-15",
    "title": "News",
    "section": "0.3.3",
    "text": "0.3.3\nNew supported models:\n\nmclogit::mclogit\nrobust::lmRob\nrobustlmm::rlmer\nfixest confidence intervals in predictions\n\nMisc:\n\nSupport modelbased::visualisation_matrix in newdata without having to specify x explicitly.\ntidy.predictions() and summary.predictions() methods.\nDocumentation improvements.\nCRAN test fixes"
  },
  {
    "objectID": "articles/NEWS.html#section-16",
    "href": "articles/NEWS.html#section-16",
    "title": "News",
    "section": "0.3.2",
    "text": "0.3.2\nSupport for new models and packages:\n\nbrglm2::bracl\nmclogit::mblogit\nscam::scam\nlmerTest::lmer\n\nMisc:\n\nDrop numDeriv dependency, but make it available via a global option: options(“marginaleffects_numDeriv” = list(method = “Richardson”, method.args = list(eps = 1e-5, d = 0.0001)))\nBugfixes\nDocumentation improvements\nCRAN tests"
  },
  {
    "objectID": "articles/NEWS.html#section-17",
    "href": "articles/NEWS.html#section-17",
    "title": "News",
    "section": "0.3.1",
    "text": "0.3.1\ndocumentation bugfix"
  },
  {
    "objectID": "articles/NEWS.html#section-18",
    "href": "articles/NEWS.html#section-18",
    "title": "News",
    "section": "0.3.0",
    "text": "0.3.0\nBreaking changes:\n\npredictions returns predictions for every observation in the original dataset instead of newdata=datagrid().\nmarginalmeans objects have new column names, as do the corresponding tidy and summary outputs.\n\nNew supported packages and models:\n\nbrms::brm\nrstanarm::stanglm\nbrglm2::brmultinom\nMASS::glmmPQL\naod::betabin\n\nMisc:\n\ndatagrid function supersedes typical and counterfactual with the grid.type argument. The typical and counterfactual functions will remain available and exported, but their use is not encouraged.\nposterior_draws function can be applied to a predictions or a marginaleffects object to extract draws from the posterior distribution.\nmarginalmeans standard errors are now computed using the delta method.\npredictions standard errors are now computed using the delta method when they are not available from insight::get_predicted.\nNew vignette on Bayesian models with brms\nNew vignette on Mixed effects models with lme4\nIf the data.table package is installed, marginaleffects will automatically use it to speed things up.\nContrast definition reported in a separate column of marginaleffects output.\nSafer handling of the type argument.\nComprehensive list of supported and tests models on the website.\nMany bug fixes\nMany new tests, including several against emmeans"
  },
  {
    "objectID": "articles/NEWS.html#section-19",
    "href": "articles/NEWS.html#section-19",
    "title": "News",
    "section": "0.2.0",
    "text": "0.2.0\nBreaking change:\n\ndata argument becomes newdata in all functions.\n\nNew supported packages and models:\n\nlme4:glmer.nb\nmgcv::gam\nordinal::clm\nmgcv\n\nmarginalmeans:\n\nNew variables_grid argument\n\npredictions:\n\nSupport mgcv\n\nplot_cap\n\nNew type argument\n\nMisc:\n\nNew validity checks and tests"
  },
  {
    "objectID": "articles/NEWS.html#section-20",
    "href": "articles/NEWS.html#section-20",
    "title": "News",
    "section": "0.1.0",
    "text": "0.1.0\nFirst release. Bravo!\nThanks to Marco Avina Mendoza, Resul Umit, and all those who offered comments and suggestions."
  },
  {
    "objectID": "functions.html",
    "href": "functions.html",
    "title": "Functions",
    "section": "",
    "text": "Goal\nFunction\n\n\n\nPredictions\npredictions()\n\n\n\navg_predictions()\n\n\n\nplot_predictions()\n\n\nComparisons: Difference, Ratio, Odds, etc.\ncomparisons()\n\n\n\navg_comparisons()\n\n\n\nplot_comparisons()\n\n\nSlopes\nslopes()\n\n\n\navg_slopes()\n\n\n\nplot_slopes()\n\n\nMarginal Means\nmarginal_means()\n\n\nGrids\ndatagrid()\n\n\n\ndatagridcf()\n\n\nHypothesis & Equivalence\nhypotheses()\n\n\nBayes, Bootstrap, Simulation\nposterior_draws()\n\n\n\ninferences()"
  },
  {
    "objectID": "reference/predictions.html#description",
    "href": "reference/predictions.html#description",
    "title": "predictions",
    "section": "Description",
    "text": "Description\n\nOutcome predicted by a fitted model on a specified scale for a given combination of values of the predictor variables, such as their observed values, their means, or factor levels (a.k.a. \"reference grid\").\n\n\n\n\npredictions(): unit-level (conditional) estimates.\n\n\n\n\navg_predictions(): average (marginal) estimates.\n\n\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\n\n\nSee the predictions vignette and package website for worked examples and case studies:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/predictions.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/"
  },
  {
    "objectID": "reference/predictions.html#usage",
    "href": "reference/predictions.html#usage",
    "title": "predictions",
    "section": "Usage",
    "text": "Usage\npredictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = FALSE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  ...\n)\n\navg_predictions(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  by = TRUE,\n  byfun = NULL,\n  wts = NULL,\n  transform = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  ...\n)"
  },
  {
    "objectID": "reference/predictions.html#arguments",
    "href": "reference/predictions.html#arguments",
    "title": "predictions",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate predictions.\n\n\n\n\nNULL (default): Predictions for each observed value in the original dataset. See insight::get_data()\n\n\n\n\ndata frame: Predictions for each row of the newdata data frame.\n\n\n\n\nstring:\n\n\n\n\n\"mean\": Predictions at the Mean. Predictions when each predictor is held at its mean or mode.\n\n\n\n\n\"median\": Predictions at the Median. Predictions when each predictor is held at its median or mode.\n\n\n\n\n\"marginalmeans\": Predictions at Marginal Means. See Details section below.\n\n\n\n\n\"tukey\": Predictions at Tukey’s 5 numbers.\n\n\n\n\n\"grid\": Predictions on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\n\n\n\n\nvariables\n\n\nCounterfactual variables.\n\n\n\n\nOutput:\n\n\n\n\npredictions(): The entire dataset is replicated once for each unique combination of variables, and predictions are made.\n\n\n\n\navg_predictions(): The entire dataset is replicated, predictions are made, and they are marginalized by variables categories.\n\n\n\n\nWarning: This can be expensive in large datasets.\n\n\n\n\nWarning: Users who need \"conditional\" predictions should use the newdata argument instead of variables.\n\n\n\n\n\n\nInput:\n\n\n\n\nNULL: computes one prediction per row of newdata\n\n\n\n\nCharacter vector: the dataset is replicated once of every combination of unique values of the variables identified in variables.\n\n\n\n\nNamed list: names identify the subset of variables of interest and their values. For numeric variables, the variables argument supports functions and string shortcuts:\n\n\n\n\nA function which returns a numeric value\n\n\n\n\nNumeric vector: Contrast between the 2nd element and the 1st element of the x vector.\n\n\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\n\n\"threenum\": mean and 1 standard deviation on both sides\n\n\n\n\n\"fivenum\": Tukey’s five numbers\n\n\n\n\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the default value is used. This default is the first model-related row in the marginaleffects:::type_dictionary dataframe. See the details section for a note on backtransformation.\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\n\n\nbyfun\n\n\nA function such as mean() or sum() used to aggregate estimates within the subgroups defined by the by argument. NULL uses the mean() function. Must accept a numeric vector and return a single numeric value. This is sometimes used to take the sum or mean of predicted probabilities across outcome or predictor levels. See examples section.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ⁠avg_*()⁠ or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/predictions.html#details",
    "href": "reference/predictions.html#details",
    "title": "predictions",
    "section": "Details",
    "text": "Details\n\nFor glm(), MASS::glm.nb, gam::gam(), and feols::feglm models with type, transform and hypothesis all equal to NULL (the default), predictions() first predicts on the link scale, and then backtransforms the estimates and confidence intervals. This implies that the estimate produced by avg_predictions() will not be exactly equal to the average of the estimate column produced by predictions(). Users can circumvent this behavior and average predictions directly on the response scale by setting type=“response” explicitly. With type=“response”, the intervals are symmetric and may have undesirable properties (e.g., stretching beyond the ⁠[0,1]⁠ bounds for a binary outcome regression)."
  },
  {
    "objectID": "reference/predictions.html#value",
    "href": "reference/predictions.html#value",
    "title": "predictions",
    "section": "Value",
    "text": "Value\n\nA data.frame with one row per observation and several columns:\n\n\n\n\nrowid: row number of the newdata data frame\n\n\n\n\ntype: prediction type, as defined by the type argument\n\n\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\n\n\nestimate: predicted outcome\n\n\n\n\nstd.error: standard errors computed using the delta method.\n\n\n\n\nconf.low: lower bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\nconf.high: upper bound of the confidence interval (or equal-tailed interval for bayesian models)\n\n\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument. For models of class feglm, Gam, glm and negbin, p values are computed on the link scale by default unless the type argument is specified explicitly.\n\n\n\n\nSee ?print.marginaleffects for printing options."
  },
  {
    "objectID": "reference/predictions.html#functions",
    "href": "reference/predictions.html#functions",
    "title": "predictions",
    "section": "Functions",
    "text": "Functions\n\n\n\navg_predictions(): Average predictions"
  },
  {
    "objectID": "reference/predictions.html#standard-errors-using-the-delta-method",
    "href": "reference/predictions.html#standard-errors-using-the-delta-method",
    "title": "predictions",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/bootstrap.html"
  },
  {
    "objectID": "reference/predictions.html#model-specific-arguments",
    "href": "reference/predictions.html#model-specific-arguments",
    "title": "predictions",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws"
  },
  {
    "objectID": "reference/predictions.html#bayesian-posterior-summaries",
    "href": "reference/predictions.html#bayesian-posterior-summaries",
    "title": "predictions",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(“marginaleffects_posterior_interval” = “eti”)\n\n\noptions(“marginaleffects_posterior_interval” = “hdi”)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(“marginaleffects_posterior_center” = “mean”)\n\n\noptions(“marginaleffects_posterior_center” = “median”)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default)."
  },
  {
    "objectID": "reference/predictions.html#equivalence-inferiority-superiority",
    "href": "reference/predictions.html#equivalence-inferiority-superiority",
    "title": "predictions",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n is an estimate, _ its estimated standard error, and [a, b] are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\nH_0: a\n\n\n\n\nH_1: > a\n\n\n\n\nt=(- a)/_\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\nH_0: b\n\n\n\n\nH_1: < b\n\n\n\n\nt=(- b)/_\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature."
  },
  {
    "objectID": "reference/predictions.html#examples",
    "href": "reference/predictions.html#examples",
    "title": "predictions",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\n# Adjusted Prediction for every row of the original dataset\nmod <- lm(mpg ~ hp + factor(cyl), data = mtcars)\npred <- predictions(mod)\nhead(pred)\n\n\n Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n     20.0      1.204 16.6   <0.001 204.1  17.7   22.4\n     20.0      1.204 16.6   <0.001 204.1  17.7   22.4\n     26.4      0.962 27.5   <0.001 549.0  24.5   28.3\n     20.0      1.204 16.6   <0.001 204.1  17.7   22.4\n     15.9      0.992 16.0   <0.001 190.0  14.0   17.9\n     20.2      1.219 16.5   <0.001 201.8  17.8   22.5\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n\n# Adjusted Predictions at User-Specified Values of the Regressors\npredictions(mod, newdata = datagrid(hp = c(100, 120), cyl = 4))\n\n\n Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp cyl\n     26.2      0.986 26.6   <0.001 516.6  24.3   28.2 100   4\n     25.8      1.110 23.2   <0.001 393.8  23.6   27.9 120   4\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, cyl \n\nm <- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\npredictions(m, newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n\n Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp drat cyl am\n     22.0       1.29 17.0   <0.001 214.0  19.4   24.5 123  3.7   6  1\n     18.2       1.27 14.3   <0.001 151.9  15.7   20.7 123  3.7   6  0\n     25.5       1.32 19.3   <0.001 274.0  23.0   28.1 123  3.7   4  1\n     21.8       1.54 14.1   <0.001 148.3  18.8   24.8 123  3.7   4  0\n     22.6       2.14 10.6   <0.001  84.2  18.4   26.8 123  3.7   8  1\n     18.9       1.73 10.9   <0.001  89.0  15.5   22.3 123  3.7   8  0\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, drat, cyl, am \n\n# Average Adjusted Predictions (AAP)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nmod <- lm(mpg ~ hp * am * vs, mtcars)\n\navg_predictions(mod)\n\n\n Estimate Std. Error    z Pr(>|z|)   S 2.5 % 97.5 %\n     20.1      0.484 41.5   <0.001 Inf  19.1     21\n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\npredictions(mod, by = \"am\")\n\n\n am Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n  1     24.4      0.760 32.1   <0.001 748.3  22.9   25.9\n  0     17.1      0.629 27.3   <0.001 541.7  15.9   18.4\n\nColumns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Conditional Adjusted Predictions\nplot_predictions(mod, condition = \"hp\")\n\n\n\n# Counterfactual predictions with the `variables` argument\n# the `mtcars` dataset has 32 rows\n\nmod <- lm(mpg ~ hp + am, data = mtcars)\np <- predictions(mod)\nhead(p)\n\n\n Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n     25.4      0.818 31.0   <0.001 700.5  23.8   27.0\n     25.4      0.818 31.0   <0.001 700.5  23.8   27.0\n     26.4      0.850 31.1   <0.001 701.1  24.7   28.1\n     20.1      0.775 25.9   <0.001 490.0  18.6   21.6\n     16.3      0.677 24.0   <0.001 421.6  15.0   17.6\n     20.4      0.796 25.6   <0.001 478.6  18.8   22.0\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, am \n\nnrow(p)\n\n[1] 32\n\n# average counterfactual predictions\navg_predictions(mod, variables = \"am\")\n\n\n am Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n  0     17.9      0.676 26.6   <0.001 513.7  16.6   19.3\n  1     23.2      0.822 28.3   <0.001 581.2  21.6   24.8\n\nColumns: am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# counterfactual predictions obtained by replicating the entire for different\n# values of the predictors\np <- predictions(mod, variables = list(hp = c(90, 110)))\nnrow(p)\n\n[1] 64\n\n# hypothesis test: is the prediction in the 1st row equal to the prediction in the 2nd row\nmod <- lm(mpg ~ wt + drat, data = mtcars)\n\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 = b2\")\n\n\n  Term Estimate Std. Error z Pr(>|z|)    S 2.5 % 97.5 %\n b1=b2     4.78      0.797 6   <0.001 28.9  3.22   6.35\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# same hypothesis test using row indices\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = \"b1 - b2 = 0\")\n\n\n    Term Estimate Std. Error z Pr(>|z|)    S 2.5 % 97.5 %\n b1-b2=0     4.78      0.797 6   <0.001 28.9  3.22   6.35\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# same hypothesis test using numeric vector of weights\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = c(1, -1))\n\n\n   Term Estimate Std. Error z Pr(>|z|)    S 2.5 % 97.5 %\n custom     4.78      0.797 6   <0.001 28.9  3.22   6.35\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# two custom contrasts using a matrix of weights\nlc <- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\npredictions(\n    mod,\n    newdata = datagrid(wt = 2:3),\n    hypothesis = lc)\n\n\n   Term Estimate Std. Error    z Pr(>|z|)     S  2.5 % 97.5 % drat wt\n custom     4.78      0.797  6.0   <0.001  28.9   3.22   6.35  3.6  2\n custom   115.21      3.647 31.6   <0.001 725.0 108.07 122.36  3.6  3\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, drat, wt \n\n# `by` argument\nmod <- lm(mpg ~ hp * am * vs, data = mtcars)\npredictions(mod, by = c(\"am\", \"vs\"))\n\n\n am vs Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n  1  0     19.7      1.119 17.7   <0.001 229.3  17.6   21.9\n  1  1     28.4      1.036 27.4   <0.001 546.3  26.3   30.4\n  0  1     20.7      1.036 20.0   <0.001 294.0  18.7   22.8\n  0  0     15.0      0.791 19.0   <0.001 265.7  13.5   16.6\n\nColumns: am, vs, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nlibrary(nnet)\nnom <- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\n\n# first 5 raw predictions\npredictions(nom, type = \"probs\") |> head()\n\n\n Group Estimate Std. Error        z Pr(>|z|)   S     2.5 %   97.5 %\n     3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n     3 3.62e-05   2.00e-03   0.0181   0.9856 0.0 -3.89e-03 3.96e-03\n     3 9.35e-08   6.91e-06   0.0135   0.9892 0.0 -1.35e-05 1.36e-05\n     3 4.04e-01   1.97e-01   2.0569   0.0397 4.7  1.91e-02 7.90e-01\n     3 1.00e+00   1.25e-03 802.4451   <0.001 Inf  9.98e-01 1.00e+00\n     3 5.18e-01   2.90e-01   1.7884   0.0737 3.8 -4.97e-02 1.09e+00\n\nColumns: rowid, group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, gear, mpg, am, vs \n\n# average predictions\navg_predictions(nom, type = \"probs\", by = \"group\")\n\n\n Group Estimate Std. Error     z Pr(>|z|)     S  2.5 % 97.5 %\n     3    0.469     0.0404 11.60   <0.001 100.9 0.3895  0.548\n     4    0.375     0.0614  6.11   <0.001  29.9 0.2546  0.495\n     5    0.156     0.0462  3.38   <0.001  10.4 0.0656  0.247\n\nColumns: group, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nby <- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\n\npredictions(nom, type = \"probs\", by = by)\n\n\n Estimate Std. Error     z Pr(>|z|)     S  2.5 % 97.5 %  By\n    0.422     0.0231 18.25   <0.001 244.7 0.3766  0.467 3,4\n    0.156     0.0462  3.38   <0.001  10.4 0.0656  0.247 5  \n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by \n\n# sum of predicted probabilities for combined response levels\nmod <- multinom(factor(cyl) ~ mpg + am, data = mtcars, trace = FALSE)\nby <- data.frame(\n    by = c(\"4,6\", \"4,6\", \"8\"),\n    group = as.character(c(4, 6, 8)))\npredictions(mod, newdata = \"mean\", byfun = sum, by = by)\n\n\n Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %  By\n   0.9158      0.121 7.547   <0.001 44.3  0.678  1.154 4,6\n   0.0842      0.121 0.693    0.488  1.0 -0.154  0.322 8  \n\nColumns: estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, by"
  },
  {
    "objectID": "reference/comparisons.html#description",
    "href": "reference/comparisons.html#description",
    "title": "comparisons",
    "section": "Description",
    "text": "Description\n\nPredict the outcome variable at different regressor values (e.g., college graduates vs. others), and compare those predictions by computing a difference, ratio, or some other function. comparisons() can return many quantities of interest, such as contrasts, differences, risk ratios, changes in log odds, slopes, elasticities, etc.\n\n\n\n\ncomparisons(): unit-level (conditional) estimates.\n\n\n\n\navg_comparisons(): average (marginal) estimates.\n\n\n\n\nvariables identifies the focal regressors whose \"effect\" we are interested in. comparison determines how predictions with different regressor values are compared (difference, ratio, odds, etc.). The newdata argument and the datagrid() function control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\n\n\nSee the comparisons vignette and package website for worked examples and case studies:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/comparisons.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/"
  },
  {
    "objectID": "reference/comparisons.html#usage",
    "href": "reference/comparisons.html#usage",
    "title": "comparisons",
    "section": "Usage",
    "text": "Usage\ncomparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  comparison = \"difference\",\n  type = NULL,\n  vcov = TRUE,\n  by = FALSE,\n  conf_level = 0.95,\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  ...\n)\n\navg_comparisons(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  vcov = TRUE,\n  by = TRUE,\n  conf_level = 0.95,\n  comparison = \"difference\",\n  transform = NULL,\n  cross = FALSE,\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  ...\n)"
  },
  {
    "objectID": "reference/comparisons.html#arguments",
    "href": "reference/comparisons.html#arguments",
    "title": "comparisons",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate the comparisons.\n\n\n\n\nNULL (default): Unit-level contrasts for each observed value in the original dataset (empirical distribution). See insight::get_data()\n\n\n\n\ndata frame: Unit-level contrasts for each row of the newdata data frame.\n\n\n\n\nstring:\n\n\n\n\n\"mean\": Contrasts at the Mean. Contrasts when each predictor is held at its mean or mode.\n\n\n\n\n\"median\": Contrasts at the Median. Contrasts when each predictor is held at its median or mode.\n\n\n\n\n\"marginalmeans\": Contrasts at Marginal Means.\n\n\n\n\n\"tukey\": Contrasts at Tukey’s 5 numbers.\n\n\n\n\n\"grid\": Contrasts on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\n\n\nnewdata = datagrid(mpg = fivenum): mpg variable held at Tukey’s five numbers (using the fivenum function), and other regressors fixed at their means or modes.\n\n\n\n\nSee the Examples section and the datagrid documentation.\n\n\n\n\n\n\n\n\nvariables\n\n\nFocal variables\n\n\n\n\nNULL: compute comparisons for all the variables in the model object (can be slow).\n\n\n\n\nCharacter vector: subset of variables (usually faster).\n\n\n\n\nNamed list: names identify the subset of variables of interest, and values define the type of contrast to compute. Acceptable values depend on the variable type:\n\n\n\n\nFactor or character variables:\n\n\n\n\n\"reference\": Each factor level is compared to the factor reference (base) level\n\n\n\n\n\"all\": All combinations of observed levels\n\n\n\n\n\"sequential\": Each factor level is compared to the previous factor level\n\n\n\n\n\"pairwise\": Each factor level is compared to all other levels\n\n\n\n\n\"minmax\": The highest and lowest levels of a factor.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses.\n\n\n\n\nVector of length 2 with the two values to compare.\n\n\n\n\n\n\nLogical variables:\n\n\n\n\nNULL: contrast between TRUE and FALSE\n\n\n\n\n\n\nNumeric variables:\n\n\n\n\nNumeric of length 1: Contrast for a gap of x, computed at the observed value plus and minus x / 2. For example, estimating a +1 contrast compares adjusted predictions when the regressor is equal to its observed value minus 0.5 and its observed value plus 0.5.\n\n\n\n\nNumeric vector of length 2: Contrast between the 2nd element and the 1st element of the x vector.\n\n\n\n\nData frame with the same number of rows as newdata, with two columns of \"low\" and \"high\" values to compare.\n\n\n\n\nFunction which accepts a numeric vector and returns a data frame with two columns of \"low\" and \"high\" values to compare. See examples below.\n\n\n\n\n\"iqr\": Contrast across the interquartile range of the regressor.\n\n\n\n\n\"sd\": Contrast across one standard deviation around the regressor mean.\n\n\n\n\n\"2sd\": Contrast across two standard deviations around the regressor mean.\n\n\n\n\n\"minmax\": Contrast between the maximum and the minimum values of the regressor.\n\n\n\n\n\n\nExamples:\n\n\n\n\nvariables = list(gear = “pairwise”, hp = 10)\n\n\n\n\nvariables = list(gear = “sequential”, hp = c(100, 120))\n\n\n\n\nSee the Examples section below for more.\n\n\n\n\n\n\n\n\n\n\ncomparison\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\n\n\nstring: shortcuts to common contrast functions.\n\n\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, expdydx, expdydxavg, expdydxavgwts\n\n\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the default value is used. This default is the first model-related row in the marginaleffects:::type_dictionary dataframe.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntransform\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\ncross\n\n\n\n\nFALSE: Contrasts represent the change in adjusted predictions when one predictor changes and all other variables are held constant.\n\n\n\n\nTRUE: Contrasts represent the changes in adjusted predictions when all the predictors specified in the variables argument are manipulated simultaneously (a \"cross-contrast\").\n\n\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ⁠avg_*()⁠ or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\neps\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/comparisons.html#value",
    "href": "reference/comparisons.html#value",
    "title": "comparisons",
    "section": "Value",
    "text": "Value\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\n\n\nrowid: row number of the newdata data frame\n\n\n\n\ntype: prediction type, as defined by the type argument\n\n\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\n\n\nterm: the variable whose marginal effect is computed\n\n\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\n\n\nstd.error: standard errors computed by via the delta method.\n\n\n\n\np.value: p value associated to the estimate column. The null is determined by the hypothesis argument (0 by default), and p values are computed before applying the transform argument.\n\n\n\n\nSee ?print.marginaleffects for printing options."
  },
  {
    "objectID": "reference/comparisons.html#functions",
    "href": "reference/comparisons.html#functions",
    "title": "comparisons",
    "section": "Functions",
    "text": "Functions\n\n\n\navg_comparisons(): Average comparisons"
  },
  {
    "objectID": "reference/comparisons.html#standard-errors-using-the-delta-method",
    "href": "reference/comparisons.html#standard-errors-using-the-delta-method",
    "title": "comparisons",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/bootstrap.html"
  },
  {
    "objectID": "reference/comparisons.html#model-specific-arguments",
    "href": "reference/comparisons.html#model-specific-arguments",
    "title": "comparisons",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws"
  },
  {
    "objectID": "reference/comparisons.html#comparison-argument-functions",
    "href": "reference/comparisons.html#comparison-argument-functions",
    "title": "comparisons",
    "section": "comparison argument functions",
    "text": "comparison argument functions\n\nThe following transformations can be applied by supplying one of the shortcut strings to the comparison argument. hi is a vector of adjusted predictions for the \"high\" side of the contrast. lo is a vector of adjusted predictions for the \"low\" side of the contrast. y is a vector of adjusted predictions for the original data. x is the predictor in the original data. eps is the step size to use to compute derivatives and elasticities.\n\n\n\n\n\nShortcut\n\n\nFunction\n\n\n\n\ndifference\n\n\n(hi, lo) hi - lo\n\n\n\n\ndifferenceavg\n\n\n(hi, lo) mean(hi) - mean(lo)\n\n\n\n\ndydx\n\n\n(hi, lo, eps) (hi - lo)/eps\n\n\n\n\neyex\n\n\n(hi, lo, eps, y, x) (hi - lo)/eps * (x/y)\n\n\n\n\neydx\n\n\n(hi, lo, eps, y, x) ((hi - lo)/eps)/y\n\n\n\n\ndyex\n\n\n(hi, lo, eps, x) ((hi - lo)/eps) * x\n\n\n\n\ndydxavg\n\n\n(hi, lo, eps) mean((hi - lo)/eps)\n\n\n\n\neyexavg\n\n\n(hi, lo, eps, y, x) mean((hi - lo)/eps * (x/y))\n\n\n\n\neydxavg\n\n\n(hi, lo, eps, y, x) mean(((hi - lo)/eps)/y)\n\n\n\n\ndyexavg\n\n\n(hi, lo, eps, x) mean(((hi - lo)/eps) * x)\n\n\n\n\nratio\n\n\n(hi, lo) hi/lo\n\n\n\n\nratioavg\n\n\n(hi, lo) mean(hi)/mean(lo)\n\n\n\n\nlnratio\n\n\n(hi, lo) log(hi/lo)\n\n\n\n\nlnratioavg\n\n\n(hi, lo) log(mean(hi)/mean(lo))\n\n\n\n\nlnor\n\n\n(hi, lo) log((hi/(1 - hi))/(lo/(1 - lo)))\n\n\n\n\nlnoravg\n\n\n(hi, lo) log((mean(hi)/(1 - mean(hi)))/(mean(lo)/(1 - mean(lo))))\n\n\n\n\nexpdydx\n\n\n(hi, lo, eps) ((exp(hi) - exp(lo))/exp(eps))/eps\n\n\n\n\nexpdydxavg\n\n\n(hi, lo, eps) mean(((exp(hi) - exp(lo))/exp(eps))/eps)"
  },
  {
    "objectID": "reference/comparisons.html#bayesian-posterior-summaries",
    "href": "reference/comparisons.html#bayesian-posterior-summaries",
    "title": "comparisons",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(“marginaleffects_posterior_interval” = “eti”)\n\n\noptions(“marginaleffects_posterior_interval” = “hdi”)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(“marginaleffects_posterior_center” = “mean”)\n\n\noptions(“marginaleffects_posterior_center” = “median”)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default)."
  },
  {
    "objectID": "reference/comparisons.html#equivalence-inferiority-superiority",
    "href": "reference/comparisons.html#equivalence-inferiority-superiority",
    "title": "comparisons",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n is an estimate, _ its estimated standard error, and [a, b] are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\nH_0: a\n\n\n\n\nH_1: > a\n\n\n\n\nt=(- a)/_\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\nH_0: b\n\n\n\n\nH_1: < b\n\n\n\n\nt=(- b)/_\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature."
  },
  {
    "objectID": "reference/comparisons.html#examples",
    "href": "reference/comparisons.html#examples",
    "title": "comparisons",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# Linear model\ntmp <- mtcars\ntmp$am <- as.logical(tmp$am)\nmod <- lm(mpg ~ am + factor(cyl), tmp)\navg_comparisons(mod, variables = list(cyl = \"reference\"))\n\nWarning: The `cyl` variable is treated as a categorical (factor) variable, but\n  the original data is of class numeric. It is safer and faster to convert\n  such variables to factor before fitting the model and calling `slopes`\n  functions.\n  \n  This warning appears once per session.\n\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n  cyl    6 - 4    -6.16       1.54 -4.01   <0.001 14.0  -9.17  -3.15\n  cyl    8 - 4   -10.07       1.45 -6.93   <0.001 37.8 -12.91  -7.22\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, variables = list(cyl = \"sequential\"))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n  cyl    6 - 4    -6.16       1.54 -4.01  < 0.001 14.0 -9.17  -3.15\n  cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0 -6.79  -1.03\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, variables = list(cyl = \"pairwise\"))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n  cyl    6 - 4    -6.16       1.54 -4.01  < 0.001 14.0  -9.17  -3.15\n  cyl    8 - 4   -10.07       1.45 -6.93  < 0.001 37.8 -12.91  -7.22\n  cyl    8 - 6    -3.91       1.47 -2.66  0.00781  7.0  -6.79  -1.03\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# GLM with different scale types\nmod <- glm(am ~ factor(gear), data = mtcars)\navg_comparisons(mod, type = \"response\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, type = \"link\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Contrasts at the mean\ncomparisons(mod, newdata = \"mean\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, gear \n\n# Contrasts between marginal means\ncomparisons(mod, newdata = \"marginalmeans\")\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n# Contrasts at user-specified values\ncomparisons(mod, newdata = datagrid(am = 0, gear = tmp$gear))\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, gear \n\ncomparisons(mod, newdata = datagrid(am = unique, gear = max))\n\n\n Term Contrast Estimate Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    4 - 3    0.667      0.117 5.68   <0.001 26.1 0.436  0.897\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n gear    5 - 3    1.000      0.157 6.39   <0.001 32.5 0.693  1.307\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, gear \n\nm <- lm(mpg ~ hp + drat + factor(cyl) + factor(am), data = mtcars)\ncomparisons(m, variables = \"hp\", newdata = datagrid(FUN_factor = unique, FUN_numeric = median))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)   S   2.5 % 97.5 % drat cyl\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016  3.7   6\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016  3.7   6\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016  3.7   4\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016  3.7   4\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016  3.7   8\n   hp       +1  -0.0452     0.0149 -3.04  0.00239 8.7 -0.0744 -0.016  3.7   8\n am\n  1\n  0\n  1\n  0\n  1\n  0\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp, drat, cyl, am \n\n# Numeric contrasts\nmod <- lm(mpg ~ hp, data = mtcars)\navg_comparisons(mod, variables = list(hp = 1))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %\n   hp       +1  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, variables = list(hp = 5))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n   hp       +5   -0.341     0.0506 -6.74   <0.001 35.9 -0.44 -0.242\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, variables = list(hp = c(90, 100)))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S  2.5 % 97.5 %\n   hp 100 - 90   -0.682      0.101 -6.74   <0.001 35.9 -0.881 -0.484\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, variables = list(hp = \"iqr\"))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n   hp  Q3 - Q1     -5.7      0.845 -6.74   <0.001 35.9 -7.35  -4.04\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, variables = list(hp = \"sd\"))\n\n\n Term                Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 %\n   hp (x + sd/2) - (x - sd/2)    -4.68      0.694 -6.74   <0.001 35.9 -6.04\n 97.5 %\n  -3.32\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\navg_comparisons(mod, variables = list(hp = \"minmax\"))\n\n\n Term  Contrast Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n   hp Max - Min    -19.3       2.86 -6.74   <0.001 35.9 -24.9  -13.7\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# using a function to specify a custom difference in one regressor\ndat <- mtcars\ndat$new_hp <- 49 * (dat$hp - min(dat$hp)) / (max(dat$hp) - min(dat$hp)) + 1\nmodlog <- lm(mpg ~ log(new_hp) + factor(cyl), data = dat)\nfdiff <- \\(x) data.frame(x, x + 10)\navg_comparisons(modlog, variables = list(new_hp = fdiff))\n\n\n   Term Contrast Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n new_hp   custom    -1.97      0.711 -2.78  0.00547 7.5 -3.37 -0.581\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Adjusted Risk Ratio: see the contrasts vignette\nmod <- glm(vs ~ mpg, data = mtcars, family = binomial)\navg_comparisons(mod, comparison = \"lnratioavg\", transform = exp)\n\n\n Term Contrast Estimate Pr(>|z|)    S 2.5 % 97.5 %\n  mpg mean(+1)     1.14   <0.001 31.9  1.09   1.18\n\nColumns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n# Adjusted Risk Ratio: Manual specification of the `comparison`\navg_comparisons(\n     mod,\n     comparison = function(hi, lo) log(mean(hi) / mean(lo)),\n     transform = exp)\n\n\n Term Contrast Estimate Pr(>|z|)    S 2.5 % 97.5 %\n  mpg       +1     1.14   <0.001 31.9  1.09   1.18\n\nColumns: term, contrast, estimate, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n# cross contrasts\nmod <- lm(mpg ~ factor(cyl) * factor(gear) + hp, data = mtcars)\navg_comparisons(mod, variables = c(\"cyl\", \"gear\"), cross = TRUE)\n\nWarning: Model matrix is rank deficient. Some variance-covariance parameters are\n  missing.\n\n\n\n C: cyl C: gear Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n  6 - 4   4 - 3   -0.631       3.40 -0.185    0.853 0.2 -7.30   6.04\n  6 - 4   5 - 3    2.678       4.62  0.580    0.562 0.8 -6.37  11.73\n  8 - 4   4 - 3    3.348       6.43  0.521    0.602 0.7 -9.25  15.95\n  8 - 4   5 - 3    5.525       5.87  0.942    0.346 1.5 -5.98  17.03\n\nColumns: term, contrast_cyl, contrast_gear, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# variable-specific contrasts\navg_comparisons(mod, variables = list(gear = \"sequential\", hp = 10))\n\nWarning: Model matrix is rank deficient. Some variance-covariance parameters are\n  missing.\n\n\n\n Term Contrast Estimate Std. Error      z Pr(>|z|)   S 2.5 % 97.5 %\n gear    4 - 3    3.409      2.587  1.318   0.1876 2.4 -1.66  8.481\n gear    5 - 4    2.628      2.747  0.957   0.3387 1.6 -2.76  8.011\n hp      +10     -0.574      0.225 -2.552   0.0107 6.5 -1.02 -0.133\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod <- lm(mpg ~ wt + drat, data = mtcars)\n\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n\n    Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n wt=drat    -6.23       1.05 -5.92   <0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# same hypothesis test using row indices\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n\n    Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n b1-b2=0    -6.23       1.05 -5.92   <0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# same hypothesis test using numeric vector of weights\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n\n   Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n custom    -6.23       1.05 -5.92   <0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# two custom contrasts using a matrix of weights\nlc <- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncomparisons(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n\n\n   Term Estimate Std. Error      z Pr(>|z|)    S  2.5 % 97.5 %\n custom    -6.23       1.05 -5.919   <0.001 28.2  -8.29  -4.16\n custom    -5.24       5.62 -0.931    0.352  1.5 -16.26   5.78\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# `by` argument\nmod <- lm(mpg ~ hp * am * vs, data = mtcars)\ncomparisons(mod, by = TRUE)\n\n\n Term Contrast Estimate Std. Error      z Pr(>|z|)    S  2.5 %  97.5 %\n   hp    +1     -0.0688     0.0182 -3.780   <0.001 12.6 -0.104 -0.0331\n   am    1 - 0   4.6980     1.0601  4.432   <0.001 16.7  2.620  6.7758\n   vs    1 - 0  -0.2943     2.3379 -0.126      0.9  0.2 -4.877  4.2879\n\nColumns: term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nmod <- lm(mpg ~ hp * am * vs, data = mtcars)\navg_comparisons(mod, variables = \"hp\", by = c(\"vs\", \"am\"))\n\n\n Term Contrast vs am Estimate Std. Error     z Pr(>|z|)   S   2.5 %   97.5 %\n   hp mean(+1)  0  1  -0.0368     0.0124 -2.97  0.00297 8.4 -0.0612 -0.01254\n   hp mean(+1)  1  1  -0.1112     0.0463 -2.40  0.01645 5.9 -0.2020 -0.02034\n   hp mean(+1)  1  0  -0.0994     0.0534 -1.86  0.06289 4.0 -0.2042  0.00534\n   hp mean(+1)  0  0  -0.0422     0.0248 -1.70  0.08879 3.5 -0.0907  0.00639\n\nColumns: term, contrast, vs, am, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\nlibrary(nnet)\nmod <- multinom(factor(gear) ~ mpg + am * vs, data = mtcars, trace = FALSE)\nby <- data.frame(\n    group = c(\"3\", \"4\", \"5\"),\n    by = c(\"3,4\", \"3,4\", \"5\"))\ncomparisons(mod, type = \"probs\", by = by)\n\n\n Term  By  Estimate Std. Error     z Pr(>|z|)   S   2.5 %  97.5 %\n  mpg 3,4  0.000463    0.00579  0.08  0.93624 0.1 -0.0109  0.0118\n  am  3,4 -0.222793    0.07959 -2.80  0.00512 7.6 -0.3788 -0.0668\n  vs  3,4  0.102102    0.07334  1.39  0.16386 2.6 -0.0416  0.2458\n  mpg 5   -0.000927    0.01159 -0.08  0.93624 0.1 -0.0236  0.0218\n  am  5    0.445585    0.15917  2.80  0.00512 7.6  0.1336  0.7576\n  vs  5   -0.204204    0.14668 -1.39  0.16386 2.6 -0.4917  0.0833\n\nColumns: term, by, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "reference/slopes.html#description",
    "href": "reference/slopes.html#description",
    "title": "slopes",
    "section": "Description",
    "text": "Description\n\nPartial derivative of the regression equation with respect to a regressor of interest.\n\n\n\n\nslopes(): unit-level (conditional) estimates.\n\n\n\n\navg_slopes(): average (marginal) estimates.\n\n\n\n\nThe newdata argument and the datagrid() function can be used to control where statistics are evaluated in the predictor space: \"at observed values\", \"at the mean\", \"at representative values\", etc.\n\n\nSee the slopes vignette and package website for worked examples and case studies:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/slopes.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/"
  },
  {
    "objectID": "reference/slopes.html#usage",
    "href": "reference/slopes.html#usage",
    "title": "slopes",
    "section": "Usage",
    "text": "Usage\nslopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = FALSE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  ...\n)\n\navg_slopes(\n  model,\n  newdata = NULL,\n  variables = NULL,\n  type = NULL,\n  by = TRUE,\n  vcov = TRUE,\n  conf_level = 0.95,\n  slope = \"dydx\",\n  wts = NULL,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  eps = NULL,\n  ...\n)"
  },
  {
    "objectID": "reference/slopes.html#arguments",
    "href": "reference/slopes.html#arguments",
    "title": "slopes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\nGrid of predictor values at which we evaluate the slopes.\n\n\n\n\nNULL (default): Unit-level slopes for each observed value in the original dataset. See insight::get_data()\n\n\n\n\ndata frame: Unit-level slopes for each row of the newdata data frame.\n\n\n\n\ndatagrid() call to specify a custom grid of regressors. For example:\n\n\n\n\nnewdata = datagrid(cyl = c(4, 6)): cyl variable equal to 4 and 6 and other regressors fixed at their means or modes.\n\n\n\n\nSee the Examples section and the datagrid() documentation.\n\n\n\n\n\n\nstring:\n\n\n\n\n\"mean\": Marginal Effects at the Mean. Slopes when each predictor is held at its mean or mode.\n\n\n\n\n\"median\": Marginal Effects at the Median. Slopes when each predictor is held at its median or mode.\n\n\n\n\n\"marginalmeans\": Marginal Effects at Marginal Means. See Details section below.\n\n\n\n\n\"tukey\": Marginal Effects at Tukey’s 5 numbers.\n\n\n\n\n\"grid\": Marginal Effects on a grid of representative numbers (Tukey’s 5 numbers and unique values of categorical predictors).\n\n\n\n\n\n\n\n\nvariables\n\n\nFocal variables\n\n\n\n\nNULL: compute slopes or comparisons for all the variables in the model object (can be slow).\n\n\n\n\nCharacter vector: subset of variables (usually faster).\n\n\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the default value is used. This default is the first model-related row in the marginaleffects:::type_dictionary dataframe.\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nslope\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\n\n\"dydx\": dY/dX\n\n\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\n\n\"eydx\": dY/dX * Y\n\n\n\n\n\"dyex\": dY/dX / X\n\n\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ⁠avg_*()⁠ or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\neps\n\n\nNULL or numeric value which determines the step size to use when calculating numerical derivatives: (f(x+eps)-f(x))/eps. When eps is NULL, the step size is 0.0001 multiplied by the difference between the maximum and minimum values of the variable with respect to which we are taking the derivative. Changing eps may be necessary to avoid numerical problems in certain models.\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/slopes.html#details",
    "href": "reference/slopes.html#details",
    "title": "slopes",
    "section": "Details",
    "text": "Details\n\nA \"slope\" or \"marginal effect\" is the partial derivative of the regression equation with respect to a variable in the model. This function uses automatic differentiation to compute slopes for a vast array of models, including non-linear models with transformations (e.g., polynomials). Uncertainty estimates are computed using the delta method.\n\n\nNumerical derivatives for the slopes function are calculated using a simple epsilon difference approach: Y / X = (f(X + /2) - f(X-/2)) / , where f is the predict() method associated with the model class, and  is determined by the eps argument."
  },
  {
    "objectID": "reference/slopes.html#value",
    "href": "reference/slopes.html#value",
    "title": "slopes",
    "section": "Value",
    "text": "Value\n\nA data.frame with one row per observation (per term/group) and several columns:\n\n\n\n\nrowid: row number of the newdata data frame\n\n\n\n\ntype: prediction type, as defined by the type argument\n\n\n\n\ngroup: (optional) value of the grouped outcome (e.g., categorical outcome models)\n\n\n\n\nterm: the variable whose marginal effect is computed\n\n\n\n\ndydx: slope of the outcome with respect to the term, for a given combination of predictor values\n\n\n\n\nstd.error: standard errors computed by via the delta method.\n\n\n\n\nSee ?print.marginaleffects for printing options."
  },
  {
    "objectID": "reference/slopes.html#functions",
    "href": "reference/slopes.html#functions",
    "title": "slopes",
    "section": "Functions",
    "text": "Functions\n\n\n\navg_slopes(): Average slopes"
  },
  {
    "objectID": "reference/slopes.html#standard-errors-using-the-delta-method",
    "href": "reference/slopes.html#standard-errors-using-the-delta-method",
    "title": "slopes",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/bootstrap.html"
  },
  {
    "objectID": "reference/slopes.html#model-specific-arguments",
    "href": "reference/slopes.html#model-specific-arguments",
    "title": "slopes",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws"
  },
  {
    "objectID": "reference/slopes.html#bayesian-posterior-summaries",
    "href": "reference/slopes.html#bayesian-posterior-summaries",
    "title": "slopes",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(“marginaleffects_posterior_interval” = “eti”)\n\n\noptions(“marginaleffects_posterior_interval” = “hdi”)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(“marginaleffects_posterior_center” = “mean”)\n\n\noptions(“marginaleffects_posterior_center” = “median”)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default)."
  },
  {
    "objectID": "reference/slopes.html#equivalence-inferiority-superiority",
    "href": "reference/slopes.html#equivalence-inferiority-superiority",
    "title": "slopes",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n is an estimate, _ its estimated standard error, and [a, b] are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\nH_0: a\n\n\n\n\nH_1: > a\n\n\n\n\nt=(- a)/_\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\nH_0: b\n\n\n\n\nH_1: < b\n\n\n\n\nt=(- b)/_\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature."
  },
  {
    "objectID": "reference/slopes.html#examples",
    "href": "reference/slopes.html#examples",
    "title": "slopes",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\n\n\n\n# Unit-level (conditional) Marginal Effects\nmod <- glm(am ~ hp * wt, data = mtcars, family = binomial)\nmfx <- slopes(mod)\nhead(mfx)\n\n\n Term Estimate Std. Error     z Pr(>|z|)   S     2.5 %   97.5 %\n   hp 0.006983   0.005848 1.194    0.232 2.1 -0.004478 0.018445\n   hp 0.016404   0.012338 1.330    0.184 2.4 -0.007778 0.040586\n   hp 0.002828   0.003764 0.751    0.452 1.1 -0.004549 0.010206\n   hp 0.001935   0.002441 0.793    0.428 1.2 -0.002848 0.006718\n   hp 0.002993   0.003203 0.934    0.350 1.5 -0.003285 0.009271\n   hp 0.000148   0.000322 0.458    0.647 0.6 -0.000484 0.000779\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, hp, wt \n\n# Average Marginal Effect (AME)\navg_slopes(mod, by = TRUE)\n\n\n Term Estimate Std. Error     z Pr(>|z|)   S    2.5 %   97.5 %\n   hp  0.00265    0.00209  1.27  0.20539 2.3 -0.00145  0.00676\n   wt -0.43578    0.14093 -3.09  0.00199 9.0 -0.71200 -0.15957\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Marginal Effect at the Mean (MEM)\nslopes(mod, newdata = datagrid())\n\n\n Term Estimate Std. Error     z Pr(>|z|)   S    2.5 % 97.5 %  hp   wt\n   hp  0.00853    0.00823  1.04    0.300 1.7 -0.00759 0.0246 147 3.22\n   wt -1.74453    1.55734 -1.12    0.263 1.9 -4.79685 1.3078 147 3.22\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, hp, wt \n\n# Marginal Effect at User-Specified Values\n# Variables not explicitly included in `datagrid()` are held at their means\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n\n Term Estimate Std. Error      z Pr(>|z|)   S    2.5 %  97.5 %   wt  hp\n   hp  0.00117    0.00171  0.684    0.494 1.0 -0.00218 0.00451 3.22 100\n   hp  0.00190    0.00240  0.789    0.430 1.2 -0.00281 0.00661 3.22 110\n   wt -0.19468    0.29925 -0.651    0.515 1.0 -0.78119 0.39184 3.22 100\n   wt -0.33154    0.42925 -0.772    0.440 1.2 -1.17284 0.50977 3.22 110\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, wt, hp \n\n# Group-Average Marginal Effects (G-AME)\n# Calculate marginal effects for each observation, and then take the average\n# marginal effect within each subset of observations with different observed\n# values for the `cyl` variable:\nmod2 <- lm(mpg ~ hp * cyl, data = mtcars)\navg_slopes(mod2, variables = \"hp\", by = \"cyl\")\n\n\n Term    Contrast cyl Estimate Std. Error      z Pr(>|z|)   S   2.5 %  97.5 %\n   hp mean(dY/dX)   6  -0.0523     0.0204 -2.561  0.01044 6.6 -0.0923 -0.0123\n   hp mean(dY/dX)   4  -0.0917     0.0353 -2.596  0.00942 6.7 -0.1610 -0.0225\n   hp mean(dY/dX)   8  -0.0128     0.0143 -0.891  0.37280 1.4 -0.0409  0.0153\n\nColumns: term, contrast, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo \n\n# Marginal Effects at User-Specified Values (counterfactual)\n# Variables not explicitly included in `datagrid()` are held at their\n# original values, and the whole dataset is duplicated once for each\n# combination of the values in `datagrid()`\nmfx <- slopes(mod,\n              newdata = datagrid(hp = c(100, 110),\n              grid_type = \"counterfactual\"))\nhead(mfx)\n\n\n rowidcf Term Estimate Std. Error     z Pr(>|z|)   S     2.5 %   97.5 %   wt\n       1   hp 0.012035   0.009939 1.211    0.226 2.1 -0.007446 0.031515 2.62\n       2   hp 0.014161   0.010526 1.345    0.179 2.5 -0.006470 0.034791 2.88\n       3   hp 0.001564   0.002196 0.712    0.476 1.1 -0.002739 0.005868 2.32\n       4   hp 0.001191   0.001733 0.687    0.492 1.0 -0.002206 0.004587 3.21\n       5   hp 0.000145   0.000321 0.453    0.651 0.6 -0.000484 0.000775 3.44\n       6   hp 0.000120   0.000274 0.439    0.661 0.6 -0.000416 0.000657 3.46\n  hp\n 100\n 100\n 100\n 100\n 100\n 100\n\nColumns: rowid, rowidcf, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, wt, hp \n\n# Heteroskedasticity robust standard errors\nmfx <- slopes(mod, vcov = sandwich::vcovHC(mod))\nhead(mfx)\n\n\n Term Estimate Std. Error     z Pr(>|z|)   S     2.5 %   97.5 %\n   hp 0.006983   0.009052 0.771    0.440 1.2 -0.010759 0.024725\n   hp 0.016404   0.012458 1.317    0.188 2.4 -0.008013 0.040821\n   hp 0.002828   0.004877 0.580    0.562 0.8 -0.006731 0.012388\n   hp 0.001935   0.002026 0.955    0.340 1.6 -0.002036 0.005906\n   hp 0.002993   0.002907 1.030    0.303 1.7 -0.002704 0.008690\n   hp 0.000148   0.000235 0.628    0.530 0.9 -0.000313 0.000609\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, am, hp, wt \n\n# hypothesis test: is the `hp` marginal effect at the mean equal to the `drat` marginal effect\nmod <- lm(mpg ~ wt + drat, data = mtcars)\n\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"wt = drat\")\n\n\n    Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n wt=drat    -6.23       1.05 -5.92   <0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# same hypothesis test using row indices\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = \"b1 - b2 = 0\")\n\n\n    Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n b1-b2=0    -6.23       1.05 -5.92   <0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# same hypothesis test using numeric vector of weights\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = c(1, -1))\n\n\n   Term Estimate Std. Error     z Pr(>|z|)    S 2.5 % 97.5 %\n custom    -6.23       1.05 -5.92   <0.001 28.2 -8.29  -4.16\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# two custom contrasts using a matrix of weights\nlc <- matrix(c(\n    1, -1,\n    2, 3),\n    ncol = 2)\ncolnames(lc) <- c(\"Contrast A\", \"Contrast B\")\nslopes(\n    mod,\n    newdata = \"mean\",\n    hypothesis = lc)\n\n\n       Term Estimate Std. Error      z Pr(>|z|)    S  2.5 % 97.5 %\n Contrast A    -6.23       1.05 -5.919   <0.001 28.2  -8.29  -4.16\n Contrast B    -5.24       5.62 -0.931    0.352  1.5 -16.26   5.78\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "reference/marginal_means.html#description",
    "href": "reference/marginal_means.html#description",
    "title": "marginal_means",
    "section": "Description",
    "text": "Description\n\nMarginal means are adjusted predictions, averaged across a grid of categorical predictors, holding other numeric predictors at their means. To learn more, read the marginal means vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/marginalmeans.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/"
  },
  {
    "objectID": "reference/marginal_means.html#usage",
    "href": "reference/marginal_means.html#usage",
    "title": "marginal_means",
    "section": "Usage",
    "text": "Usage\nmarginal_means(\n  model,\n  variables = NULL,\n  newdata = NULL,\n  vcov = TRUE,\n  conf_level = 0.95,\n  type = NULL,\n  transform = NULL,\n  cross = FALSE,\n  hypothesis = NULL,\n  equivalence = NULL,\n  p_adjust = NULL,\n  df = Inf,\n  wts = \"equal\",\n  by = NULL,\n  ...\n)"
  },
  {
    "objectID": "reference/marginal_means.html#arguments",
    "href": "reference/marginal_means.html#arguments",
    "title": "marginal_means",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nFocal variables\n\n\n\n\nCharacter vector of variable names: compute marginal means for each category of the listed variables.\n\n\n\n\nNULL: calculate marginal means for all logical, character, or factor variables in the dataset used to fit model. Hint: Set cross=TRUE to compute marginal means for combinations of focal variables.\n\n\n\n\n\n\nnewdata\n\n\nGrid of predictor values over which we marginalize.\n\n\n\n\nNULL create a grid with all combinations of all categorical predictors in the model. Warning: can be expensive.\n\n\n\n\nCharacter vector: subset of categorical variables to use when building the balanced grid of predictors. Other variables are held to their mean or mode.\n\n\n\n\nData frame: A data frame which includes all the predictors in the original model. The full dataset is replicated once for every combination of the focal variables in the variables argument, using the datagridcf() function.\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute marginal effects or contrasts. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the default value is used. This default is the first model-related row in the marginaleffects:::type_dictionary dataframe. If type is NULL and the default value is \"response\", the function tries to compute marginal means on the link scale before backtransforming them using the inverse link function.\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\ncross\n\n\nTRUE or FALSE\n\n\n\n\nFALSE (default): Marginal means are computed for each predictor individually.\n\n\n\n\nTRUE: Marginal means are computed for each combination of predictors specified in the variables argument.\n\n\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html\n\n\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\np_adjust\n\n\nAdjust p-values for multiple comparisons: \"holm\", \"hochberg\", \"hommel\", \"bonferroni\", \"BH\", \"BY\", or \"fdr\". See stats::p.adjust\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nwts\n\n\ncharacter value. Weights to use in the averaging.\n\n\n\n\n\"equal\": each combination of variables in newdata gets equal weight.\n\n\n\n\n\"cells\": each combination of values for the variables in the newdata gets a weight proportional to its frequency in the original data.\n\n\n\n\n\"proportional\": each combination of values for the variables in newdata – except for those in the variables argument – gets a weight proportional to its frequency in the original data.\n\n\n\n\n\n\nby\n\n\nCollapse marginal means into categories. Data frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/marginal_means.html#details",
    "href": "reference/marginal_means.html#details",
    "title": "marginal_means",
    "section": "Details",
    "text": "Details\n\nThis function begins by calling the predictions function to obtain a grid of predictors, and adjusted predictions for each cell. The grid includes all combinations of the categorical variables listed in the variables and newdata arguments, or all combinations of the categorical variables used to fit the model if newdata is NULL. In the prediction grid, numeric variables are held at their means.\n\n\nAfter constructing the grid and filling the grid with adjusted predictions, marginal_means computes marginal means for the variables listed in the variables argument, by average across all categories in the grid.\n\n\nmarginal_means can only compute standard errors for linear models, or for predictions on the link scale, that is, with the type argument set to \"link\".\n\n\nThe marginaleffects website compares the output of this function to the popular emmeans package, which provides similar but more advanced functionality: https://vincentarelbundock.github.io/marginaleffects/"
  },
  {
    "objectID": "reference/marginal_means.html#value",
    "href": "reference/marginal_means.html#value",
    "title": "marginal_means",
    "section": "Value",
    "text": "Value\n\nData frame of marginal means with one row per variable-value combination."
  },
  {
    "objectID": "reference/marginal_means.html#standard-errors-using-the-delta-method",
    "href": "reference/marginal_means.html#standard-errors-using-the-delta-method",
    "title": "marginal_means",
    "section": "Standard errors using the delta method",
    "text": "Standard errors using the delta method\n\nStandard errors for all quantities estimated by marginaleffects can be obtained via the delta method. This requires differentiating a function with respect to the coefficients in the model using a finite difference approach. In some models, the delta method standard errors can be sensitive to various aspects of the numeric differentiation strategy, including the step size. By default, the step size is set to 1e-8, or to 1e-4 times the smallest absolute model coefficient, whichever is largest.\n\n\nmarginaleffects can delegate numeric differentiation to the numDeriv package, which allows more flexibility. To do this, users can pass arguments to the numDeriv::jacobian function through a global option. For example:\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “simple”, method.args = list(eps = 1e-6)))\n\n\n\n\noptions(marginaleffects_numDeriv = list(method = “Richardson”, method.args = list(eps = 1e-5)))\n\n\n\n\noptions(marginaleffects_numDeriv = NULL)\n\n\n\n\nSee the \"Standard Errors and Confidence Intervals\" vignette on the marginaleffects website for more details on the computation of standard errors:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/uncertainty.html\n\n\nNote that the inferences() function can be used to compute uncertainty estimates using a bootstrap or simulation-based inference. See the vignette:\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/bootstrap.html"
  },
  {
    "objectID": "reference/marginal_means.html#model-specific-arguments",
    "href": "reference/marginal_means.html#model-specific-arguments",
    "title": "marginal_means",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws"
  },
  {
    "objectID": "reference/marginal_means.html#bayesian-posterior-summaries",
    "href": "reference/marginal_means.html#bayesian-posterior-summaries",
    "title": "marginal_means",
    "section": "Bayesian posterior summaries",
    "text": "Bayesian posterior summaries\n\nBy default, credible intervals in bayesian models are built as equal-tailed intervals. This can be changed to a highest density interval by setting a global option:\n\n\noptions(“marginaleffects_posterior_interval” = “eti”)\n\n\noptions(“marginaleffects_posterior_interval” = “hdi”)\n\n\nBy default, the center of the posterior distribution in bayesian models is identified by the median. Users can use a different summary function by setting a global option:\n\n\noptions(“marginaleffects_posterior_center” = “mean”)\n\n\noptions(“marginaleffects_posterior_center” = “median”)\n\n\nWhen estimates are averaged using the by argument, the tidy() function, or the summary() function, the posterior distribution is marginalized twice over. First, we take the average across units but within each iteration of the MCMC chain, according to what the user requested in by argument or tidy()/summary() functions. Then, we identify the center of the resulting posterior using the function supplied to the “marginaleffects_posterior_center” option (the median by default)."
  },
  {
    "objectID": "reference/marginal_means.html#equivalence-inferiority-superiority",
    "href": "reference/marginal_means.html#equivalence-inferiority-superiority",
    "title": "marginal_means",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n is an estimate, _ its estimated standard error, and [a, b] are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\nH_0: a\n\n\n\n\nH_1: > a\n\n\n\n\nt=(- a)/_\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\nH_0: b\n\n\n\n\nH_1: < b\n\n\n\n\nt=(- b)/_\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature."
  },
  {
    "objectID": "reference/marginal_means.html#examples",
    "href": "reference/marginal_means.html#examples",
    "title": "marginal_means",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\n\n# simple marginal means for each level of `cyl`\ndat <- mtcars\ndat$carb <- factor(dat$carb)\ndat$cyl <- factor(dat$cyl)\ndat$am <- as.logical(dat$am)\nmod <- lm(mpg ~ carb + cyl + am, dat)\n\nmarginal_means(\n  mod,\n  variables = \"cyl\")\n\n\n Term Value Mean Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n  cyl     6 20.4       1.34 15.2   <0.001 171.9  17.8   23.0\n  cyl     4 23.1       1.66 13.9   <0.001 144.3  19.9   26.4\n  cyl     8 16.2       1.07 15.1   <0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, value, cyl, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# collapse levels of cyl by averaging\nby <- data.frame(\n  cyl = c(4, 6, 8),\n  by = c(\"4 &amp; 6\", \"4 &amp; 6\", \"8\"))\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by)\n\n\n        By Mean Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %\n 4 &amp; 6 21.7       1.13 19.2   <0.001 270.8  19.5   24.0\n 8         16.2       1.07 15.1   <0.001 169.0  14.1   18.3\n\nResults averaged over levels of: carb, am, cyl \nColumns: by, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# pairwise differences between collapsed levels\nmarginal_means(mod,\n  variables = \"cyl\",\n  by = by,\n  hypothesis = \"pairwise\")\n\n\n          Term Mean Std. Error    z Pr(>|z|)    S 2.5 % 97.5 %\n 4 &amp; 6 - 8 5.54       1.51 3.66   <0.001 12.0  2.57    8.5\n\nResults averaged over levels of: carb, am, cyl \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# cross\nmarginal_means(mod,\n  variables = c(\"cyl\", \"carb\"),\n  cross = TRUE)\n\n\n Mean Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n 19.1       1.34 14.31   <0.001 151.8 16.53   21.8\n 23.1       1.77 13.08   <0.001 127.4 19.63   26.5\n 22.9       1.87 12.24   <0.001 112.0 19.20   26.5\n 22.6       2.37  9.56   <0.001  69.5 17.98   27.2\n 17.6       3.00  5.85   <0.001  27.6 11.68   23.5\n 17.0       3.48  4.89   <0.001  19.9 10.21   23.9\n 21.9       1.90 11.51   <0.001  99.4 18.15   25.6\n 25.8       1.26 20.43   <0.001 305.7 23.34   28.3\n 25.6       1.17 21.93   <0.001 351.8 23.30   27.9\n 25.3       2.37 10.71   <0.001  86.6 20.70   30.0\n 20.3       3.77  5.39   <0.001  23.7 12.91   27.7\n 19.8       3.81  5.18   <0.001  22.1 12.29   27.2\n 15.0       1.20 12.53   <0.001 117.2 12.63   17.3\n 18.9       1.94  9.74   <0.001  72.1 15.11   22.7\n 18.7       1.57 11.90   <0.001 106.0 15.61   21.8\n 18.4       1.83 10.07   <0.001  76.8 14.85   22.0\n 13.4       3.36  3.99   <0.001  13.9  6.81   20.0\n 12.9       3.00  4.28   <0.001  15.7  6.98   18.8\n\nResults averaged over levels of: am \nColumns: cyl, carb, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# collapsed cross\nby <- expand.grid(\n  cyl = unique(mtcars$cyl),\n  carb = unique(mtcars$carb))\nby$by <- ifelse(\n  by$cyl == 4,\n  paste(\"Control:\", by$carb),\n  paste(\"Treatment:\", by$carb))\n\n\n# Convert numeric variables to categorical before fitting the model\ndat <- mtcars\ndat$am <- as.logical(dat$am)\ndat$carb <- as.factor(dat$carb)\nmod <- lm(mpg ~ hp + am + carb, data = dat)\n\n# Compute and summarize marginal means\nmarginal_means(mod)\n\n\n Term Value Mean Std. Error     z Pr(>|z|)     S 2.5 % 97.5 %\n am   TRUE  23.1      0.974 23.72   <0.001 410.9  21.2   25.0\n am   FALSE 17.9      1.244 14.37   <0.001 153.0  15.4   20.3\n carb 4     18.8      1.042 18.06   <0.001 239.9  16.8   20.9\n carb 1     22.0      1.345 16.35   <0.001 197.2  19.4   24.6\n carb 2     21.5      1.025 20.95   <0.001 321.5  19.5   23.5\n carb 3     20.6      1.780 11.55   <0.001 100.1  17.1   24.0\n carb 6     18.5      3.019  6.12   <0.001  30.0  12.6   24.4\n carb 8     21.6      4.055  5.33   <0.001  23.3  13.7   29.6\n\nResults averaged over levels of: hp, am, carb \nColumns: term, value, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Contrast between marginal means (carb2 - carb1), or \"is the 1st marginal means equal to the 2nd?\"\n# see the vignette on \"Hypothesis Tests and Custom Contrasts\" on the `marginaleffects` website.\nlc <- c(-1, 1, 0, 0, 0, 0)\nmarginal_means(mod, variables = \"carb\", hypothesis = \"b2 = b1\")\n\n\n  Term Mean Std. Error    z Pr(>|z|)   S  2.5 % 97.5 %\n b2=b1 3.18        1.9 1.67   0.0949 3.4 -0.552    6.9\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n   Term Mean Std. Error    z Pr(>|z|)   S  2.5 % 97.5 %\n custom 3.18        1.9 1.67   0.0949 3.4 -0.552    6.9\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# Multiple custom contrasts\nlc <- matrix(c(\n    -2, 1, 1, 0, -1, 1,\n    -1, 1, 0, 0, 0, 0\n    ),\n  ncol = 2,\n  dimnames = list(NULL, c(\"A\", \"B\")))\nmarginal_means(mod, variables = \"carb\", hypothesis = lc)\n\n\n Term Mean Std. Error    z Pr(>|z|)   S  2.5 % 97.5 %\n    A 8.99       4.73 1.90   0.0572 4.1 -0.273   18.2\n    B 3.18       1.90 1.67   0.0949 3.4 -0.552    6.9\n\nResults averaged over levels of: am, carb \nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high"
  },
  {
    "objectID": "reference/plot_predictions.html#description",
    "href": "reference/plot_predictions.html#description",
    "title": "plot_predictions",
    "section": "Description",
    "text": "Description\n\nPlot predictions on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\n\n\nThe by argument is used to plot marginal predictions, that is, predictions made on the original data, but averaged by subgroups. This is analogous to using the by argument in the predictions() function.\n\n\nThe condition argument is used to plot conditional predictions, that is, predictions made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a predictions() call.\n\n\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below.\n\n\nSee the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/plot.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects"
  },
  {
    "objectID": "reference/plot_predictions.html#usage",
    "href": "reference/plot_predictions.html#usage",
    "title": "plot_predictions",
    "section": "Usage",
    "text": "Usage\nplot_predictions(\n  model,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  transform = NULL,\n  points = 0,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)"
  },
  {
    "objectID": "reference/plot_predictions.html#arguments",
    "href": "reference/plot_predictions.html#arguments",
    "title": "plot_predictions",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object\n\n\n\n\ncondition\n\n\nConditional predictions\n\n\n\n\nCharacter vector (max length 3): Names of the predictors to display.\n\n\n\n\nNamed list (max length 3): List names correspond to predictors. List elements can be:\n\n\n\n\nNumeric vector\n\n\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n\n\n1: x-axis. 2: color/shape. 3: facets.\n\n\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum\n\n\n\n\n\n\nby\n\n\nMarginal predictions\n\n\n\n\nCharacter vector (max length 3): Names of the categorical predictors to marginalize across.\n\n\n\n\n1: x-axis. 2: color. 3: facets.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the predictions() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the default value is used. This default is the first model-related row in the marginaleffects:::type_dictionary dataframe.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ⁠avg_*()⁠ or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ntransform\n\n\nA function applied to unit-level adjusted predictions and confidence intervals just before the function returns results. For bayesian models, this function is applied to individual draws from the posterior distribution, before computing summaries.\n\n\n\n\npoints\n\n\nNumber between 0 and 1 which controls the transparency of raw data points. 0 (default) does not display any points.\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/plot_predictions.html#value",
    "href": "reference/plot_predictions.html#value",
    "title": "plot_predictions",
    "section": "Value",
    "text": "Value\n\nA ggplot2 object or data frame (if draw=FALSE)"
  },
  {
    "objectID": "reference/plot_predictions.html#model-specific-arguments",
    "href": "reference/plot_predictions.html#model-specific-arguments",
    "title": "plot_predictions",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws"
  },
  {
    "objectID": "reference/plot_predictions.html#examples",
    "href": "reference/plot_predictions.html#examples",
    "title": "plot_predictions",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ hp + wt, data = mtcars)\nplot_predictions(mod, condition = \"wt\")\n\n\n\nmod <- lm(mpg ~ hp * wt * am, data = mtcars)\nplot_predictions(mod, condition = c(\"hp\", \"wt\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = \"threenum\"))\n\n\n\nplot_predictions(mod, condition = list(\"hp\", wt = range))"
  },
  {
    "objectID": "reference/plot_comparisons.html#description",
    "href": "reference/plot_comparisons.html#description",
    "title": "plot_comparisons",
    "section": "Description",
    "text": "Description\n\nPlot comparisons on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\n\n\nThe by argument is used to plot marginal comparisons, that is, comparisons made on the original data, but averaged by subgroups. This is analogous to using the by argument in the comparisons() function.\n\n\nThe condition argument is used to plot conditional comparisons, that is, comparisons made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a comparisons() call.\n\n\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/plot.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects"
  },
  {
    "objectID": "reference/plot_comparisons.html#usage",
    "href": "reference/plot_comparisons.html#usage",
    "title": "plot_comparisons",
    "section": "Usage",
    "text": "Usage\nplot_comparisons(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  comparison = \"difference\",\n  transform = NULL,\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)"
  },
  {
    "objectID": "reference/plot_comparisons.html#arguments",
    "href": "reference/plot_comparisons.html#arguments",
    "title": "plot_comparisons",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nName of the variable whose contrast we want to plot on the y-axis.\n\n\n\n\ncondition\n\n\nConditional slopes\n\n\n\n\nCharacter vector (max length 3): Names of the predictors to display.\n\n\n\n\nNamed list (max length 3): List names correspond to predictors. List elements can be:\n\n\n\n\nNumeric vector\n\n\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n\n\n1: x-axis. 2: color/shape. 3: facets.\n\n\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum.\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the comparisons() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the default value is used. This default is the first model-related row in the marginaleffects:::type_dictionary dataframe.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ⁠avg_*()⁠ or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\ncomparison\n\n\nHow should pairs of predictions be compared? Difference, ratio, odds ratio, or user-defined functions.\n\n\n\n\nstring: shortcuts to common contrast functions.\n\n\n\n\nSupported shortcuts strings: difference, differenceavg, differenceavgwts, dydx, eyex, eydx, dyex, dydxavg, eyexavg, eydxavg, dyexavg, dydxavgwts, eyexavgwts, eydxavgwts, dyexavgwts, ratio, ratioavg, ratioavgwts, lnratio, lnratioavg, lnratioavgwts, lnor, lnoravg, lnoravgwts, expdydx, expdydxavg, expdydxavgwts\n\n\n\n\nSee the Comparisons section below for definitions of each transformation.\n\n\n\n\n\n\nfunction: accept two equal-length numeric vectors of adjusted predictions (hi and lo) and returns a vector of contrasts of the same length, or a unique numeric value.\n\n\n\n\nSee the Transformations section below for examples of valid functions.\n\n\n\n\n\n\n\n\ntransform\n\n\nstring or function. Transformation applied to unit-level estimates and confidence intervals just before the function returns results. Functions must accept a vector and return a vector of the same length. Support string shortcuts: \"exp\", \"ln\"\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/plot_comparisons.html#value",
    "href": "reference/plot_comparisons.html#value",
    "title": "plot_comparisons",
    "section": "Value",
    "text": "Value\n\nA ggplot2 object"
  },
  {
    "objectID": "reference/plot_comparisons.html#model-specific-arguments",
    "href": "reference/plot_comparisons.html#model-specific-arguments",
    "title": "plot_comparisons",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws"
  },
  {
    "objectID": "reference/plot_comparisons.html#examples",
    "href": "reference/plot_comparisons.html#examples",
    "title": "plot_comparisons",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nmod <- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_comparisons(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_comparisons(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\nWarning: The `am` variable is treated as a categorical (factor) variable, but the\n  original data is of class numeric. It is safer and faster to convert\n  such variables to factor before fitting the model and calling `slopes`\n  functions.\n  \n  This warning appears once per session.\n\n\n\n\nplot_comparisons(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))"
  },
  {
    "objectID": "reference/plot_slopes.html#description",
    "href": "reference/plot_slopes.html#description",
    "title": "plot_slopes",
    "section": "Description",
    "text": "Description\n\nPlot slopes on the y-axis against values of one or more predictors (x-axis, colors/shapes, and facets).\n\n\nThe by argument is used to plot marginal slopes, that is, slopes made on the original data, but averaged by subgroups. This is analogous to using the by argument in the slopes() function.\n\n\nThe condition argument is used to plot conditional slopes, that is, slopes made on a user-specified grid. This is analogous to using the newdata argument and datagrid() function in a slopes() call.\n\n\nAll unspecified variables are held at their mean or mode. This includes grouping variables in mixed-effects models, so analysts who fit such models may want to specify the groups of interest using the variables argument, or supply model-specific arguments to compute population-level estimates. See details below. See the \"Plots\" vignette and website for tutorials and information on how to customize plots:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/plot.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects"
  },
  {
    "objectID": "reference/plot_slopes.html#usage",
    "href": "reference/plot_slopes.html#usage",
    "title": "plot_slopes",
    "section": "Usage",
    "text": "Usage\nplot_slopes(\n  model,\n  variables = NULL,\n  condition = NULL,\n  by = NULL,\n  newdata = NULL,\n  type = \"response\",\n  vcov = NULL,\n  conf_level = 0.95,\n  wts = NULL,\n  slope = \"dydx\",\n  rug = FALSE,\n  gray = FALSE,\n  draw = TRUE,\n  ...\n)"
  },
  {
    "objectID": "reference/plot_slopes.html#arguments",
    "href": "reference/plot_slopes.html#arguments",
    "title": "plot_slopes",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nvariables\n\n\nName of the variable whose marginal effect (slope) we want to plot on the y-axis.\n\n\n\n\ncondition\n\n\nConditional slopes\n\n\n\n\nCharacter vector (max length 3): Names of the predictors to display.\n\n\n\n\nNamed list (max length 3): List names correspond to predictors. List elements can be:\n\n\n\n\nNumeric vector\n\n\n\n\nFunction which returns a numeric vector or a set of unique categorical values\n\n\n\n\nShortcut strings for common reference values: \"minmax\", \"quartile\", \"threenum\"\n\n\n\n\n\n\n1: x-axis. 2: color/shape. 3: facets.\n\n\n\n\nNumeric variables in positions 2 and 3 are summarized by Tukey’s five numbers ?stats::fivenum.\n\n\n\n\n\n\nby\n\n\nAggregate unit-level estimates (aka, marginalize, average over). Valid inputs:\n\n\n\n\nFALSE: return the original unit-level estimates.\n\n\n\n\nTRUE: aggregate estimates for each term.\n\n\n\n\nCharacter vector of column names in newdata or in the data frame produced by calling the function without the by argument.\n\n\n\n\nData frame with a by column of group labels, and merging columns shared by newdata or the data frame produced by calling the same function without the by argument.\n\n\n\n\nSee examples below.\n\n\n\n\n\n\nnewdata\n\n\nWhen newdata is NULL, the grid is determined by the condition argument. When newdata is not NULL, the argument behaves in the same way as in the slopes() function.\n\n\n\n\ntype\n\n\nstring indicates the type (scale) of the predictions used to compute contrasts or slopes. This can differ based on the model type, but will typically be a string such as: \"response\", \"link\", \"probs\", or \"zero\". When an unsupported string is entered, the model-specific list of acceptable values is returned in an error message. When type is NULL, the default value is used. This default is the first model-related row in the marginaleffects:::type_dictionary dataframe.\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\nwts\n\n\nstring or numeric: weights to use when computing average contrasts or slopes. These weights only affect the averaging in ⁠avg_*()⁠ or with the by argument, and not the unit-level estimates themselves. Internally, estimates and weights are passed to the weighted.mean() function.\n\n\n\n\nstring: column name of the weights variable in newdata. When supplying a column name to wts, it is recommended to supply the original data (including the weights variable) explicitly to newdata.\n\n\n\n\nnumeric: vector of length equal to the number of rows in the original data or in newdata (if supplied).\n\n\n\n\n\n\nslope\n\n\nstring indicates the type of slope or (semi-)elasticity to compute:\n\n\n\n\n\"dydx\": dY/dX\n\n\n\n\n\"eyex\": dY/dX * Y / X\n\n\n\n\n\"eydx\": dY/dX * Y\n\n\n\n\n\"dyex\": dY/dX / X\n\n\n\n\nY is the predicted value of the outcome; X is the observed value of the predictor.\n\n\n\n\n\n\nrug\n\n\nTRUE displays tick marks on the axes to mark the distribution of raw data.\n\n\n\n\ngray\n\n\nFALSE grayscale or color plot\n\n\n\n\ndraw\n\n\nTRUE returns a ggplot2 plot. FALSE returns a data.frame of the underlying data.\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/plot_slopes.html#value",
    "href": "reference/plot_slopes.html#value",
    "title": "plot_slopes",
    "section": "Value",
    "text": "Value\n\nA ggplot2 object"
  },
  {
    "objectID": "reference/plot_slopes.html#model-specific-arguments",
    "href": "reference/plot_slopes.html#model-specific-arguments",
    "title": "plot_slopes",
    "section": "Model-Specific Arguments",
    "text": "Model-Specific Arguments\n\nSome model types allow model-specific arguments to modify the nature of marginal effects, predictions, marginal means, and contrasts. Please report other package-specific predict() arguments on Github so we can add them to the table below.\n\n\nhttps://github.com/vincentarelbundock/marginaleffects/issues\n\n\n\n\n\nPackage\n\n\nClass\n\n\nArgument\n\n\nDocumentation\n\n\n\n\nbrms\n\n\nbrmsfit\n\n\nndraws\n\n\nbrms::posterior_predict\n\n\n\n\n\n\n\n\nre_formula\n\n\nbrms::posterior_predict\n\n\n\n\nlme4\n\n\nmerMod\n\n\nre.form\n\n\nlme4::predict.merMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nlme4::predict.merMod\n\n\n\n\nglmmTMB\n\n\nglmmTMB\n\n\nre.form\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\n\n\n\n\nzitype\n\n\nglmmTMB::predict.glmmTMB\n\n\n\n\nmgcv\n\n\nbam\n\n\nexclude\n\n\nmgcv::predict.bam\n\n\n\n\nrobustlmm\n\n\nrlmerMod\n\n\nre.form\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\n\n\n\n\nallow.new.levels\n\n\nrobustlmm::predict.rlmerMod\n\n\n\n\nMCMCglmm\n\n\nMCMCglmm\n\n\nndraws"
  },
  {
    "objectID": "reference/plot_slopes.html#examples",
    "href": "reference/plot_slopes.html#examples",
    "title": "plot_slopes",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod <- lm(mpg ~ hp * drat * factor(am), data = mtcars)\n\nplot_slopes(mod, variables = \"hp\", condition = \"drat\")\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = c(\"drat\", \"am\"))\n\n\n\nplot_slopes(mod, variables = \"hp\", condition = list(\"am\", \"drat\" = 3:5))\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = range))\n\nWarning: The `am` variable is treated as a categorical (factor) variable, but the\n  original data is of class numeric. It is safer and faster to convert\n  such variables to factor before fitting the model and calling `slopes`\n  functions.\n  \n  This warning appears once per session.\n\n\n\n\nplot_slopes(mod, variables = \"am\", condition = list(\"hp\", \"drat\" = \"threenum\"))"
  },
  {
    "objectID": "reference/datagrid.html#description",
    "href": "reference/datagrid.html#description",
    "title": "datagrid",
    "section": "Description",
    "text": "Description\n\nGenerate a data grid of user-specified values for use in the newdata argument of the predictions(), comparisons(), and slopes() functions. This is useful to define where in the predictor space we want to evaluate the quantities of interest. Ex: the predicted outcome or slope for a 37 year old college graduate.\n\n\n\n\ndatagrid() generates data frames with combinations of \"typical\" or user-supplied predictor values.\n\n\n\n\ndatagridcf() generates \"counter-factual\" data frames, by replicating the entire dataset once for every combination of predictor values supplied by the user."
  },
  {
    "objectID": "reference/datagrid.html#usage",
    "href": "reference/datagrid.html#usage",
    "title": "datagrid",
    "section": "Usage",
    "text": "Usage\ndatagrid(\n  ...,\n  model = NULL,\n  newdata = NULL,\n  by = NULL,\n  FUN_character = get_mode,\n  FUN_factor = get_mode,\n  FUN_logical = get_mode,\n  FUN_numeric = function(x) mean(x, na.rm = TRUE),\n  FUN_integer = function(x) round(mean(x, na.rm = TRUE)),\n  FUN_other = function(x) mean(x, na.rm = TRUE),\n  grid_type = \"typical\"\n)\n\ndatagridcf(..., model = NULL, newdata = NULL)"
  },
  {
    "objectID": "reference/datagrid.html#arguments",
    "href": "reference/datagrid.html#arguments",
    "title": "datagrid",
    "section": "Arguments",
    "text": "Arguments\n\n\n\n...\n\n\nnamed arguments with vectors of values or functions for user-specified variables.\n\n\n\n\nFunctions are applied to the variable in the model dataset or newdata, and must return a vector of the appropriate type.\n\n\n\n\nCharacter vectors are automatically transformed to factors if necessary. +The output will include all combinations of these variables (see Examples below.)\n\n\n\n\n\n\nmodel\n\n\nModel object\n\n\n\n\nnewdata\n\n\ndata.frame (one and only one of the model and newdata arguments can be used.)\n\n\n\n\nby\n\n\ncharacter vector with grouping variables within which ⁠FUN_*⁠ functions are applied to create \"sub-grids\" with unspecified variables.\n\n\n\n\nFUN_character\n\n\nthe function to be applied to character variables.\n\n\n\n\nFUN_factor\n\n\nthe function to be applied to factor variables.\n\n\n\n\nFUN_logical\n\n\nthe function to be applied to factor variables.\n\n\n\n\nFUN_numeric\n\n\nthe function to be applied to numeric variables.\n\n\n\n\nFUN_integer\n\n\nthe function to be applied to integer variables.\n\n\n\n\nFUN_other\n\n\nthe function to be applied to other variable types.\n\n\n\n\ngrid_type\n\n\ncharacter\n\n\n\n\n\"typical\": variables whose values are not explicitly specified by the user in … are set to their mean or mode, or to the output of the functions supplied to FUN_type arguments.\n\n\n\n\n\"counterfactual\": the entire dataset is duplicated for each combination of the variable values specified in …. Variables not explicitly supplied to datagrid() are set to their observed values in the original dataset."
  },
  {
    "objectID": "reference/datagrid.html#details",
    "href": "reference/datagrid.html#details",
    "title": "datagrid",
    "section": "Details",
    "text": "Details\n\nIf datagrid is used in a predictions(), comparisons(), or slopes() call as the newdata argument, the model is automatically inserted in the model argument of datagrid() call, and users do not need to specify either the model or newdata arguments.\n\n\nIf users supply a model, the data used to fit that model is retrieved using the insight::get_data function."
  },
  {
    "objectID": "reference/datagrid.html#value",
    "href": "reference/datagrid.html#value",
    "title": "datagrid",
    "section": "Value",
    "text": "Value\n\nA data.frame in which each row corresponds to one combination of the named predictors supplied by the user via the … dots. Variables which are not explicitly defined are held at their mean or mode."
  },
  {
    "objectID": "reference/datagrid.html#functions",
    "href": "reference/datagrid.html#functions",
    "title": "datagrid",
    "section": "Functions",
    "text": "Functions\n\n\n\ndatagridcf(): Counterfactual data grid"
  },
  {
    "objectID": "reference/datagrid.html#examples",
    "href": "reference/datagrid.html#examples",
    "title": "datagrid",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\n# The output only has 2 rows, and all the variables except `hp` are at their\n# mean or mode.\ndatagrid(newdata = mtcars, hp = c(100, 110))\n\n       mpg    cyl     disp     drat      wt     qsec     vs      am   gear\n1 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n2 20.09062 6.1875 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875\n    carb  hp\n1 2.8125 100\n2 2.8125 110\n\n# We get the same result by feeding a model instead of a data.frame\nmod <- lm(mpg ~ hp, mtcars)\ndatagrid(model = mod, hp = c(100, 110))\n\n       mpg  hp\n1 20.09062 100\n2 20.09062 110\n\n# Use in `marginaleffects` to compute \"Typical Marginal Effects\". When used\n# in `slopes()` or `predictions()` we do not need to specify the\n#`model` or `newdata` arguments.\nslopes(mod, newdata = datagrid(hp = c(100, 110)))\n\n\n Term Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %  hp\n   hp  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484 100\n   hp  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484 110\n\nColumns: rowid, term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp \n\n# datagrid accepts functions\ndatagrid(hp = range, cyl = unique, newdata = mtcars)\n\n       mpg     disp     drat      wt     qsec     vs      am   gear   carb  hp\n1 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n2 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n3 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125  52\n4 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n5 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n6 20.09062 230.7219 3.596563 3.21725 17.84875 0.4375 0.40625 3.6875 2.8125 335\n  cyl\n1   6\n2   4\n3   8\n4   6\n5   4\n6   8\n\ncomparisons(mod, newdata = datagrid(hp = fivenum))\n\n\n Term Contrast Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 %\n   hp       +1  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484\n   hp       +1  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484\n   hp       +1  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484\n   hp       +1  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484\n   hp       +1  -0.0682     0.0101 -6.74   <0.001 35.9 -0.0881 -0.0484\n\nColumns: rowid, term, contrast, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, predicted, predicted_hi, predicted_lo, mpg, hp \n\n# The full dataset is duplicated with each observation given counterfactual\n# values of 100 and 110 for the `hp` variable. The original `mtcars` includes\n# 32 rows, so the resulting dataset includes 64 rows.\ndg <- datagrid(newdata = mtcars, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64\n\n# We get the same result by feeding a model instead of a data.frame\nmod <- lm(mpg ~ hp, mtcars)\ndg <- datagrid(model = mod, hp = c(100, 110), grid_type = \"counterfactual\")\nnrow(dg)\n\n[1] 64"
  },
  {
    "objectID": "reference/hypotheses.html#description",
    "href": "reference/hypotheses.html#description",
    "title": "hypotheses",
    "section": "Description",
    "text": "Description\n\nUncertainty estimates are calculated as first-order approximate standard errors for linear or non-linear functions of a vector of random variables with known or estimated covariance matrix. In that sense, hypotheses emulates the behavior of the excellent and well-established car::deltaMethod and car::linearHypothesis functions, but it supports more models; requires fewer dependencies; expands the range of tests to equivalence and superiority/inferiority; and offers convenience features like robust standard errors.\n\n\nTo learn more, read the hypothesis tests vignette, visit the package website, or scroll down this page for a full list of vignettes:\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html\n\n\n\n\nhttps://vincentarelbundock.github.io/marginaleffects/\n\n\n\n\nWarning #1: Tests are conducted directly on the scale defined by the type argument. For some models, it can make sense to conduct hypothesis or equivalence tests on the “link” scale instead of the “response” scale which is often the default.\n\n\nWarning #2: For hypothesis tests on objects produced by the marginaleffects package, it is safer to use the hypothesis argument of the original function. Using hypotheses() may not work in certain environments, in lists, or when working programmatically with *apply style functions."
  },
  {
    "objectID": "reference/hypotheses.html#usage",
    "href": "reference/hypotheses.html#usage",
    "title": "hypotheses",
    "section": "Usage",
    "text": "Usage\nhypotheses(\n  model,\n  hypothesis = NULL,\n  vcov = NULL,\n  conf_level = 0.95,\n  df = Inf,\n  equivalence = NULL,\n  joint = FALSE,\n  joint_test = \"f\",\n  FUN = NULL,\n  ...\n)"
  },
  {
    "objectID": "reference/hypotheses.html#arguments",
    "href": "reference/hypotheses.html#arguments",
    "title": "hypotheses",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nmodel\n\n\nModel object or object generated by the comparisons(), slopes(), predictions(), or marginal_means() functions.\n\n\n\n\nhypothesis\n\n\nspecify a hypothesis test or custom contrast using a numeric value, vector, or matrix, a string, or a string formula.\n\n\n\n\nNumeric:\n\n\n\n\nSingle value: the null hypothesis used in the computation of Z and p (before applying transform).\n\n\n\n\nVector: Weights to compute a linear combination of (custom contrast between) estimates. Length equal to the number of rows generated by the same function call, but without the hypothesis argument.\n\n\n\n\nMatrix: Each column is a vector of weights, as describe above, used to compute a distinct linear combination of (contrast between) estimates. The column names of the matrix are used as labels in the output.\n\n\n\n\n\n\nString formula to specify linear or non-linear hypothesis tests. If the term column uniquely identifies rows, terms can be used in the formula. Otherwise, use b1, b2, etc. to identify the position of each parameter. Examples:\n\n\n\n\nhp = drat\n\n\n\n\nhp + drat = 12\n\n\n\n\nb1 + b2 + b3 = 0\n\n\n\n\n\n\nString:\n\n\n\n\n\"pairwise\": pairwise differences between estimates in each row.\n\n\n\n\n\"reference\": differences between the estimates in each row and the estimate in the first row.\n\n\n\n\n\"sequential\": difference between an estimate and the estimate in the next row.\n\n\n\n\n\"revpairwise\", \"revreference\", \"revsequential\": inverse of the corresponding hypotheses, as described above.\n\n\n\n\n\n\nSee the Examples section below and the vignette: https://vincentarelbundock.github.io/marginaleffects/articles/hypothesis.html\n\n\n\n\n\n\nvcov\n\n\nType of uncertainty estimates to report (e.g., for robust standard errors). Acceptable values:\n\n\n\n\nFALSE: Do not compute standard errors. This can speed up computation considerably.\n\n\n\n\nTRUE: Unit-level standard errors using the default vcov(model) variance-covariance matrix.\n\n\n\n\nString which indicates the kind of uncertainty estimates to return.\n\n\n\n\nHeteroskedasticity-consistent: “HC”, “HC0”, “HC1”, “HC2”, “HC3”, “HC4”, “HC4m”, “HC5”. See ?sandwich::vcovHC\n\n\n\n\nHeteroskedasticity and autocorrelation consistent: “HAC”\n\n\n\n\nMixed-Models degrees of freedom: \"satterthwaite\", \"kenward-roger\"\n\n\n\n\nOther: “NeweyWest”, “KernHAC”, “OPG”. See the sandwich package documentation.\n\n\n\n\n\n\nOne-sided formula which indicates the name of cluster variables (e.g., ~unit_id). This formula is passed to the cluster argument of the sandwich::vcovCL function.\n\n\n\n\nSquare covariance matrix\n\n\n\n\nFunction which returns a covariance matrix (e.g., stats::vcov(model))\n\n\n\n\n\n\nconf_level\n\n\nnumeric value between 0 and 1. Confidence level to use to build a confidence interval.\n\n\n\n\ndf\n\n\nDegrees of freedom used to compute p values and confidence intervals. A single numeric value between 1 and Inf. When df is Inf, the normal distribution is used. When df is finite, the t distribution is used. See insight::get_df for a convenient function to extract degrees of freedom. Ex: slopes(model, df = insight::get_df(model))\n\n\n\n\nequivalence\n\n\nNumeric vector of length 2: bounds used for the two-one-sided test (TOST) of equivalence, and for the non-inferiority and non-superiority tests. See Details section below.\n\n\n\n\njoint\n\n\nJoint test of statistical significance. The null hypothesis value can be set using the hypothesis argument.\n\n\n\n\nFALSE: Hypotheses are not tested jointly.\n\n\n\n\nTRUE: All parameters are tested jointly.\n\n\n\n\nString: A regular expression to match parameters to be tested jointly. grep(joint, perl = TRUE)\n\n\n\n\nCharacter vector of parameter names to be tested. Characters refer to the names of the vector returned by coef(object).\n\n\n\n\nInteger vector of indices. Which parameters positions to test jointly.\n\n\n\n\n\n\njoint_test\n\n\nA character string specifying the type of test, either \"f\" or \"chisq\". The null hypothesis is set by the hypothesis argument, with default null equal to 0 for all parameters.\n\n\n\n\nFUN\n\n\nNULL or function.\n\n\n\n\nNULL (default): hypothesis test on a model’s coefficients, or on the quantities estimated by one of the marginaleffects package functions.\n\n\n\n\nFunction which accepts a model object and returns a numeric vector or a data.frame with two columns called term and estimate. This argument can be useful when users want to conduct a hypothesis test on an arbitrary function of quantities held in a model object.\n\n\n\n\n\n\n...\n\n\nAdditional arguments are passed to the predict() method supplied by the modeling package.These arguments are particularly useful for mixed-effects or bayesian models (see the online vignettes on the marginaleffects website). Available arguments can vary from model to model, depending on the range of supported arguments by each modeling package. See the \"Model-Specific Arguments\" section of the ?marginaleffects documentation for a non-exhaustive list of available arguments."
  },
  {
    "objectID": "reference/hypotheses.html#joint-hypothesis-tests",
    "href": "reference/hypotheses.html#joint-hypothesis-tests",
    "title": "hypotheses",
    "section": "Joint hypothesis tests",
    "text": "Joint hypothesis tests\n\nThe test statistic for the joint Wald test is calculated as (R * theta_hat - r)’ * inv(R * V_hat * R’) * (R * theta_hat - r) / Q, where theta_hat is the vector of estimated parameters, V_hat is the estimated covariance matrix, R is a Q x P matrix for testing Q hypotheses on P parameters, r is a Q x 1 vector for the null hypothesis, and Q is the number of rows in R. If the test is a Chi-squared test, the test statistic is not normalized.\n\n\nThe p-value is then calculated based on either the F-distribution (for F-test) or the Chi-squared distribution (for Chi-squared test). For the F-test, the degrees of freedom are Q and (n - P), where n is the sample size and P is the number of parameters. For the Chi-squared test, the degrees of freedom are Q."
  },
  {
    "objectID": "reference/hypotheses.html#equivalence-inferiority-superiority",
    "href": "reference/hypotheses.html#equivalence-inferiority-superiority",
    "title": "hypotheses",
    "section": "Equivalence, Inferiority, Superiority",
    "text": "Equivalence, Inferiority, Superiority\n\n is an estimate, _ its estimated standard error, and [a, b] are the bounds of the interval supplied to the equivalence argument.\n\n\nNon-inferiority:\n\n\n\n\nH_0: a\n\n\n\n\nH_1: > a\n\n\n\n\nt=(- a)/_\n\n\n\n\np: Upper-tail probability\n\n\n\n\nNon-superiority:\n\n\n\n\nH_0: b\n\n\n\n\nH_1: < b\n\n\n\n\nt=(- b)/_\n\n\n\n\np: Lower-tail probability\n\n\n\n\nEquivalence: Two One-Sided Tests (TOST)\n\n\n\n\np: Maximum of the non-inferiority and non-superiority p values.\n\n\n\n\nThanks to Russell V. Lenth for the excellent emmeans package and documentation which inspired this feature."
  },
  {
    "objectID": "reference/hypotheses.html#examples",
    "href": "reference/hypotheses.html#examples",
    "title": "hypotheses",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nmod <- lm(mpg ~ hp + wt + factor(cyl), data = mtcars)\n\n# When `FUN` and `hypotheses` are `NULL`, `hypotheses()` returns a data.frame of parameters\nhypotheses(mod)\n\n\n Term Estimate Std. Error     z Pr(>|z|)   2.5 %    97.5 %     S\n   b1  35.8460      2.041 17.56   <0.001 31.8457 39.846319 227.0\n   b2  -0.0231      0.012 -1.93   0.0531 -0.0465  0.000306   4.2\n   b3  -3.1814      0.720 -4.42   <0.001 -4.5918 -1.771012  16.6\n   b4  -3.3590      1.402 -2.40   0.0166 -6.1062 -0.611803   5.9\n   b5  -3.1859      2.170 -1.47   0.1422 -7.4399  1.068169   2.8\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n# Test of equality between coefficients\nhypotheses(mod, hypothesis = \"hp = wt\")\n\n\n    Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %    S\n hp = wt     3.16       0.72 4.39   <0.001  1.75   4.57 16.4\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n# Non-linear function\nhypotheses(mod, hypothesis = \"exp(hp + wt) = 0.1\")\n\n\n               Term Estimate Std. Error     z Pr(>|z|)  2.5 %  97.5 %   S\n exp(hp + wt) = 0.1  -0.0594     0.0292 -2.04   0.0418 -0.117 -0.0022 4.6\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n# Robust standard errors\nhypotheses(mod, hypothesis = \"hp = wt\", vcov = \"HC3\")\n\n\n    Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %    S\n hp = wt     3.16      0.805 3.92   <0.001  1.58   4.74 13.5\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n# b1, b2, ... shortcuts can be used to identify the position of the\n# parameters of interest in the output of FUN\nhypotheses(mod, hypothesis = \"b2 = b3\")\n\n\n    Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %    S\n b2 = b3     3.16       0.72 4.39   <0.001  1.75   4.57 16.4\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n# term names with special characters have to be enclosed in backticks\nhypotheses(mod, hypothesis = \"`factor(cyl)6` = `factor(cyl)8`\")\n\n\n                            Term Estimate Std. Error      z Pr(>|z|) 2.5 %\n `factor(cyl)6` = `factor(cyl)8`   -0.173       1.65 -0.105    0.917 -3.41\n 97.5 %   S\n   3.07 0.1\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\nmod2 <- lm(mpg ~ hp * drat, data = mtcars)\nhypotheses(mod2, hypothesis = \"`hp:drat` = drat\")\n\n\n             Term Estimate Std. Error    z Pr(>|z|) 2.5 % 97.5 %   S\n `hp:drat` = drat    -6.08       2.89 -2.1   0.0357 -11.8 -0.405 4.8\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n# predictions(), comparisons(), and slopes()\nmod <- glm(am ~ hp + mpg, data = mtcars, family = binomial)\ncmp <- comparisons(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b1 = b2\")\n\n\n  Term Estimate Std. Error     z Pr(>|z|)   S  2.5 %  97.5 %\n b1=b2   -0.288      0.125 -2.31   0.0209 5.6 -0.532 -0.0435\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\nmfx <- slopes(mod, newdata = \"mean\")\nhypotheses(cmp, hypothesis = \"b2 = 0.2\")\n\n\n   Term Estimate Std. Error     z Pr(>|z|)   S  2.5 % 97.5 %\n b2=0.2    0.101      0.131 0.774    0.439 1.2 -0.155  0.358\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\npre <- predictions(mod, newdata = datagrid(hp = 110, mpg = c(30, 35)))\nhypotheses(pre, hypothesis = \"b1 = b2\")\n\n\n  Term  Estimate Std. Error      z Pr(>|z|)   S     2.5 %   97.5 %\n b1=b2 -3.57e-05   0.000172 -0.207    0.836 0.3 -0.000373 0.000302\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high \n\n# The `FUN` argument can be used to compute standard errors for fitted values\nmod <- glm(am ~ hp + mpg, data = mtcars, family = binomial)\n\nf <- function(x) predict(x, type = \"link\", newdata = mtcars)\np <- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error      z Pr(>|z|) 2.5 % 97.5 %   S\n   b1   -1.098      0.716 -1.534    0.125 -2.50  0.305 3.0\n   b2   -1.098      0.716 -1.534    0.125 -2.50  0.305 3.0\n   b3    0.233      0.781  0.299    0.765 -1.30  1.764 0.4\n   b4   -0.595      0.647 -0.919    0.358 -1.86  0.674 1.5\n   b5   -0.418      0.647 -0.645    0.519 -1.69  0.851 0.9\n   b6   -5.026      2.195 -2.290    0.022 -9.33 -0.725 5.5\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\nf <- function(x) predict(x, type = \"response\", newdata = mtcars)\np <- hypotheses(mod, FUN = f)\nhead(p)\n\n\n Term Estimate Std. Error     z Pr(>|z|)   2.5 % 97.5 %   S\n   b1  0.25005     0.1342 1.863  0.06243 -0.0130 0.5131 4.0\n   b2  0.25005     0.1342 1.863  0.06243 -0.0130 0.5131 4.0\n   b3  0.55803     0.1926 2.898  0.00376  0.1806 0.9355 8.1\n   b4  0.35560     0.1483 2.399  0.01646  0.0650 0.6462 5.9\n   b5  0.39710     0.1550 2.561  0.01043  0.0932 0.7010 6.6\n   b6  0.00652     0.0142 0.459  0.64635 -0.0213 0.0344 0.6\n\nColumns: term, estimate, std.error, statistic, p.value, conf.low, conf.high, s.value \n\n# Equivalence, non-inferiority, and non-superiority tests\nmod <- lm(mpg ~ hp + factor(gear), data = mtcars)\np <- predictions(mod, newdata = \"median\")\nhypotheses(p, equivalence = c(17, 18))\n\n\n Estimate Std. Error    z Pr(>|z|)     S 2.5 % 97.5 %  hp gear p (NonInf)\n     19.7          1 19.6   <0.001 281.3  17.7   21.6 123    3    0.00404\n p (NonSup) p (Equiv)\n      0.951     0.951\n\nColumns: rowid, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, mpg, hp, gear, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n\nmfx <- avg_slopes(mod, variables = \"hp\")\nhypotheses(mfx, equivalence = c(-.1, .1))\n\n\n Term Estimate Std. Error     z Pr(>|z|)    S   2.5 %  97.5 % p (NonInf)\n   hp  -0.0669      0.011 -6.05   <0.001 29.4 -0.0885 -0.0452    0.00135\n p (NonSup) p (Equiv)\n     <0.001   0.00135\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n\ncmp <- avg_comparisons(mod, variables = \"gear\", hypothesis = \"pairwise\")\n\nWarning: The `gear` variable is treated as a categorical (factor) variable, but\n  the original data is of class numeric. It is safer and faster to convert\n  such variables to factor before fitting the model and calling `slopes`\n  functions.\n  \n  This warning appears once per session.\n\nhypotheses(cmp, equivalence = c(0, 10))\n\n\n              Term Estimate Std. Error     z Pr(>|z|)   S 2.5 % 97.5 %\n (4 - 3) - (5 - 3)    -3.94       2.05 -1.92   0.0543 4.2 -7.95 0.0727\n p (NonInf) p (NonSup) p (Equiv)\n      0.973     <0.001     0.973\n\nColumns: term, estimate, std.error, statistic, p.value, s.value, conf.low, conf.high, statistic.noninf, statistic.nonsup, p.value.noninf, p.value.nonsup, p.value.equiv \n\n# joint hypotheses: character vector\nmodel <- lm(mpg ~ as.factor(cyl) * hp, data = mtcars)\nhypotheses(model, joint = c(\"as.factor(cyl)6:hp\", \"as.factor(cyl)8:hp\"))\n\n\n\nJoint hypothesis test:\nas.factor(cyl)6:hp = 0\nas.factor(cyl)8:hp = 0\n \n    F Pr(>|F|) Df 1 Df 2\n 2.11    0.142    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: regular expression\nhypotheses(model, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n as.factor(cyl)6:hp = 0\n as.factor(cyl)8:hp = 0\n \n   F Pr(>|F|) Df 1 Df 2\n 5.7  0.00197    4   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: integer indices\nhypotheses(model, joint = 2:3)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 0\n as.factor(cyl)8 = 0\n \n    F Pr(>|F|) Df 1 Df 2\n 6.12  0.00665    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: different null hypotheses\nhypotheses(model, joint = 2:3, hypothesis = 1)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 1\n \n    F Pr(>|F|) Df 1 Df 2\n 6.84  0.00411    2   26\n\nColumns: statistic, p.value, df1, df2 \n\nhypotheses(model, joint = 2:3, hypothesis = 1:2)\n\n\n\nJoint hypothesis test:\n as.factor(cyl)6 = 1\n as.factor(cyl)8 = 2\n \n    F Pr(>|F|) Df 1 Df 2\n 7.47  0.00273    2   26\n\nColumns: statistic, p.value, df1, df2 \n\n# joint hypotheses: marginaleffects object\ncmp <- avg_comparisons(model)\nhypotheses(cmp, joint = \"cyl\")\n\n\n\nJoint hypothesis test:\n cyl 6 - 4 = 0\n cyl 8 - 4 = 0\n \n   F Pr(>|F|) Df 1 Df 2\n 1.6    0.219    2   29\n\nColumns: statistic, p.value, df1, df2"
  },
  {
    "objectID": "reference/inferences.html#description",
    "href": "reference/inferences.html#description",
    "title": "inferences",
    "section": "Description",
    "text": "Description\n\nWarning: This function is experimental. It may be renamed, the user interface may change, or the functionality may migrate to arguments in other marginaleffects functions.\n\n\nApply this function to a marginaleffects object to change the inferential method used to compute uncertainty estimates."
  },
  {
    "objectID": "reference/inferences.html#usage",
    "href": "reference/inferences.html#usage",
    "title": "inferences",
    "section": "Usage",
    "text": "Usage\ninferences(x, method, R = 1000, conf_type = \"perc\", ...)"
  },
  {
    "objectID": "reference/inferences.html#arguments",
    "href": "reference/inferences.html#arguments",
    "title": "inferences",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nx\n\n\nObject produced by one of the core marginaleffects functions.\n\n\n\n\nmethod\n\n\nString\n\n\n\n\n\"delta\": delta method standard errors\n\n\n\n\n\"boot\" package\n\n\n\n\n\"fwb\": fractional weighted bootstrap\n\n\n\n\n\"rsample\" package\n\n\n\n\n\"simulation\" from a multivariate normal distribution (Krinsky & Robb, 1986)\n\n\n\n\n\"mi\" multiple imputation for missing data\n\n\n\n\n\n\nR\n\n\nNumber of resamples or simulations.\n\n\n\n\nconf_type\n\n\nString: type of bootstrap interval to construct.\n\n\n\n\nboot: \"perc\", \"norm\", \"basic\", or \"bca\"\n\n\n\n\nfwb: \"perc\", \"norm\", \"basic\", \"bc\", or \"bca\"\n\n\n\n\nrsample: \"perc\" or \"bca\"\n\n\n\n\nsimulation: argument ignored.\n\n\n\n\n\n\n...\n\n\n\n\nIf method=“boot”, additional arguments are passed to boot::boot().\n\n\n\n\nIf method=“fwb”, additional arguments are passed to fwb::fwb().\n\n\n\n\nIf method=“rsample”, additional arguments are passed to rsample::bootstraps().\n\n\n\n\nIf method=“simulation”, additional arguments are ignored."
  },
  {
    "objectID": "reference/inferences.html#details",
    "href": "reference/inferences.html#details",
    "title": "inferences",
    "section": "Details",
    "text": "Details\n\nWhen method=“simulation”, we conduct simulation-based inference following the method discussed in Krinsky & Robb (1986):\n\n\n\n\nDraw R sets of simulated coefficients from a multivariate normal distribution with mean equal to the original model’s estimated coefficients and variance equal to the model’s variance-covariance matrix (classical, \"HC3\", or other).\n\n\n\n\nUse the R sets of coefficients to compute R sets of estimands: predictions, comparisons, or slopes.\n\n\n\n\nTake quantiles of the resulting distribution of estimands to obtain a confidence interval and the standard deviation of simulated estimates to estimate the standard error.\n\n\n\n\nWhen method=“fwb”, drawn weights are supplied to the model fitting function’s weights argument; if the model doesn’t accept non-integer weights, this method should not be used. If weights were included in the original model fit, they are extracted by weights() and multiplied by the drawn weights. These weights are supplied to the wts argument of the estimation function (e.g., comparisons())."
  },
  {
    "objectID": "reference/inferences.html#value",
    "href": "reference/inferences.html#value",
    "title": "inferences",
    "section": "Value",
    "text": "Value\n\nA marginaleffects object with simulation or bootstrap resamples and objects attached."
  },
  {
    "objectID": "reference/inferences.html#references",
    "href": "reference/inferences.html#references",
    "title": "inferences",
    "section": "References",
    "text": "References\n\nKrinsky, I., and A. L. Robb. 1986. “On Approximating the Statistical Properties of Elasticities.” Review of Economics and Statistics 68 (4): 715–9.\n\n\nKing, Gary, Michael Tomz, and Jason Wittenberg. \"Making the most of statistical analyses: Improving interpretation and presentation.\" American journal of political science (2000): 347-361\n\n\nDowd, Bryan E., William H. Greene, and Edward C. Norton. \"Computation of standard errors.\" Health services research 49.2 (2014): 731-750."
  },
  {
    "objectID": "reference/inferences.html#examples",
    "href": "reference/inferences.html#examples",
    "title": "inferences",
    "section": "Examples",
    "text": "Examples\n\nlibrary(marginaleffects)\n\nlibrary(marginaleffects)\nlibrary(magrittr)\nset.seed(1024)\nmod <- lm(Sepal.Length ~ Sepal.Width * Species, data = iris)\n\n# bootstrap\navg_predictions(mod, by = \"Species\") %>%\n  inferences(method = \"boot\")\n\n\n    Species Estimate Std. Error 2.5 % 97.5 %\n setosa         5.01     0.0344  4.93   5.07\n versicolor     5.94     0.0619  5.82   6.07\n virginica      6.59     0.0788  6.44   6.75\n\nColumns: Species, estimate, std.error, conf.low, conf.high \n\navg_predictions(mod, by = \"Species\") %>%\n  inferences(method = \"rsample\")\n\n\n    Species Estimate 2.5 % 97.5 %\n setosa         5.01  4.94   5.07\n versicolor     5.94  5.82   6.06\n virginica      6.59  6.44   6.75\n\nColumns: Species, estimate, conf.low, conf.high \n\n# Fractional (bayesian) bootstrap\navg_slopes(mod, by = \"Species\") %>%\n  inferences(method = \"fwb\") %>%\n  posterior_draws(\"rvar\") %>%\n  data.frame()\n\n         term                        contrast    Species  estimate predicted\n1 Sepal.Width                     mean(dY/dX)     setosa 0.6904897  5.055715\n2 Sepal.Width                     mean(dY/dX) versicolor 0.8650777  6.307983\n3 Sepal.Width                     mean(dY/dX)  virginica 0.9015345  6.881900\n4     Species mean(versicolor) - mean(setosa)     setosa 1.4992211  5.055715\n5     Species mean(versicolor) - mean(setosa) versicolor 1.3843422  6.307983\n6     Species mean(versicolor) - mean(setosa)  virginica 1.4199582  6.881900\n7     Species  mean(virginica) - mean(setosa)     setosa 1.9912967  5.055715\n8     Species  mean(virginica) - mean(setosa) versicolor 1.8524292  6.307983\n9     Species  mean(virginica) - mean(setosa)  virginica 1.8954823  6.881900\n  predicted_hi predicted_lo  std.error  conf.low conf.high         rvar\n1     5.055881     5.055715 0.08102709 0.5487008 0.8583514 0.69 ± 0.081\n2     6.308191     6.307983 0.20712193 0.4808519 1.2809997 0.88 ± 0.207\n3     6.882117     6.881900 0.23704875 0.3867165 1.3338278 0.89 ± 0.237\n4     6.567507     5.055715 0.15422309 1.2037516 1.7935150 1.50 ± 0.154\n5     6.307983     4.848568 0.08693716 1.2216215 1.5524924 1.38 ± 0.087\n6     6.394491     4.917617 0.08908202 1.2437332 1.5875035 1.42 ± 0.089\n7     7.062207     5.055715 0.12038738 1.7300974 2.2013722 1.99 ± 0.120\n8     6.791747     4.848568 0.12456281 1.6287253 2.1275295 1.86 ± 0.125\n9     6.881900     4.917617 0.09656870 1.7196001 2.0992510 1.90 ± 0.097\n\n# Simulation-based inference\nslopes(mod) %>%\n  inferences(method = \"simulation\") %>%\n  head()\n\n\n        Term Contrast Estimate Std. Error 2.5 % 97.5 %\n Sepal.Width    dY/dX    0.696      0.167 0.366   1.01\n Sepal.Width    dY/dX    0.696      0.167 0.366   1.01\n Sepal.Width    dY/dX    0.696      0.167 0.366   1.01\n Sepal.Width    dY/dX    0.696      0.167 0.366   1.01\n Sepal.Width    dY/dX    0.696      0.167 0.366   1.01\n Sepal.Width    dY/dX    0.696      0.167 0.366   1.01\n\nColumns: rowid, term, contrast, estimate, std.error, conf.low, conf.high, predicted, predicted_hi, predicted_lo, tmp_idx, Sepal.Length, Sepal.Width, Species"
  },
  {
    "objectID": "reference/posterior_draws.html#description",
    "href": "reference/posterior_draws.html#description",
    "title": "posterior_draws",
    "section": "Description",
    "text": "Description\n\nExtract Posterior Draws or Bootstrap Resamples from marginaleffects Objects"
  },
  {
    "objectID": "reference/posterior_draws.html#usage",
    "href": "reference/posterior_draws.html#usage",
    "title": "posterior_draws",
    "section": "Usage",
    "text": "Usage\nposterior_draws(x, shape = \"long\")"
  },
  {
    "objectID": "reference/posterior_draws.html#arguments",
    "href": "reference/posterior_draws.html#arguments",
    "title": "posterior_draws",
    "section": "Arguments",
    "text": "Arguments\n\n\n\nx\n\n\nAn object produced by a marginaleffects package function, such as predictions(), avg_slopes(), hypotheses(), etc.\n\n\n\n\nshape\n\n\nstring indicating the shape of the output format:\n\n\n\n\n\"long\": long format data frame\n\n\n\n\n\"DxP\": Matrix with draws as rows and parameters as columns\n\n\n\n\n\"PxD\": Matrix with draws as rows and parameters as columns\n\n\n\n\n\"rvar\": Random variable datatype (see posterior package documentation)."
  },
  {
    "objectID": "reference/posterior_draws.html#value",
    "href": "reference/posterior_draws.html#value",
    "title": "posterior_draws",
    "section": "Value",
    "text": "Value\n\nA data.frame with drawid and draw columns."
  }
]